question,answer,explanation
"Which component on a Kubernetes node is responsible for managing the node's communication with the control plane and running containers?

A) kube-proxy
B) kubelet
C) container runtime
D) kube-controller-manager",B) kubelet,"The correct answer is B) kubelet. The kubelet is an agent running on each node that manages the node's communication with the control plane and ensures containers are running in pods. Option A) kube-proxy handles network proxy and load balancing but does not manage node communication or container lifecycle. Option C) container runtime (like containerd or Docker) is responsible for running containers but not for managing communication with Kubernetes. Option D) kube-controller-manager is a control plane component, not a node component."
"What flag must be set to false if you want to manually create and manage Node objects using kubectl in Kubernetes?

A) --register-node
B) --node-labels
C) --node-ip
D) --node-status-update-frequency",A) --register-node,"The correct answer is A) --register-node. Setting --register-node=false instructs the kubelet not to automatically register itself with the API server, enabling manual creation and management of Node objects via kubectl. Option B) --node-labels is used to assign labels during registration but does not control registration mode. Option C) --node-ip specifies the node's IP address, not registration mode. Option D) --node-status-update-frequency controls how often the kubelet updates node status, not registration."
"Which command marks a node as unschedulable, preventing new pods from being scheduled on it?

A) kubectl drain
B) kubectl delete node
C) kubectl cordon
D) kubectl taint nodes",C) kubectl cordon,"The correct answer is C) kubectl cordon. This command marks a node as unschedulable, meaning the scheduler will not place new pods on it but existing pods will remain. Option A) kubectl drain is used to safely evict pods from a node, often before maintenance, but does not just mark the node as unschedulable. Option B) kubectl delete node deletes the node object from the cluster. Option D) kubectl taint nodes applies taints to influence scheduling but does not directly mark the node as unschedulable."
"How can you view detailed status information, including conditions and capacity, of a specific node?

A) kubectl get nodes
B) kubectl describe node <node-name>
C) kubectl get node <node-name> -o yaml
D) kubectl logs <node-name>",B) kubectl describe node <node-name>,"The correct answer is B) kubectl describe node <node-name>. This command provides detailed information about the node, including conditions, capacity, and status. Option A) kubectl get nodes shows a summary list of nodes, but not detailed status. Option C) kubectl get node <node-name> -o yaml outputs the node object in YAML format, which includes detailed info but is less user-friendly for quick status review. Option D) kubectl logs <node-name> is invalid because logs are not available directly for nodes."
"Which component in Kubernetes is responsible for monitoring node health, updating node conditions, and triggering eviction of unreachable nodes?

A) kube-controller-manager
B) kube-scheduler
C) node controller
D) kube-proxy",C) node controller,"The correct answer is C) node controller. It manages node health, updates the node status conditions, and triggers eviction of pods when a node becomes unreachable. Option A) kube-controller-manager contains various controllers, including the node controller, but the specific responsibility lies with the node controller. Option B) kube-scheduler assigns pods to nodes but does not monitor node health. Option D) kube-proxy handles network proxying and load balancing, not health monitoring."
"In Kubernetes, what is the default protocol used for communication between nodes and the control plane's API server?

A) HTTP
B) HTTPS
C) TCP
D) SSH",B) HTTPS,"The correct answer is B) HTTPS. By default, the API server listens on port 443 (HTTPS) and secures communications with nodes and clients using TLS encryption. Option A) HTTP is incorrect because it is insecure and not used by default for control plane communications. Option C) TCP is too generic and does not specify encryption; the actual protocol is HTTPS, which runs over TCP. Option D) SSH is used for tunneling or administrative access, not for standard API server communication."
"Which component is primarily responsible for fetching logs and attaching to running pods from the control plane?

A) kube-proxy
B) kube-controller-manager
C) kubelet
D) kube-scheduler",C) kubelet,"The correct answer is C) kubelet. The kubelet runs on each node and is responsible for managing individual pods, including providing logs and attaching to pods via commands like 'kubectl logs' and 'kubectl attach'. Option A) kube-proxy handles network proxying. Option B) kube-controller-manager manages higher-level control loops but does not handle logs or direct pod attachment. Option D) kube-scheduler schedules pods onto nodes but does not manage logs or attachments."
"What is the purpose of the '--kubelet-certificate-authority' flag when used with the Kubernetes API server?

A) To specify the CA for verifying the API server's certificate
B) To specify the CA for verifying the kubelet's serving certificate
C) To authenticate client certificates for kubelets
D) To enable SSH tunneling to the kubelet",B) To specify the CA for verifying the kubelet's serving certificate,"The correct answer is B) To specify the CA for verifying the kubelet's serving certificate. This flag provides the API server with a root CA bundle to validate the kubelet's TLS certificate, ensuring secure communication. Option A) is incorrect because it pertains to the API server's own certificate verification, which is configured separately. Option C) refers to client certificate authentication, which is managed through different flags and configurations. Option D) is incorrect because the flag does not enable SSH tunneling."
"In the context of Kubernetes control plane to node communication, why are plain HTTP connections considered unsafe over untrusted networks?

A) They do not encrypt data, risking exposure of sensitive information.
B) They are incompatible with most network devices.
C) They automatically require VPNs to function.
D) They only work within the same subnet.","A) They do not encrypt data, risking exposure of sensitive information.","The correct answer is A) They do not encrypt data, risking exposure of sensitive information. Plain HTTP connections are unencrypted, so data transmitted can be intercepted and read by malicious actors when used over untrusted networks. Option B) is incorrect because HTTP can work over any network; the issue is security, not compatibility. Option C) is incorrect because HTTP does not require VPNs; VPNs are a security measure, not a requirement. Option D) is incorrect because HTTP can operate over any network segment; its security depends on encryption, not network topology."
"What is the primary purpose of the Konnectivity service in Kubernetes?

A) To replace SSH tunnels for control plane to node communication.
B) To provide a new scheduler for pods.
C) To manage persistent storage for stateful applications.
D) To monitor network traffic within the cluster.",A) To replace SSH tunnels for control plane to node communication.,"The correct answer is A) To replace SSH tunnels for control plane to node communication. The Konnectivity service provides a TCP-level proxy that secures control plane to node traffic, replacing deprecated SSH tunnels. Option B) is incorrect because it does not relate to scheduling. Option C) is incorrect because storage management is handled by storage classes and CSI drivers, not Konnectivity. Option D) is incorrect as network traffic monitoring is performed by other tools like network plugins or monitoring solutions; Konnectivity's role is secure communication."
"In Kubernetes, what is the primary role of a controller?

A) To manually manage the cluster state
B) To watch the current state and make or request changes to reach the desired state
C) To directly create all resources in the cluster
D) To replace the API server in managing cluster state",B) To watch the current state and make or request changes to reach the desired state,The correct answer is B. Controllers in Kubernetes are control loops that continuously monitor the current state of the cluster and work to bring it closer to the desired state by making necessary changes. Option A is incorrect because controllers automate management rather than manual intervention. Option C is incorrect because controllers do not directly create resources; they request the API server to do so. Option D is wrong because controllers do not replace the API server; they work in conjunction with it.
"Which component in Kubernetes manages the control loops for built-in controllers such as Deployment and Job?

A) kube-scheduler
B) kube-controller-manager
C) kube-apiserver
D) kubelet",B) kube-controller-manager,"The correct answer is B. The kube-controller-manager runs the control loops for various built-in controllers like Deployment, Job, and others, ensuring the cluster state aligns with the desired configurations. Option A, kube-scheduler, is responsible for scheduling Pods onto nodes but does not manage controllers. Option C, kube-apiserver, handles API requests but does not run controllers directly. Option D, kubelet, manages individual node operations but is not responsible for control loops."
"What does the Job controller in Kubernetes do?

A) Creates Pods to run a specific task until completion
B) Manages the lifecycle of Deployments
C) Handles the scaling of Nodes
D) Monitors the health of the cluster nodes",A) Creates Pods to run a specific task until completion,"The correct answer is A. The Job controller creates Pods to execute a specific task and ensures they run to completion, marking the Job as finished when done. Option B is incorrect because Deployment controllers manage Pods for deploying applications, not Jobs. Option C is incorrect because Node scaling is managed by other controllers or external systems. Option D pertains to node health monitoring, which is outside the scope of the Job controller."
"Which command would you use to create a Deployment named 'nginx-deploy' with 3 replicas using the nginx:1.20 image?

A) kubectl run nginx-deploy --image=nginx:1.20 --replicas=3
B) kubectl create deployment nginx-deploy --image=nginx:1.20 --replicas=3
C) kubectl apply -f deployment.yaml
D) kubectl create pod nginx-deploy --image=nginx:1.20 --replicas=3",B) kubectl create deployment nginx-deploy --image=nginx:1.20 --replicas=3,"The correct answer is B. This command directly creates a Deployment named 'nginx-deploy' with 3 replicas using the specified image. Option A is incorrect because 'kubectl run' no longer supports the '--replicas' flag in newer versions; it is mainly used for quick runs and not for detailed deployment management. Option C is a valid way if you have a YAML file, but it is not the direct command. Option D is incorrect because 'kubectl create pod' does not support the '--replicas' flag and is not suitable for creating multiple Pods as a Deployment does."
"Which of the following statements about controllers in Kubernetes is correct?

A) Controllers only run inside the Kubernetes control plane
B) Controllers can only manage resources within the cluster
C) Controllers can run inside or outside the control plane and may manage external systems
D) Controllers are not fault-tolerant and will stop working if one fails",C) Controllers can run inside or outside the control plane and may manage external systems,"The correct answer is C. Kubernetes controllers can be deployed within the control plane or externally, and some controllers interact with external systems to manage resources outside the cluster, such as cloud provider APIs. Option A is incorrect because controllers can also run outside the control plane. Option B is incorrect because controllers can manage external systems as well. Option D is incorrect because Kubernetes is designed to be resilient; if one controller fails, others can take over, ensuring fault tolerance."
"In Kubernetes, what API group do Lease objects belong to?

A) coordination.k8s.io
B) apps.k8s.io
C) core.k8s.io
D) batch.k8s.io",A) coordination.k8s.io,"The correct answer is A) coordination.k8s.io. Lease objects are part of the coordination.k8s.io API group, which is used for distributed coordination tasks such as leader election and node heartbeats. Option B) apps.k8s.io is incorrect because it's typically related to workload resources like Deployments and StatefulSets. Option C) core.k8s.io is incorrect as there is no such API group; core resources are in the core API group. Option D) batch.k8s.io pertains to batch jobs and CronJobs, not leases."
"Which namespace is used by default for storing Lease objects related to node heartbeats?

A) kube-system
B) kube-node-lease
C) default
D) kube-leases",B) kube-node-lease,"The correct answer is B) kube-node-lease. Kubernetes stores node heartbeat Leases in the 'kube-node-lease' namespace by default. This allows the control plane to track node availability through lease updates. Option A) kube-system is a common namespace for system components but not specifically for node heartbeats. Option C) default is the default namespace for user resources, not for node leases. Option D) kube-leases is incorrect because it is not a standard namespace in Kubernetes."
"What field in the Lease object is used by Kubernetes control plane to determine node availability?

A) spec.holderIdentity
B) spec.leaseDurationSeconds
C) spec.renewTime
D) metadata.creationTimestamp",C) spec.renewTime,"The correct answer is C) spec.renewTime. The Kubernetes control plane uses the 'renewTime' timestamp to determine whether a node is alive, as it indicates the last time the node sent a heartbeat update. Option A) holderIdentity identifies the current holder of the lease but does not directly determine node availability. Option B) leaseDurationSeconds specifies how long the lease is valid but isn't used directly for health checks. Option D) creationTimestamp indicates when the lease was created but does not reflect recent activity."
"Starting from which Kubernetes version did kube-apiserver begin publishing its identity using Leases?

A) v1.24
B) v1.25
C) v1.26
D) v1.27",C) v1.26,"The correct answer is C) v1.26. As indicated in the content, starting in Kubernetes v1.26, kube-apiserver uses the Lease API to publish its identity, which helps in discovering how many API server instances are running. Options A), B), and D) are incorrect because this feature was introduced specifically in v1.26 and not in earlier versions."
"Which command lists all Lease objects in the kube-system namespace with the label 'apiserver.kubernetes.io/identity=kube-apiserver'?

A) kubectl get lease -n kube-system
B) kubectl get lease -l apiserver.kubernetes.io/identity=kube-apiserver -n kube-system
C) kubectl describe lease -l apiserver.kubernetes.io/identity=kube-apiserver
D) kubectl get lease -A -l apiserver.kubernetes.io/identity=kube-apiserver",B) kubectl get lease -l apiserver.kubernetes.io/identity=kube-apiserver -n kube-system,"The correct answer is B) kubectl get lease -l apiserver.kubernetes.io/identity=kube-apiserver -n kube-system. This command lists Lease objects in the 'kube-system' namespace filtered by the specified label selector. Option A) lacks the label selector, so it lists all Lease objects in that namespace. Option C) uses 'describe', which provides detailed info but not a list with filtering. Option D) lists all namespaces but is not necessary here; the question specifies the 'kube-system' namespace."
Create a Lease named 'leader-election' in the 'default' namespace with a lease duration of 30 seconds using kubectl.,kubectl create lease leader-election --namespace=default --lease-duration=30,"This command creates a Lease resource named 'leader-election' in the 'default' namespace with a lease duration of 30 seconds. The '--lease-duration' flag sets how long the lease is valid before it expires if not renewed. Note that in actual practice, creating a Lease resource typically involves YAML or API calls; the 'kubectl create lease' command may need to be supplemented with a YAML manifest depending on the Kubernetes version. However, as a simplified example, this command aligns with the task. The key is understanding how to specify the name, namespace, and lease duration."
"Which controller within the cloud-controller-manager is responsible for updating Node objects when new servers are created in the cloud infrastructure?

A) Route controller
B) Service controller
C) Node controller
D) Cloud controller",C) Node controller,"The correct answer is C) Node controller. It is responsible for updating Node objects with information about the cloud servers, such as unique identifiers, labels, hostnames, and health status. Option A) Route controller manages network routes between nodes, not node object updates. Option B) Service controller handles load balancer and service integration with cloud infrastructure, not node objects. Option D) 'Cloud controller' is a general term, but the specific controller for node updates is the Node controller."
"Which RBAC verb permissions are required by the cloud-controller-manager's Service controller to update the status of a Service?

A) get, list, watch
B) patch, update
C) create, delete
D) get, patch, update","D) get, patch, update","The correct answer is D) get, patch, update. The Service controller needs 'get' to read Service objects, and 'patch' and 'update' to modify the status subresource of Services, such as load balancer details. Option A) get, list, watch are used for watching resources but not for updating status. Option B) patch, update alone are insufficient without 'get' to read the current state. Option C) create, delete are for resource lifecycle management, not status updates."
"What is the primary purpose of the cloud-controller-manager in Kubernetes?

A) To manage Pods and ReplicaSets
B) To embed cloud-specific control logic and interface with cloud provider APIs
C) To handle internal network routing between nodes
D) To monitor cluster health and perform scheduling",B) To embed cloud-specific control logic and interface with cloud provider APIs,"The correct answer is B) To embed cloud-specific control logic and interface with cloud provider APIs. The cloud-controller-manager allows Kubernetes to interact with cloud infrastructure via plugins, managing cloud resources such as nodes, routes, and load balancers. Option A) managing Pods and ReplicaSets is handled by the core Kubernetes controllers, not the cloud-controller-manager. Option C) internal network routing is the responsibility of network plugins or CNI, not this component. Option D) monitoring and scheduling are core Kubernetes functions, separate from cloud-specific control logic."
"Create a ClusterRole binding that grants the cloud-controller-manager full access to Node objects, including create, get, list, update, patch, watch, and delete permissions.

A) kubectl create clusterrolebinding cloud-controller --clusterrole=cluster-admin --user=system:serviceaccount:kube-system:cloud-controller
B) kubectl create clusterrolebinding cc-role --clusterrole=edit --serviceaccount=kube-system:cloud-controller
C) kubectl create clusterrolebinding cc-role --clusterrole=cluster-admin --serviceaccount=kube-system:cloud-controller
D) kubectl create clusterrolebinding cc-role --clusterrole=admin --user=system:serviceaccount:kube-system:cloud-controller",C) kubectl create clusterrolebinding cc-role --clusterrole=cluster-admin --serviceaccount=kube-system:cloud-controller,"The correct answer is C) to bind a ClusterRole with full permissions (like cluster-admin) to the specific ServiceAccount used by the cloud-controller-manager. This grants the necessary permissions, including create, get, list, update, patch, watch, and delete on Node objects. Option A) binds a user, not a ServiceAccount, and assigns cluster-admin privileges, which is overly permissive and not recommended. Option B) uses the 'edit' role, which has fewer permissions than necessary. Option D) binds to a user with 'admin' privileges but is not typical for service accounts; the preferred method is binding the ServiceAccount directly."
"Which interface does the cloud-controller-manager implement to allow cloud provider integrations?

A) CloudInterface
B) CloudProvider
C) CloudController
D) CloudPlugin",B) CloudProvider,The correct answer is B) CloudProvider. The CloudProvider interface defined in cloud.go is used by cloud controllers to implement cloud-specific logic. Option A) CloudInterface is not a standard Kubernetes interface. Option C) CloudController is not a defined interface in Kubernetes; controllers are part of the cloud provider implementation. Option D) CloudPlugin is also not a standard interface; plugins implement the CloudProvider interface.
"What is the primary benefit of cgroup v2 over cgroup v1 in Kubernetes resource management?

A) Support for multiple hierarchies
B) Unified control system with enhanced resource management capabilities
C) Compatibility only with legacy Linux kernels
D) Removal of resource isolation features",B) Unified control system with enhanced resource management capabilities,"The correct answer is B) cgroup v2 provides a single, unified hierarchy for resource control, offering improved resource management and isolation across various resources. Option A) is incorrect because cgroup v2 consolidates hierarchies rather than supports multiple separate ones. Option C) is incorrect because cgroup v2 requires Linux Kernel 5.8 or later, not legacy kernels. Option D) is incorrect because cgroup v2 enhances, rather than removes, resource isolation features."
"Which command can you run on a Linux node to determine whether it is using cgroup v1 or v2?

A) cat /proc/cgroups
B) systemctl status
C) stat -fc %T /sys/fs/cgroup/
D) uname -r",C) stat -fc %T /sys/fs/cgroup/,"The correct answer is C) 'stat -fc %T /sys/fs/cgroup/' outputs 'cgroup2fs' for cgroup v2 and 'tmpfs' for cgroup v1, allowing you to identify the cgroup version in use. Option A) 'cat /proc/cgroups' shows information about cgroups but does not clearly differentiate between v1 and v2. Option B) 'systemctl status' provides system service status but not cgroup version info. Option D) 'uname -r' shows kernel version, which is related but does not directly indicate cgroup version."
"Which Linux kernel version is required to support cgroup v2 in Kubernetes?

A) 4.15 or later
B) 5.8 or later
C) 3.10 or later
D) 6.0 or later",B) 5.8 or later,"The correct answer is B) Linux Kernel 5.8 or later is required for native support of cgroup v2. Option A) 4.15 is incorrect because it predates cgroup v2 support. Option C) 3.10 is incorrect for the same reason. Option D) 6.0 is correct but not the minimum; the minimum supported version is 5.8, making B the most accurate choice."
"How can you manually enable cgroup v2 on a Linux distribution that uses GRUB bootloader?

A) Add 'systemd.unified_cgroup_hierarchy=1' to /etc/default/grub and run 'sudo update-grub'
B) Set 'cgroup2' in /etc/cgroups.conf and reboot
C) Run 'systemctl enable cgroup-v2'
D) Edit /etc/cgroup/config and set 'v2=true'",A) Add 'systemd.unified_cgroup_hierarchy=1' to /etc/default/grub and run 'sudo update-grub',"The correct answer is A) To manually enable cgroup v2 on distributions using GRUB, you add 'systemd.unified_cgroup_hierarchy=1' to the GRUB_CMDLINE_LINUX in /etc/default/grub, then run 'sudo update-grub' to apply changes. Option B) is incorrect because there is no such file as /etc/cgroups.conf for this purpose. Option C) 'systemctl enable cgroup-v2' is invalid as there is no such systemd service. Option D) editing /etc/cgroup/config is not standard for enabling cgroup v2."
"Which container runtime versions support cgroup v2 in Kubernetes?

A) containerd v1.3 and cri-o v1.19
B) containerd v1.4 and cri-o v1.20
C) Docker v19.03 only
D) All container runtimes support cgroup v2 by default",B) containerd v1.4 and cri-o v1.20,The correct answer is B) containerd v1.4 and cri-o v1.20 support cgroup v2. Option A) is incorrect because containerd v1.3 and cri-o v1.19 do not support cgroup v2 natively. Option C) Docker v19.03 is outdated and not the recommended runtime for cgroup v2 support in Kubernetes. Option D) is incorrect because not all container runtimes support cgroup v2 by default; support depends on the specific version.
"What Kubernetes component is responsible for ensuring containers are running and restarting them if they fail?

A) kube-proxy
B) kubelet
C) API Server
D) scheduler",B) kubelet,"The correct answer is B) kubelet. The kubelet runs on each node and is responsible for managing the lifecycle of containers, ensuring they are running as specified, and restarting them if they fail. Option A) kube-proxy handles network routing and load balancing but does not manage container lifecycle. Option C) API Server exposes the Kubernetes API but does not directly manage container states. Option D) scheduler assigns Pods to nodes but does not handle container restarts."
"Which Kubernetes controller maintains the desired number of Pod replicas for stateless workloads?

A) StatefulSet
B) DaemonSet
C) Deployment
D) ReplicaSet",C) Deployment,The correct answer is C) Deployment. Deployments manage stateless applications and ensure that the specified number of pod replicas are running by creating and updating ReplicaSets accordingly. Option A) StatefulSet is used for stateful applications requiring persistent identities. Option B) DaemonSet ensures a Pod runs on each node but does not manage replica counts. Option D) ReplicaSet is used internally by Deployments to manage replicas but is not typically created directly by users for high-level management.
"Which self-healing feature allows Kubernetes to replace a failed Pod in a StatefulSet?

A) Container restart based on restartPolicy
B) Replica replacement managed by StatefulSet controller
C) Persistent volume reattachment
D) Service load balancing",B) Replica replacement managed by StatefulSet controller,"The correct answer is B) Replica replacement managed by StatefulSet controller. StatefulSets maintain stable identities for Pods and automatically replace failed Pods to meet the desired replica count. Option A) pertains to container restarts within a Pod, not Pod replacement. Option C) relates to persistent volume management, not Pod replacement. Option D) load balancing handles traffic routing but does not replace Pods."
Create a PersistentVolumeClaim named 'my-pvc' requesting 10Gi of storage of class 'standard'.,"kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: standard
EOF",This command creates a PersistentVolumeClaim (PVC) named 'my-pvc' requesting 10Gi of storage with the storage class 'standard'. It uses a YAML manifest piped into 'kubectl apply -f -' for quick creation. 'accessModes' specify how the volume can be mounted; 'ReadWriteOnce' allows a single node to read/write. The 'storage' resource requests 10Gi. Specifying 'storageClassName' ensures it binds to the appropriate StorageClass.
"If a Pod behind a Service fails, how does Kubernetes handle traffic routing?

A) It restarts the Pod automatically.
B) It removes the Pod from the Service's endpoints.
C) It creates a new Pod to replace it.
D) It scales the Deployment up.",B) It removes the Pod from the Service's endpoints.,"The correct answer is B) It removes the Pod from the Service's endpoints. Kubernetes continuously monitors Pod health, and if a Pod fails, it is automatically removed from the Service endpoints so that traffic is only routed to healthy Pods. Option A) refers to container restart, which is a separate self-healing mechanism. Option C) creating a new Pod is handled by higher-level controllers like Deployments, but the Service's endpoints update dynamically. Option D) scaling is a different process related to adjusting the number of Pods, not individual Pod failures."
"Which API version must a container runtime support to be compatible with Kubernetes v1.26 and later?

A) v1
B) v2
C) v1beta1
D) v2beta1",A) v1,"The correct answer is A) v1. Kubernetes v1.26 and later require that the container runtime supports the v1 CRI API. This ensures compatibility and proper registration of the node. Option B) v2 and options C) v1beta1, D) v2beta1 are incorrect because they are either non-existent or deprecated versions not supported by Kubernetes v1.26+ for CRI compliance."
"What is the primary purpose of the Container Runtime Interface (CRI) in Kubernetes?

A) To define a protocol for communication between kubelet and container runtime
B) To manage network policies for pods
C) To orchestrate storage volumes for containers
D) To monitor container resource usage",A) To define a protocol for communication between kubelet and container runtime,"The correct answer is A) To define a protocol for communication between kubelet and container runtime. CRI enables the kubelet to interact with various container runtimes without recompiling cluster components. Option B) managing network policies, C) orchestrating storage, and D) monitoring resource usage are responsibilities handled by other Kubernetes components or features, not CRI."
"Which flag is used with kubelet to specify the container runtime endpoint?

A) --container-runtime-endpoint
B) --runtime-endpoint
C) --cri-endpoint
D) --container-endpoint",A) --container-runtime-endpoint,The correct answer is A) --container-runtime-endpoint. This flag tells kubelet where to connect to the container runtime via gRPC. Option B) --runtime-endpoint is incorrect because it is not a valid flag. Option C) --cri-endpoint is incorrect; the correct flag is --container-runtime-endpoint. Option D) --container-endpoint does not exist in kubelet flags.
"In Kubernetes, what happens if the container runtime does not support the v1 CRI API when using Kubernetes v1.26 or later?

A) The kubelet will fail to register the node
B) The cluster will automatically downgrade to an earlier version
C) The kubelet will ignore the runtime and proceed
D) The container runtime will be automatically upgraded",A) The kubelet will fail to register the node,"The correct answer is A) The kubelet will fail to register the node because support for the v1 CRI API is mandatory for Kubernetes v1.26+. If the runtime does not support v1, registration fails. Option B) automatic downgrade is incorrect; Kubernetes does not perform automatic version downgrades. Option C) ignoring the runtime is incorrect as registration depends on v1 support. Option D) automatic runtime upgrade is incorrect; runtime support must be manually upgraded for compatibility."
"Which protocol does the CRI primarily use for communication between kubelet and container runtime?

A) HTTP
B) gRPC
C) TCP
D) UDP",B) gRPC,"The correct answer is B) gRPC. CRI defines the main communication protocol between kubelet and container runtime as gRPC, which is suitable for high-performance, language-agnostic RPC communication. Option A) HTTP is incorrect because CRI uses gRPC, not plain HTTP. Options C) TCP and D) UDP are transport protocols but not the specific protocol used at the CRI API level; gRPC typically runs over HTTP/2, which uses TCP."
"Which of the following resources is automatically cleaned up by Kubernetes garbage collection when it has no owner references?

A) Terminated pods
B) Running pods
C) PersistentVolumes with retain policy
D) Active Services",A) Terminated pods,"The correct answer is A) Terminated pods. Kubernetes garbage collection cleans up resources like terminated pods, completed jobs, and objects without owner references. Option B) Running pods are active and not cleaned up automatically. Option C) PersistentVolumes with retain policy are not automatically deleted; they require manual intervention. Option D) Active services are not subject to garbage collection based on owner references."
"What does setting the ownerReference.field.ownerDeletion=true on a dependent object enforce during cascading deletion?

A) Dependents are orphaned upon owner deletion
B) Dependents are deleted only after the owner is fully deleted
C) Dependents are deleted immediately, regardless of finalizers
D) Dependents are not deleted at all",B) Dependents are deleted only after the owner is fully deleted,"The correct answer is B) Dependents are deleted only after the owner is fully deleted. When ownerReference.ownerDeletion=true, Kubernetes ensures dependents are not deleted until the owner object has been completely removed, enforcing a strict delete order. Option A) is incorrect because this setting prevents orphaning dependents. Option C) is false because finalizers may delay deletion until cleanup tasks complete. Option D) is incorrect because dependents are intended to be deleted."
"Which command can you run to check for invalid cross-namespace owner references reported as events?

A) kubectl get events --all-namespaces
B) kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace
C) kubectl describe pod
D) kubectl get ownerreferences",B) kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace,"The correct answer is B) kubectl get events -A --field-selector=reason=OwnerRefInvalidNamespace. This command filters events across all namespaces for the specific reason related to invalid owner references, which is useful for diagnosing garbage collection issues. Option A) lists all events but does not filter by reason. Option C) describes a pod but does not show owner reference events. Option D) is not a valid command; owner references are part of resource metadata, not a standalone resource."
"In foreground cascading deletion, what is the role of the metadata.finalizers field?

A) It prevents the deletion of dependent objects.
B) It marks the owner object as being in deletion in progress.
C) It immediately deletes the owner object and all dependents.
D) It prevents the owner object from being deleted.",B) It marks the owner object as being in deletion in progress.,"The correct answer is B) It marks the owner object as being in deletion in progress. When foreground deletion is initiated, Kubernetes sets metadata.deletionTimestamp and metadata.finalizers to indicate deletion is underway, allowing dependent objects to be cleaned up before the owner is fully removed. Option A) is incorrect because finalizers do not prevent deletion but control cleanup order. Option C) describes background deletion, not foreground. Option D) is incorrect because finalizers are part of the deletion process, not a block."
"Which kubelet configuration setting controls the maximum age an image can remain unused before being eligible for garbage collection?

A) imageMinimumGCAge
B) imageMaximumGCAge
C) imageRetentionPeriod
D) imageUnusedTimeout",B) imageMaximumGCAge,"The correct answer is B) imageMaximumGCAge. This setting specifies the maximum duration an image can remain unused before it is considered for garbage collection. Option A) is incorrect because it suggests a minimum age, which is not a standard setting. Options C) and D) are not valid kubelet configuration parameters; the correct parameter is imageMaximumGCAge, which is a duration value (e.g., 12h45m)."
"Describe the purpose of the 'MaxPerPodContainer' parameter in kubelet garbage collection configuration.

A) It limits the total number of containers in the cluster.
B) It sets the maximum number of dead containers per pod.
C) It specifies the maximum age of a container before garbage collection.
D) It defines the maximum size of container logs to retain.",B) It sets the maximum number of dead containers per pod.,"The correct answer is B) It sets the maximum number of dead containers per pod. This parameter controls how many terminated containers are retained per pod before they are garbage collected. Option A) is incorrect because it does not limit total containers in the cluster, only per pod. Option C) relates to container age, which is controlled by MinAge. Option D) concerns log retention, which is unrelated to MaxPerPodContainer."
"What is the primary purpose of the Mixed Version Proxy feature in Kubernetes v1.33?

A) To enable rolling updates without downtime
B) To proxy resource requests between API servers running different versions
C) To improve network security between cluster components
D) To automatically upgrade API resources to the latest version",B) To proxy resource requests between API servers running different versions,"The correct answer is B) To proxy resource requests between API servers running different versions. This feature allows API servers with different Kubernetes versions to handle requests for resources they do not recognize locally by proxying those requests to peer API servers that can serve them. Option A) is incorrect because it pertains to rolling updates, not proxying. Option C) is incorrect because the primary goal isn't network security. Option D) is incorrect because the feature doesn't automatically upgrade resources but facilitates communication during version upgrades."
"Which feature gate must be enabled to activate the Mixed Version Proxy in the API server?

A) EnableAggregator
B) UnknownVersionInteroperabilityProxy
C) ProxyToPeerAPIServers
D) EnableProxyLayer",B) UnknownVersionInteroperabilityProxy,"The correct answer is B) UnknownVersionInteroperabilityProxy. This feature gate must be enabled to activate the mixed version proxy functionality, allowing the API server to proxy requests to peer servers with different versions. Option A) EnableAggregator is incorrect because it's unrelated to this specific feature. Option C) ProxyToPeerAPIServers is not a valid feature gate name in Kubernetes. Option D) EnableProxyLayer is also incorrect because it is not a recognized feature gate."
"In configuring the Mixed Version Proxy, which command-line argument specifies the CA certificate bundle used to verify peer API servers?

A) --peer-ca-file
B) --proxy-client-cert-file
C) --requestheader-client-ca-file
D) --peer-advertise-ip",A) --peer-ca-file,"The correct answer is A) --peer-ca-file. This argument specifies the path to the CA certificate bundle used by the source API server to verify the identity of peer API servers. Option B) --proxy-client-cert-file specifies the client certificate used by the API server to authenticate itself to peers. Option C) --requestheader-client-ca-file is used for request header authentication, not peer verification. Option D) --peer-advertise-ip specifies the IP address for peer communication, not the CA bundle."
"What happens if a resource request arrives at an API server that cannot serve the requested API version and the Mixed Version Proxy feature is enabled?

A) The API server returns a 404 Not Found response
B) The request is proxied to a peer API server that can serve the resource
C) The request is ignored and logged
D) The API server automatically upgrades the resource to the latest API version",B) The request is proxied to a peer API server that can serve the resource,"The correct answer is B) The request is proxied to a peer API server that can serve the resource. When the local API server cannot serve a specific API version, it attempts to proxy the request to a peer that recognizes the API group/version/resource. Option A) is incorrect because the API server will try to proxy first before returning 404. Option C) is incorrect because the request isn't ignored; it's proxied. Option D) is incorrect because the system doesn't upgrade resources automatically; it proxies requests to servers that can handle them."
"Describe the role of the --peer-advertise-ip and --peer-advertise-port flags in the context of the Mixed Version Proxy.

A) They specify the IP and port for the API server to listen for incoming proxy requests.
B) They set the IP and port used by peer API servers to identify themselves to each other.
C) They configure the IP and port for external access to the API server.
D) They define the IP and port for the API server's health check endpoint.",B) They set the IP and port used by peer API servers to identify themselves to each other.,The correct answer is B) They set the IP and port used by peer API servers to identify themselves to each other. These flags are used to advertise the network location of each API server so that peers can proxy requests correctly. Option A) is incorrect because these flags do not specify where the server listens for requests; they advertise the server’s identity for peer communication. Option C) is incorrect because they are not for external access configuration. Option D) is incorrect because they are not related to health check endpoints.
"Explain what the internal StorageVersion API is used for in the context of the Mixed Version Proxy.

A) To store the current Kubernetes cluster version
B) To determine which API servers can serve specific resource versions
C) To manage persistent storage for API server state
D) To hold the configuration of proxy settings for API servers",B) To determine which API servers can serve specific resource versions,"The correct answer is B) To determine which API servers can serve specific resource versions. The StorageVersion API helps API servers identify which API groups and versions they support, facilitating proxying requests to the appropriate peer. Option A) is incorrect because it doesn't store cluster version info. Option C) is incorrect because StorageVersion API is unrelated to persistent storage for the API server’s state. Option D) is incorrect because it does not hold proxy configuration but metadata about resource versions."
"What is the default value of the imagePullPolicy for containers in Kubernetes when no policy is explicitly specified during Pod creation?

A) Always
B) IfNotPresent
C) Never
D) OnFailure",B) IfNotPresent,"The correct answer is B) IfNotPresent. When a Pod is created without explicitly setting the imagePullPolicy, Kubernetes defaults to IfNotPresent, meaning the kubelet will skip pulling the image if it already exists locally. Option A) Always is incorrect because this is only the default if the image tag is :latest or if explicitly set. Option C) Never is incorrect because Kubernetes does not default to Never. Option D) OnFailure is not a valid default pull policy in Kubernetes."
"Which of the following image references uses a digest to specify a specific image version?

A) busybox:latest
B) registry.example.com/myapp:v2
C) registry.example.com/myapp@sha256:abcdef1234567890
D) myimage:latest",C) registry.example.com/myapp@sha256:abcdef1234567890,"The correct answer is C) registry.example.com/myapp@sha256:abcdef1234567890. This notation uses a digest (sha256 hash) to specify an immutable, specific version of the image. Option A) busybox:latest uses a tag, which is mutable and not guaranteed to be constant. Option B) registry.example.com/myapp:v2 also uses a tag, which can change over time. Option D) myimage:latest is a mutable tag and does not specify a digest."
"Which imagePullPolicy setting will cause Kubernetes to always attempt to pull the image from the registry every time the container starts, regardless of local cache?

A) IfNotPresent
B) Always
C) Never
D) OnUpdate",B) Always,"The correct answer is B) Always. When imagePullPolicy is set to Always, Kubernetes pulls the image from the registry every time the container is started, ensuring the latest image is used. Option A) IfNotPresent pulls the image only if it is not already available locally. Option C) Never skips pulling the image, assuming it is already present locally, which can cause issues if the image is missing. Option D) OnUpdate is not a valid setting in Kubernetes."
"What command would you use to create a Pod running the nginx:1.19 image with a specific image pull policy set to Always?

A) kubectl run nginx --image=nginx:1.19 --image-pull-policy=Always
B) kubectl create pod nginx --image=nginx:1.19 --pull-policy=Always
C) kubectl run nginx --image=nginx:1.19 --image-pull-policy=Always
D) kubectl create pod nginx --image=nginx:1.19 --pull-policy=Always",A) kubectl run nginx --image=nginx:1.19 --image-pull-policy=Always,"The correct command is A) 'kubectl run nginx --image=nginx:1.19 --image-pull-policy=Always'. This command creates a Pod named 'nginx' with the specified image and explicitly sets the imagePullPolicy to Always. Option B) is incorrect because 'kubectl create pod' does not accept '--pull-policy' as a flag; 'kubectl run' is the appropriate command. Option C) is close but uses the wrong flag '--image-pull-policy', which is not valid; the correct flag is '--image-pull-policy'. Option D) similarly uses an invalid flag '--pull-policy'."
"In Kubernetes, why is it discouraged to use the ':latest' tag in production deployments?

A) It causes the container to always pull the latest image, which can lead to unpredictable versions.
B) Kubernetes does not support the ':latest' tag.
C) It prevents the image from being cached locally.
D) The ':latest' tag is deprecated and no longer supported.","A) It causes the container to always pull the latest image, which can lead to unpredictable versions.","The correct answer is A) It causes the container to always pull the latest image, which can lead to unpredictable versions. Using ':latest' makes it difficult to track which version of the image is running, complicating rollbacks and consistency. Option B) is incorrect because Kubernetes supports ':latest' but its use is discouraged. Option C) is incorrect because the main issue is the unpredictability and versioning, not cache behavior. Option D) is incorrect because ':latest' is not deprecated; it is still supported, but its use in production is discouraged for stability."
"In Kubernetes, how can a container determine the hostname of the Pod it is running in?

A) By inspecting the environment variable POD_NAME
B) By executing the hostname command
C) By querying the Kubernetes API server
D) By reading /etc/hostname file",B) By executing the hostname command,"The correct answer is B) By executing the hostname command. According to Kubernetes documentation, the hostname of a container is set to the name of the Pod it is running in, and it can be retrieved using the hostname command or the gethostname function call in libc. Option A is incorrect because POD_NAME is not a default environment variable unless explicitly set. Option C is incorrect because the container does not need to query the API server for its hostname; it can determine it locally. Option D is incorrect because, while /etc/hostname may contain the hostname, the recommended way is to use the hostname command."
"Which environment variables are automatically available to a container for accessing the service endpoints within the same namespace?

A) SERVICE_HOST and SERVICE_PORT
B) <SERVICE_NAME>_SERVICE_HOST and <SERVICE_NAME>_SERVICE_PORT
C) SERVICE_IP and SERVICE_PORT
D) SERVICE_ENDPOINT and SERVICE_PORT",B) <SERVICE_NAME>_SERVICE_HOST and <SERVICE_NAME>_SERVICE_PORT,"The correct answer is B). Kubernetes automatically injects environment variables for each service in the namespace, following the pattern <SERVICE_NAME>_SERVICE_HOST and <SERVICE_NAME>_SERVICE_PORT. These variables provide the hostname/IP and port of the service. Option A is incorrect because the variables are specific to each service, not generic names. Option C is incorrect because Kubernetes does not define SERVICE_IP; instead, it provides the host via the _SERVICE_HOST variable. Option D is incorrect because SERVICE_ENDPOINT and SERVICE_PORT are not standard environment variables created automatically."
"A container needs to access the list of all services running in its namespace at startup. Which method provides this information?

A) Query the Kubernetes API server directly from the container
B) Use environment variables injected by Kubernetes
C) Read the /etc/services file
D) Use a ConfigMap mounted into the container",B) Use environment variables injected by Kubernetes,"The correct answer is B). Kubernetes injects environment variables into containers that list all services running in the same namespace at the time of container creation. These variables include the service names suffixed with _SERVICE_HOST and _SERVICE_PORT. Option A is possible but not the typical or recommended method for simple service discovery. Option C is incorrect because /etc/services is a standard file listing well-known services and ports, not dynamically generated by Kubernetes. Option D is incorrect because ConfigMaps are used for configuration data, not for listing running services."
"Which statement correctly describes how the container's environment can be configured to include information about the Pod?

A) By setting environment variables manually in the pod spec
B) By using the downward API to expose Pod information
C) By querying the Kubernetes API in the container
D) By mounting a hostPath volume with Pod info",B) By using the downward API to expose Pod information,"The correct answer is B). Kubernetes provides the downward API, which allows exposing information about the Pod, such as its name and namespace, as environment variables inside the container. Option A is possible but manual and not dynamic; the downward API is the preferred method for dynamic Pod info. Option C involves querying the API server, which is more complex and unnecessary for accessing self-information. Option D is incorrect because mounting a hostPath volume with Pod info is not how Kubernetes exposes such data; the downward API is specifically designed for this purpose."
"What is the primary resource that provides a container with its filesystem in Kubernetes?

A) The container image only
B) The container image combined with volumes
C) The Pod's persistent volume claim
D) The node's filesystem",B) The container image combined with volumes,"The correct answer is B). In Kubernetes, a container's filesystem is a combination of the container image and any volumes mounted into the container. The image provides the base filesystem, while volumes can add or override parts of this filesystem at runtime. Option A is incorrect because the image alone does not account for additional data or persistent storage. Option C is incorrect because a PersistentVolumeClaim provides persistent storage but is mounted as a volume, not the entire filesystem. Option D is incorrect because the container's filesystem is isolated and based on the image and volumes, not directly on the node's filesystem."
"What is the primary purpose of the RuntimeClass resource in Kubernetes?

A) To define pod security policies
B) To select container runtime configurations for Pods
C) To manage persistent storage
D) To control network policies",B) To select container runtime configurations for Pods,"The correct answer is B) To select container runtime configurations for Pods. RuntimeClass allows administrators to specify different runtime configurations for Pods, enabling performance and security trade-offs. Option A) is incorrect because RuntimeClass does not manage security policies directly. Option C) is incorrect as storage is managed via PersistentVolumes and StorageClasses, not RuntimeClass. Option D) is incorrect because network policies are handled separately with NetworkPolicy resources."
"Which field in a Pod spec is used to specify the RuntimeClass to be used?

A) runtime
B) runtimeHandler
C) runtimeClassName
D) runtimeProfile",C) runtimeClassName,"The correct answer is C) runtimeClassName. This field in the Pod spec explicitly specifies the name of the RuntimeClass resource to use for running the Pod. Option A) 'runtime' is incorrect as it is not a valid Pod spec field. Option B) 'runtimeHandler' is incorrect; it is a property within the CRI configuration, not the Pod spec. Option D) 'runtimeProfile' is incorrect because it is not a standard field in the Pod specification."
"What happens if a Pod specifies a runtimeClassName that does not exist or cannot be supported by the CRI handler?

A) The Pod runs with the default runtime
B) The Pod enters the Failed phase
C) The Pod is scheduled on any node regardless
D) The Pod is automatically created with a warning",B) The Pod enters the Failed phase,"The correct answer is B) The Pod enters the Failed phase. If the specified RuntimeClass does not exist or the handler cannot support it, the Pod cannot run and will be marked as Failed. Option A) is incorrect because the default runtime is used only if no runtimeClassName is specified. Option C) is incorrect because scheduling is prevented if the runtime support is unavailable, so the Pod cannot be scheduled. Option D) is incorrect because Kubernetes does not create Pods with warnings; it either runs successfully or fails."
"Which configuration files are used to set up container runtimes like containerd and CRI-O for RuntimeClass support?

A) /etc/kubelet/config.yaml
B) /etc/containerd/config.toml and /etc/crio/crio.conf
C) /etc/kube-proxy/config.yaml
D) /etc/kubelet/runtimes.yaml",B) /etc/containerd/config.toml and /etc/crio/crio.conf,"The correct answer is B) /etc/containerd/config.toml and /etc/crio/crio.conf. These files are used to configure the runtimes for containerd and CRI-O, respectively, including runtime handlers that RuntimeClass references. Option A) is incorrect as kubelet configuration files are not the primary place for configuring container runtimes. Option C) is incorrect because kube-proxy manages network proxying, not container runtimes. Option D) is incorrect; there is no standard 'runtimes.yaml' file in kubelet configuration."
"How does scheduling support ensure that Pods using a specific RuntimeClass run only on compatible nodes?

A) By setting a nodeSelector in the Pod spec
B) By specifying a runtimeClass.scheduling.nodeSelector in the RuntimeClass
C) By using taints and tolerations only
D) By configuring a dedicated node pool",B) By specifying a runtimeClass.scheduling.nodeSelector in the RuntimeClass,"The correct answer is B) By specifying a runtimeClass.scheduling.nodeSelector in the RuntimeClass. This field defines constraints so that only nodes matching the selector support the RuntimeClass, ensuring Pods run only on compatible nodes. Option A) is incorrect because nodeSelector in Pod spec is unrelated to RuntimeClass scheduling constraints. Option C) is incomplete; taints and tolerations are used for node admission, but the specific scheduling constraint is set via the nodeSelector in the RuntimeClass. Option D) is a common operational practice but not the direct mechanism described in the RuntimeClass scheduling feature."
"What is the purpose of the overhead field in a RuntimeClass?

A) To specify resource limits for Pods
B) To account for additional resource consumption by Pods utilizing this RuntimeClass
C) To restrict the RuntimeClass to specific nodes
D) To define network overhead for Pods",B) To account for additional resource consumption by Pods utilizing this RuntimeClass,"The correct answer is B) To account for additional resource consumption by Pods utilizing this RuntimeClass. The overhead field allows the scheduler to consider extra resources required for running Pods with specific runtime configurations, such as hardware virtualization. Option A) is incorrect because resource limits are set in container resource requests and limits, not in overhead. Option C) is incorrect; node restrictions are handled via labels and scheduling constraints, not overhead. Option D) is incorrect because overhead refers to compute resources, not network overhead."
"Which lifecycle hook is executed immediately after a container is created in Kubernetes?

A) PostStart
B) PreStop
C) Terminate
D) Init",A) PostStart,"The correct answer is A) PostStart. This hook runs immediately after a container is created, allowing setup tasks to be performed. Option B) PreStop is incorrect because it runs just before a container is terminated, not after creation. Option C) Terminate is not a specific lifecycle hook in Kubernetes. Option D) Init refers to init containers, which are a different concept used during pod startup, not a lifecycle hook."
"What is the primary purpose of the PreStop lifecycle hook in Kubernetes?

A) To initialize container environment variables
B) To run cleanup or save state tasks before container termination
C) To restart the container
D) To check container health periodically",B) To run cleanup or save state tasks before container termination,"The correct answer is B) To run cleanup or save state tasks before container termination. PreStop is invoked immediately before a container is terminated, allowing it to perform necessary cleanup. Option A) is incorrect because environment variables are set at container start, not during PreStop. Option C) is incorrect because PreStop does not restart containers; it executes before termination. Option D) is incorrect because health checks are managed by probes, not lifecycle hooks."
"In Kubernetes, which handler type executes a specific command inside the container during a lifecycle event?

A) HTTP
B) Exec
C) Sleep
D) StopSignal",B) Exec,"The correct answer is B) Exec. This handler type executes a specific command inside the container's process namespace during a lifecycle event, such as PostStart or PreStop. Option A) HTTP executes an HTTP request; it is not a command execution inside the container. Option C) Sleep pauses the container; it is a handler type but used to delay execution. Option D) StopSignal defines a signal sent to the container on stop, not a command execution."
"Which statement correctly describes the behavior of the PreStop hook if it hangs during execution?

A) The container will always be forcefully killed immediately.
B) The container will remain in the Terminating state until the hook completes or the terminationGracePeriodSeconds expires.
C) The container will skip the PreStop hook and continue to terminate.
D) The container will restart automatically.",B) The container will remain in the Terminating state until the hook completes or the terminationGracePeriodSeconds expires.,"The correct answer is B) The container remains in the Terminating state until the PreStop hook completes or the terminationGracePeriodSeconds expires. If the hook hangs, the container cannot fully terminate until the hook finishes or the timeout occurs. Option A) is incorrect because the container is not forcefully killed immediately if the hook hangs; it waits until timeout. Option C) is incorrect because the hook must complete before termination proceeds. Option D) is incorrect because hanging hooks do not cause the container to restart; they delay termination."
"You want to set a custom stop signal for a container that overrides the default SIGTERM. Which field should you specify in the container spec?

A) lifecycle
B) terminationMessagePath
C) stopSignal
D) stopSignal in container spec (via container's securityContext or specific configuration)",D) stopSignal in container spec (via container's securityContext or specific configuration),"The correct answer is D) stopSignal in container spec (via container's securityContext or specific configuration). Kubernetes allows setting a custom stop signal by configuring the container's securityContext or through specific image instructions, such as the STOPSIGNAL instruction in Dockerfile. Option A) lifecycle manages hooks, not stop signals. Option B) terminationMessagePath specifies a file for termination messages, not signals. Option C) stopSignal is not a direct field in Kubernetes container spec; it is set via securityContext or image configuration."
"Which Pod phase indicates that all containers in the Pod have terminated successfully and will not be restarted?

A) Pending
B) Running
C) Succeeded
D) Failed",C) Succeeded,"The correct answer is C) Succeeded. This phase signifies that all containers in the Pod have terminated with success, and the Pod will not be restarted. Option A) Pending means the Pod is accepted but not yet running. Option B) Running indicates at least one container is still active or starting. Option D) Failed indicates that all containers have terminated, but at least one did so with failure; thus, it's not the successful completion state."
"What command would you use to forcibly delete a Pod named 'my-pod' in the default namespace?

A) kubectl delete pod my-pod
B) kubectl delete pod my-pod --force
C) kubectl delete pod my-pod --grace-period=0 --force
D) kubectl delete pod my-pod --immediate",C) kubectl delete pod my-pod --grace-period=0 --force,"The correct answer is C) kubectl delete pod my-pod --grace-period=0 --force. This command forces immediate deletion of the Pod by setting the grace period to zero and using the --force flag, bypassing graceful termination. Option A) is a standard delete, which allows the Pod to terminate gracefully. Option B) alone does not force immediate deletion; it requires --grace-period=0. Option D) '--immediate' is deprecated and not used in current Kubernetes versions; the correct way is to specify '--grace-period=0'."
"Which of the following best describes the 'Pending' phase of a Pod?

A) The Pod has started running and is actively executing containers.
B) The Pod has been accepted but containers are not yet ready to run, including time waiting for image downloads.
C) All containers in the Pod have terminated successfully.
D) The Pod is in the process of being deleted.","B) The Pod has been accepted but containers are not yet ready to run, including time waiting for image downloads.","The correct answer is B) The Pod has been accepted but containers are not yet ready to run, including time waiting for image downloads. 'Pending' indicates that the Pod is accepted by the cluster but not yet in the 'Running' phase, often waiting for images to be pulled or scheduling. Option A) describes the 'Running' phase. Option C) corresponds to the 'Succeeded' phase, not Pending. Option D) relates to deletion, which is outside the Pod phases described."
"How can you check the current state and container statuses of a Pod named 'my-pod'?

A) kubectl get pod my-pod
B) kubectl describe pod my-pod
C) kubectl logs my-pod
D) kubectl status pod my-pod",B) kubectl describe pod my-pod,"The correct answer is B) kubectl describe pod my-pod. This command provides detailed information about the Pod, including the status of each container within it. Option A) gives a brief overview but does not show container states in detail. Option C) retrieves logs from the containers, not their current state. Option D) is not a valid 'kubectl' command; 'kubectl status' does not exist."
"Which container state indicates that a container is still in the process of starting, such as pulling images or applying secrets?

A) Running
B) Terminated
C) Waiting
D) Pending",C) Waiting,"The correct answer is C) Waiting. This state indicates that the container is still performing startup operations like pulling images or applying secrets before it can transition to Running. Option A) Running means the container is active and executing. Option B) Terminated indicates the container has stopped. Option D) Pending is a Pod phase, not a container state; within a container, the 'Waiting' state is used for startup delays."
"Describe the purpose of a 'controller' in Kubernetes and its relation to Pods.

A) To directly manage individual Pod lifecycle events.
B) To handle the creation, scaling, and management of Pods as disposable entities.
C) To replace the kubelet in managing container states.
D) To provide persistent storage for Pods.","B) To handle the creation, scaling, and management of Pods as disposable entities.","The correct answer is B) To handle the creation, scaling, and management of Pods as disposable entities. Controllers abstract the management of Pods, ensuring desired states are maintained, including scaling and self-healing. Option A) is incorrect because controllers manage Pods indirectly, not individual lifecycle events. Option C) is incorrect; the kubelet manages containers on nodes, not controllers. Option D) pertains to storage, which is handled by volume controllers, not the overarching controller managing Pods."
"What is the primary purpose of init containers in a Kubernetes Pod?

A) To run continuously alongside application containers
B) To run setup or initialization tasks before app containers start
C) To replace sidecar containers for lifecycle management
D) To handle load balancing between application containers",B) To run setup or initialization tasks before app containers start,"The correct answer is B) To run setup or initialization tasks before app containers start. Init containers are specialized containers that run to completion before any app containers are started, often used for setup tasks like waiting for services or configuring shared volumes. Option A) is incorrect because init containers do not run continuously; they run only during Pod initialization. Option C) is incorrect because init containers are not a replacement for sidecar containers; they serve different purposes. Option D) is incorrect because init containers are not involved in load balancing."
"Which of the following statements about init containers is true?

A) Init containers support livenessProbe and readinessProbe fields.
B) Init containers run in parallel with application containers.
C) Init containers must run to completion before application containers start.
D) Init containers can dynamically modify their resource requests during execution.",C) Init containers must run to completion before application containers start.,"The correct answer is C) Init containers must run to completion before application containers start. Init containers are designed to perform setup tasks sequentially and must succeed before app containers are initialized. Option A) is incorrect because init containers do not support livenessProbe or readinessProbe fields. Option B) is incorrect because init containers run sequentially, not in parallel. Option D) is incorrect because resource requests and limits are specified in the Pod spec; they are not dynamically modified during execution."
"You need to create a Pod with two init containers that wait for 'myservice' and 'mydb' to be available before starting the main app container. Which part of the Pod spec correctly defines the init containers?

A) containers: [...]
B) initContainers: [...]
C) sidecars: [...]
D) preContainers: [...]",B) initContainers: [...],"The correct answer is B) initContainers: [...]. This field in the Pod specification defines an array of init containers that run sequentially during Pod initialization. Option A) containers: [...] defines the main application containers, not init containers. Option C) sidecars: [...] is not a valid Kubernetes field; sidecar containers are just regular containers in the 'containers' array. Option D) preContainers: [...] is not a valid Kubernetes field."
"What happens if an init container fails during Pod startup and the Pod has a restartPolicy of 'Always'?

A) The init container is run again immediately, and the Pod remains in Pending state.
B) The init container is restarted repeatedly until it succeeds, and the Pod remains in Pending state.
C) The Pod proceeds to start app containers despite the init container failure.
D) The Pod is marked as Failed immediately and does not restart.","B) The init container is restarted repeatedly until it succeeds, and the Pod remains in Pending state.","The correct answer is B) The init container is restarted repeatedly until it succeeds, and the Pod remains in Pending state. Kubernetes will keep retrying the init container until it completes successfully if the restartPolicy allows for restarts (like 'Always'). Option A) is incorrect because the init container is retried, not just run once. Option C) is incorrect because app containers do not start until all init containers have successfully completed. Option D) is incorrect in this context; the Pod will not be marked as Failed solely due to init container failure if retries are allowed."
"Which command correctly creates a Pod with two init containers that wait for services 'myservice' and 'mydb' to be available?

A) kubectl run mypod --image=busybox --init-containers='wait-for-myservice,wait-for-mydb'
B) kubectl create -f pod-with-init-containers.yaml
C) kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: [""sh"", ""-c"", ""until nslookup myservice; do echo waiting for myservice; sleep 2; done""]
  - name: init-mydb
    image: busybox:1.28
    command: [""sh"", ""-c"", ""until nslookup mydb; do echo waiting for mydb; sleep 2; done""]
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: [""sh"", ""-c"", ""echo The app is running! && sleep 3600""]
EOF
D) kubectl create pod myapp --init=wait-for-services","C) kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: [""sh"", ""-c"", ""until nslookup myservice; do echo waiting for myservice; sleep 2; done""]
  - name: init-mydb
    image: busybox:1.28
    command: [""sh"", ""-c"", ""until nslookup mydb; do echo waiting for mydb; sleep 2; done""]
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: [""sh"", ""-c"", ""echo The app is running! && sleep 3600""]
EOF","The correct answer is C) which uses 'kubectl apply -f -' with a YAML manifest provided inline via a heredoc. This creates a Pod with specified initContainers that wait for 'myservice' and 'mydb' using nslookup. Option A) is invalid because 'kubectl run' does not support --init-containers directly. Option B) refers to creating from a YAML file, but the YAML content is not specified here. Option D) is invalid because 'kubectl create' does not have an '--init' flag and does not accept inline init container definitions."
"What is the primary purpose of a sidecar container in Kubernetes?

A) To run the main application logic
B) To provide additional services like logging or monitoring alongside the main container
C) To initialize the pod during startup
D) To replace the main application container in case of failure",B) To provide additional services like logging or monitoring alongside the main container,"The correct answer is B) To provide additional services like logging or monitoring alongside the main container. Sidecar containers run concurrently with the main application container within the same pod and extend its functionality without altering the primary application code. Option A is incorrect because sidecars do not run the main application logic. Option C is incorrect because init containers, not sidecars, handle initialization during startup. Option D is incorrect because sidecars do not replace the main container; they support it."
"Which statement accurately describes the lifecycle and termination order of sidecar containers in a Kubernetes pod?

A) They stop before the main application container, due to their role as supporting services.
B) They continue running after the main container has stopped, and are shut down in the reverse order of their appearance.
C) They are terminated simultaneously with the main application container.
D) They are only active during pod startup and are terminated once the main container is ready.","B) They continue running after the main container has stopped, and are shut down in the reverse order of their appearance.","The correct answer is B. Sidecar containers are designed to remain operational even after the main container stops, supporting ongoing processes or cleanup. During pod termination, they are shut down in the reverse order of their appearance, ensuring supporting services are terminated last. Option A is incorrect because sidecars do not stop before the main container; they often outlive it. Option C is wrong because they are not terminated simultaneously with the main container. Option D is incorrect because sidecars are active throughout the pod's lifecycle, not just during startup."
"Which feature gate must be enabled in Kubernetes to support restart policies for sidecar containers, and from which version is it enabled by default?

A) SidecarContainers, enabled by default since v1.29
B) RestartPolicy, enabled by default since v1.27
C) PodLifecycle, enabled by default since v1.30
D) SupportSidecars, enabled by default since v1.28","A) SidecarContainers, enabled by default since v1.29","The correct answer is A. The SidecarContainers feature gate must be enabled to support restart policies for sidecar containers, and it has been enabled by default since Kubernetes v1.29. Option B is incorrect because RestartPolicy is a core feature but not the specific feature gate for sidecars. Option C is incorrect because PodLifecycle is not a recognized feature gate related to sidecars. Option D is incorrect because SupportSidecars is not an actual feature gate name."
"Given the following pod spec snippet, what does the 'restartPolicy: Always' in the init container configuration imply?

spec:
  initContainers:
  - name: logshipper
    image: alpine:latest
    restartPolicy: Always
    command: [ 'sh', '-c', 'tail -F /opt/logs.txt' ]
    volumeMounts:
    - name: data
      mountPath: /opt","It means the init container 'logshipper' will be restarted automatically if it exits, and it will remain running during the pod's lifecycle.","In Kubernetes, setting restartPolicy: Always for an init container means it will be restarted if it terminates, and it can remain running during the pod's lifecycle, supporting persistent services like log shipping. Though init containers are usually designed to run once during startup, this configuration allows for their restart and continued operation as sidecars, effectively making them support ongoing processes. This is useful when using features like the SidecarContainers feature gate. Note: In typical Kubernetes configurations, init containers do not support restartPolicy: Always; this is a special case when using the feature gate. The other options are incorrect because they don't accurately describe the behavior of restartPolicy: Always in this context."
"You want to create a Kubernetes Deployment named 'myapp' with a main container running 'alpine:latest' and a sidecar container running 'busybox:latest'. Write the kubectl command to generate this deployment with 2 replicas.

A) kubectl create deployment myapp --image=alpine:latest --sidecar=busybox:latest --replicas=2
B) kubectl run myapp --image=alpine:latest --replicas=2 --sidecar=busybox:latest
C) kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: main
        image: alpine:latest
      - name: sidecar
        image: busybox:latest
EOF

D) kubectl create deployment myapp --image=alpine:latest --dry-run=client -o yaml | kubectl set image -f - --image=busybox:latest --name=sidecar --replicas=2","C) kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: main
        image: alpine:latest
      - name: sidecar
        image: busybox:latest
EOF","The correct answer is C. This command uses kubectl apply with inline YAML to create a Deployment named 'myapp' with 2 replicas, specifying both the main container and a sidecar container within the pod template. Option A is incorrect because 'kubectl create deployment' does not support adding multiple containers or sidecars directly via command flags. Option B is invalid because 'kubectl run' does not support multiple containers or sidecars in this manner. Option D is incorrect because it misuses 'kubectl set image' and does not correctly create a multi-container deployment; it is intended for updating images, not creating multi-container specs."
"Which of the following statements about ephemeral containers in Kubernetes is TRUE?

A) They can be added to a Pod after the Pod has been created.
B) They are suitable for building applications because they guarantee resource allocation.
C) They are created using the 'ephemeralcontainers' handler in the API and cannot be added via 'kubectl edit'.
D) They automatically restart if they crash.",C) They are created using the 'ephemeralcontainers' handler in the API and cannot be added via 'kubectl edit'.,"The correct answer is C. Ephemeral containers are created through the 'ephemeralcontainers' handler in the Kubernetes API, not by editing the pod.spec directly, and cannot be added via 'kubectl edit'. Option A is incorrect because ephemeral containers cannot be added after Pod creation; they are inserted via the API. Option B is incorrect because ephemeral containers lack resource guarantees and are not suitable for building applications. Option D is incorrect because ephemeral containers are never restarted automatically; they are meant for temporary troubleshooting."
"Which fields are disallowed in ephemeral containers compared to regular containers?

A) Ports, livenessProbe, resources
B) Image, command, env
C) VolumeMounts, securityContext
D) Name, imagePullPolicy","A) Ports, livenessProbe, resources","The correct answer is A. Ephemeral containers do not support fields such as ports, livenessProbe, and resources due to their transient and troubleshooting-focused nature. Option B is incorrect because image, command, and env are allowed in ephemeral containers. Option C is incorrect because volumeMounts and securityContext are generally allowed unless explicitly disallowed, but the question specifically targets known disallowed fields. Option D is incorrect because name and imagePullPolicy are valid and allowed fields for ephemeral containers."
"What is the primary use case for ephemeral containers in Kubernetes?

A) To run production application containers with guaranteed resources.
B) To perform interactive troubleshooting on existing Pods.
C) To replace containers in a running Pod automatically.
D) To create static Pods with persistent storage.",B) To perform interactive troubleshooting on existing Pods.,"The correct answer is B. Ephemeral containers are designed specifically for troubleshooting purposes, allowing users to run temporary containers in existing Pods to inspect and debug. Option A is incorrect because ephemeral containers are not suitable for production workloads or guaranteed resource allocations. Option C is incorrect because ephemeral containers cannot replace or modify existing containers, only added temporarily for troubleshooting. Option D is incorrect because ephemeral containers are unrelated to static Pods or persistent storage."
"Which command would you use to create an ephemeral container named 'debugger' in an existing Pod called 'my-pod'?

A) kubectl debug my-pod --image=busybox --target=debugger
B) kubectl create ephemeral my-pod --name=debugger --image=busybox
C) kubectl run ephemeral --image=busybox --ephemeral --name=debugger
D) kubectl exec -it my-pod --ephemeral --name=debugger --image=busybox",A) kubectl debug my-pod --image=busybox --target=debugger,"The correct answer is A. The 'kubectl debug' command is used to add ephemeral containers for troubleshooting. The '--image=busybox' specifies the container image, and '--target=debugger' indicates the ephemeral container's name. Option B is incorrect because there's no 'kubectl create ephemeral' command. Option C is incorrect because 'kubectl run' does not support creating ephemeral containers; it is used for creating regular pods. Option D is incorrect because 'kubectl exec' attaches to existing containers, not creating ephemeral ones."
"Why are ephemeral containers not supported in static Pods?

A) Because static Pods do not support any form of debugging.
B) Because static Pods are managed directly by the kubelet and do not support the API handler used for ephemeral containers.
C) Because ephemeral containers require persistent storage, which static Pods lack.
D) Because ephemeral containers automatically restart, which static Pods prevent.",B) Because static Pods are managed directly by the kubelet and do not support the API handler used for ephemeral containers.,"The correct answer is B. Static Pods are managed directly by the kubelet and do not go through the API server, which means they do not support features like ephemeral containers that are created via the 'ephemeralcontainers' API handler. Option A is incorrect because static Pods can be debugged in other ways, but ephemeral containers are not supported. Option C is incorrect because ephemeral containers do not require persistent storage; the issue is API support. Option D is incorrect because static Pods do not inherently prevent restarts, but that is unrelated to ephemeral container support."
"Which QoS class is assigned to a Pod if every Container in the Pod has equal resource requests and limits for both CPU and memory?

A) BestEffort
B) Burstable
C) Guaranteed
D) Default",C) Guaranteed,"The correct answer is C) Guaranteed. This QoS class is assigned when every Container in the Pod has both resource requests and limits set, and the values for requests and limits are equal for each resource. Option A) BestEffort is incorrect because it applies when no resource requests or limits are set. Option B) Burstable is incorrect because it applies when some containers have requests or limits, but not all, or when limits are not equal to requests. Option D) Default is incorrect because Kubernetes explicitly classifies Pods into these three QoS classes, not a default class."
"Which QoS class will Kubernetes evict first when a Node experiences resource pressure?

A) Guaranteed
B) Burstable
C) BestEffort
D) All are evicted equally",C) BestEffort,"The correct answer is C) BestEffort. When a node faces resource pressure, Kubernetes first evicts BestEffort Pods, which have no resource requests or limits set, making them the least prioritized. Option A) Guaranteed Pods are the least likely to be evicted because they have the strictest resource guarantees. Option B) Burstable Pods are evicted after BestEffort Pods but before Guaranteed Pods. Option D) is incorrect because eviction order is based on QoS class priority."
"Create a Pod named 'test-pod' with a memory request of 512Mi and a memory limit of 512Mi, and CPU request of 0.5 and limit of 0.5 using a YAML manifest.","kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  containers:
  - name: test-container
    image: nginx
    resources:
      requests:
        memory: 512Mi
        cpu: 0.5
      limits:
        memory: 512Mi
        cpu: 0.5
EOF","This command applies a YAML manifest directly from stdin, creating a Pod named 'test-pod' with specified resource requests and limits. The 'resources' section defines both requests and limits for memory and CPU, ensuring the Pod is classified as Guaranteed QoS. Using 'kubectl apply -f -' allows for inline YAML definitions. The manifest specifies the container image and resource constraints precisely."
"Which QoS class is assigned to a Pod if at least one container has a resource request or limit, but not all containers have both requests and limits set?

A) Guaranteed
B) Burstable
C) BestEffort
D) Priority",B) Burstable,"The correct answer is B) Burstable. This QoS class is assigned when at least one container has resource requests or limits, but not all containers meet the criteria for Guaranteed (which requires all containers to have equal requests and limits). Option A) Guaranteed is incorrect because that requires all containers to have equal requests and limits. Option C) BestEffort applies when no containers specify resource requests or limits. Option D) Priority is unrelated; it pertains to Pod priority class, not QoS."
"What is the primary purpose of user namespaces in Kubernetes?

A) To isolate network traffic between pods
B) To map users inside containers to different users on the host
C) To manage resource quotas for pods
D) To enable auto-scaling of pods based on load",B) To map users inside containers to different users on the host,"The correct answer is B) To map users inside containers to different users on the host. User namespaces in Linux allow processes running inside containers to have different UID/GID mappings, which enhances security by isolating container users from host users. Option A is incorrect because network isolation is handled by network namespaces, not user namespaces. Option C is incorrect because resource quotas are managed through Kubernetes resource quota objects, not user namespaces. Option D is incorrect as auto-scaling is related to Horizontal Pod Autoscaler, not user namespaces."
"Which of the following filesystems support idmap mounts required for user namespaces in Linux?

A) ext3
B) btrfs
C) NTFS
D) FAT32",B) btrfs,"The correct answer is B) btrfs. According to the Kubernetes documentation, filesystems like btrfs, ext4, xfs, fat, tmpfs, and overlayfs support idmap mounts in Linux 6.3 and above, which are necessary for user namespace functionality. Option A) ext3 is incorrect because it does not support idmap mounts in Linux 6.3. Option C) NTFS and D) FAT32 are incorrect because these filesystems are not supported for idmap mounts in Linux for user namespaces."
"Which container runtime versions support user namespaces in Kubernetes?

A) runc v1.2 or greater
B) Docker Engine v17.03 and later
C) containerd v1.0
D) cri-o v1.20",A) runc v1.2 or greater,"The correct answer is A) runc v1.2 or greater. The documentation states that runc versions 1.2+ support user namespaces, with recommended versions being 1.13+. Option B) Docker Engine v17.03 is incorrect because Docker's support for user namespaces is not specified here, and the focus is on OCI runtimes. Option C) containerd v1.0 is incorrect because the support starts from version 2.0 as per the documentation. Option D) cri-o v1.20 is incorrect because support begins from version 1.25."
"How can a Kubernetes user enable user namespaces for a specific pod?

A) Set the field `pod.spec.enableUserNamespaces` to true
B) Set the field `pod.spec.hostUsers` to false
C) Annotate the pod with `user-namespace=true`
D) Use the `--enable-user-namespaces` flag during pod creation",B) Set the field `pod.spec.hostUsers` to false,"The correct answer is B) Set the field `pod.spec.hostUsers` to false. This opt-in setting allows the pod to use user namespaces, mapping container users to different host users. Option A) is incorrect because `enableUserNamespaces` is not a valid field. Option C) is incorrect because there's no such annotation required for enabling user namespaces. Option D) is incorrect because enabling user namespaces is controlled via pod spec fields, not runtime flags during creation."
"What is the default UID/GID range used when user namespaces are enabled in Kubernetes?

A) 0-65535
B) 10000-20000
C) 0-1024
D) 65536-131071",A) 0-65535,"The correct answer is A) 0-65535. By default, when user namespaces are enabled, the valid UID/GID range is set to 0-65535, which allows mapping of container user IDs to this range on the host. Option B) 10000-20000 is incorrect because the default range is not set to that. Option C) 0-1024 is incorrect because it only covers a small subset of UIDs/GIDs. Option D) 65536-131071 is incorrect because this is outside the default range; the range starts at 0."
"Which command would you use to configure a kubelet to support custom UID/GID ranges for user namespaces?

A) kubelet --enable-userns --uid-range=100000-200000
B) kubelet --feature-gates=UserNamespaces=true
C) kubelet --kubeconfig=path/to/config
D) kubelet --register-node=true",A) kubelet --enable-userns --uid-range=100000-200000,"The correct answer is A) kubelet --enable-userns --uid-range=100000-200000. This command enables user namespaces on the kubelet and specifies a custom UID/GID range for pod mappings. Option B) is incorrect because `--feature-gates` does not control user namespace support specifically. Option C) `--kubeconfig` is used for specifying the kubeconfig file, not for enabling user namespaces. Option D) `--register-node=true` is for node registration, not user namespace configuration."
"Which of the following Pod fields can be exposed as environment variables using the downward API?

A) metadata.name
B) spec.nodeName
C) status.hostIP
D) metadata.labels",A) metadata.name,"The correct answer is A) metadata.name. This field represents the pod's name and can be exposed as an environment variable via the downward API. Option B) spec.nodeName is not available as an environment variable; it is only accessible via a downward API volume. Option C) status.hostIP is available as an environment variable but not via a downward API volume, so it is incorrect in this context. Option D) metadata.labels can be exposed in a downward API volume but not as environment variables directly."
"Which command correctly creates a downward API volume that exposes all of the pod's labels in a formatted manner?

A) kubectl create volume --name=labels --downward-api --items=metadata.labels
B) kubectl run mypod --image=nginx --overrides='{""spec"":{""volumes"":[{""name"":""labels"",""downwardAPI"":{""items"":[{""path"":""labels"",""fieldRef"":{""fieldPath"":""metadata.labels""}}]}}]}}'
C) kubectl apply -f -
D) No such command; this requires manifest YAML configuration",D) No such command; this requires manifest YAML configuration,"The correct answer is D) No such command; exposing all labels via downward API volume requires defining a volume in the pod's manifest YAML with the downwardAPI volume source and specifying the items. Options A and B are incorrect because 'kubectl create volume' or 'kubectl run' do not support direct creation of downward API volumes with such specifications. Option C) 'kubectl apply -f -' is incomplete without providing a manifest YAML, which is necessary for configuring downward API volumes."
"You want a container to have access to its own resource requests for CPU and memory via the downward API. Which field should you reference?

A) resourceFieldRef with limits.cpu
B) resourceFieldRef with requests.memory
C) resourceFieldRef with requests.cpu
D) resourceFieldRef with spec.nodeName",C) resourceFieldRef with requests.cpu,"The correct answer is C) resourceFieldRef with requests.cpu. This references the CPU request specified for the container, which is accessible via resourceFieldRef. Option A) limits.cpu refers to the CPU limit, not the request. Option B) requests.memory is valid for memory requests but not for CPU. Option D) spec.nodeName is unrelated to resource requests and is not accessible via resourceFieldRef for resources."
"In which Kubernetes version was the feature to resize CPU and memory resources while the container is running introduced as beta?

A) v1.20
B) v1.33
C) v1.25
D) v1.30",B) v1.33,"The correct answer is B) v1.33. According to the content, the feature to resize CPU and memory resources while the container is running was introduced as a beta feature in Kubernetes v1.33. The other options are incorrect as the feature was not introduced in those versions."
"Which command creates a pod with a downward API volume that exposes the pod's annotations, formatted as key=""value"" pairs?

A) kubectl run mypod --image=nginx --overrides='{""spec"":{""volumes"":[{""name"":""annotations"",""downwardAPI"":{""items"":[{""path"":""annotations"",""fieldRef"":{""fieldPath"":""metadata.annotations""}}]}}]}}'
B) kubectl create pod mypod --image=nginx --volume-name=annotations --downward-api
C) kubectl apply -f pod.yaml (where pod.yaml defines the downward API volume for annotations)
D) kubectl expose pod mypod --annotations",C) kubectl apply -f pod.yaml (where pod.yaml defines the downward API volume for annotations),"The correct answer is C) because exposing annotations via downward API volume requires defining a pod manifest YAML that specifies the volume with downwardAPI source and the annotations item, then applying it with kubectl. Option A) 'kubectl run' does not support direct specification of downward API volumes. Option B) 'kubectl create pod' with '--downward-api' is not a valid command; the configuration must be in YAML. Option D) 'kubectl expose' is used for exposing services, not for creating pods with downward API volumes."
"Which field in a Deployment specification defines how the created ReplicaSet finds which Pods to manage?

A) .spec.replicas
B) .spec.selector
C) .spec.template
D) .metadata.labels",B) .spec.selector,"The correct answer is B) .spec.selector. This field specifies how the Deployment's ReplicaSet identifies and manages the Pods it controls, typically by matching labels. Option A) .spec.replicas defines the desired number of Pods but does not influence Pod selection. Option C) .spec.template contains the Pod template but not the selection criteria. Option D) .metadata.labels are labels assigned to Pods but are not used directly to select Pods in this context; instead, the selector matches labels on Pods."
"What does the 'kubectl rollout status deployment/nginx-deployment' command do?

A) Initiates a new rollout of the deployment
B) Checks the current status of the deployment's rollout process
C) Rolls back the deployment to the previous revision
D) Deletes the deployment",B) Checks the current status of the deployment's rollout process,The correct answer is B) Checks the current status of the deployment's rollout process. This command provides real-time updates on whether the deployment has successfully rolled out or is still in progress. Option A) is incorrect because the command does not initiate a rollout. Option C) is incorrect because rolling back is done via 'kubectl rollout undo'. Option D) is incorrect as this command does not delete the deployment.
"What is the purpose of the 'pod-template-hash' label generated by a Deployment?

A) To uniquely identify each Pod
B) To ensure that ReplicaSets do not overlap or manage the same Pods
C) To specify the version of the container image
D) To label Pods for network policies",B) To ensure that ReplicaSets do not overlap or manage the same Pods,"The correct answer is B) To ensure that ReplicaSets do not overlap or manage the same Pods. The 'pod-template-hash' label is automatically generated by the Deployment controller to distinguish between different ReplicaSets and prevent overlapping management of Pods. Option A) is incorrect because the hash is not meant to uniquely identify individual Pods, but rather to differentiate ReplicaSets. Option C) is incorrect because the hash is unrelated to container image versions. Option D) is incorrect because the label is not used for network policies."
"Which command updates the container image of an existing Deployment named 'nginx-deployment' to 'nginx:1.16.1'?

A) kubectl edit deployment nginx-deployment
B) kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1
C) kubectl apply -f deployment.yaml
D) kubectl delete deployment nginx-deployment && kubectl create deployment nginx-deployment --image=nginx:1.16.1",B) kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1,"The correct answer is B) kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1. This command updates the image of the 'nginx' container in the specified Deployment, triggering a rollout of the new image. Option A) allows manual editing but is not the most straightforward method. Option C) applies a YAML file and is valid if the file is updated accordingly, but the specific command given is more direct. Option D) deletes and recreates the Deployment, which is unnecessary and causes downtime."
"Which command would you use to create a Deployment named 'myapp' with 5 replicas using the nginx:1.21 image?

A) kubectl create deployment myapp --image=nginx:1.21 --replicas=5
B) kubectl apply -f myapp.yaml
C) kubectl run myapp --image=nginx:1.21 --replicas=5
D) kubectl scale deployment myapp --replicas=5",A) kubectl create deployment myapp --image=nginx:1.21 --replicas=5,"The correct answer is A) kubectl create deployment myapp --image=nginx:1.21 --replicas=5. This command creates a new Deployment named 'myapp' with the specified image and number of replicas. Option B) applies a YAML file, which could also do this but is not the direct command asked for. Option C) 'kubectl run' in newer Kubernetes versions does not support the '--replicas' flag directly; it's more suitable for single Pod creation. Option D) scales an existing Deployment, but if the Deployment does not exist yet, this command would fail."
"What is the primary purpose of a ReplicaSet in Kubernetes?

A) To provide declarative updates to Pods
B) To maintain a stable set of replica Pods running at any given time
C) To manage rolling updates of Deployments
D) To schedule Pods onto nodes",B) To maintain a stable set of replica Pods running at any given time,"The correct answer is B) To maintain a stable set of replica Pods running at any given time. A ReplicaSet ensures that the specified number of identical Pods are always running. Option A is incorrect because declarative updates to Pods are primarily handled by Deployments, not ReplicaSets themselves. Option C is also incorrect because rolling updates are managed by Deployments, which create and manage ReplicaSets. Option D is incorrect since scheduling Pods onto nodes is handled by the Kubernetes scheduler, not ReplicaSets."
"Which field in a ReplicaSet manifest defines how Pods are selected for management?

A) spec.template
B) metadata.labels
C) spec.selector
D) spec.replicas",C) spec.selector,"The correct answer is C) spec.selector. This field defines the label selector used by the ReplicaSet to identify which Pods it manages. Option A, spec.template, defines the Pod template used to create new Pods but does not select existing Pods. Option B, metadata.labels, applies labels to the ReplicaSet itself but doesn't define selection logic. Option D, spec.replicas, defines the desired number of Pods but does not control selection."
"What happens if you create a bare Pod with labels matching a ReplicaSet's selector but without an ownerReference?

A) The Pod will be ignored by the ReplicaSet
B) The ReplicaSet will immediately acquire the Pod
C) The Pod will be deleted automatically
D) The Pod will be converted into a ReplicaSet",B) The ReplicaSet will immediately acquire the Pod,"The correct answer is B) The ReplicaSet will immediately acquire the Pod. ReplicaSets acquire Pods that match their selector and do not have a controlling ownerReference. Option A is incorrect because such Pods are acquired, not ignored. Option C is incorrect as the Pod is not deleted immediately; it is first acquired. Option D is incorrect since Pods cannot be converted into ReplicaSets."
"When is it recommended to use a ReplicaSet directly instead of a Deployment?

A) When you need declarative updates to Pods
B) When you want to manage rolling updates automatically
C) When you require custom update orchestration or no updates at all
D) When you want to automatically scale Pods based on CPU usage",C) When you require custom update orchestration or no updates at all,"The correct answer is C) When you require custom update orchestration or no updates at all. Deployments provide higher-level features such as declarative updates and rolling updates, so using ReplicaSets directly is usually only recommended for custom update strategies or static workloads. Option A and B describe Deployment features, making them incorrect. Option D is incorrect because automatic scaling is handled by Horizontal Pod Autoscalers, not ReplicaSets."
Create a ReplicaSet named 'frontend' with 3 replicas that selects Pods with label 'tier=frontend' and runs the container image 'us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5'. Use the file 'frontend.yaml' to apply this configuration.,kubectl apply -f frontend.yaml,"This command applies the ReplicaSet manifest defined in 'frontend.yaml'. The manifest should include the ReplicaSet kind, metadata with name 'frontend', spec with replicas set to 3, a selector matching 'tier=frontend', and a pod template specifying the container image 'us-docker.pkg.dev/google-samples/containers/gke/gb-frontend:v5'. Using 'kubectl apply' ensures the resource is created or updated declaratively. It's important that the manifest is correctly formatted as per Kubernetes API conventions."
"After creating a ReplicaSet, which kubectl command would you use to view detailed information including events about the ReplicaSet named 'frontend'?",kubectl describe rs/frontend,"The command 'kubectl describe rs/frontend' shows detailed information about the ReplicaSet named 'frontend', including its labels, selectors, replica counts, Pod template, and recent events such as Pod creations or deletions. This is useful for debugging and verifying state. Using 'kubectl get rs' would only show a summary, while 'kubectl describe' provides richer detail."
"Which of the following features is NOT provided by a StatefulSet in Kubernetes?

A) Stable, unique network identifiers
B) Ordered, graceful deployment and scaling
C) Automatic deletion of PersistentVolumes when scaling down
D) Ordered, automated rolling updates",C) Automatic deletion of PersistentVolumes when scaling down,"The correct answer is C) Automatic deletion of PersistentVolumes when scaling down. StatefulSets do NOT delete the volumes associated with Pods when scaled down or deleted; this is to ensure data safety. Option A is incorrect because StatefulSets provide stable, unique network identities for each Pod. Option B is incorrect because StatefulSets guarantee ordered and graceful deployment and scaling. Option D is incorrect because StatefulSets support ordered, automated rolling updates, but with some caveats requiring manual intervention in certain cases."
"What Kubernetes API object must be created and used alongside a StatefulSet to manage the network identity of its Pods?

A) ClusterIP Service
B) LoadBalancer Service
C) Headless Service
D) Ingress",C) Headless Service,The correct answer is C) Headless Service. StatefulSets require a Headless Service to control the network domain and provide stable network identities for Pods. Option A is incorrect because a ClusterIP Service provides a stable IP but does not support stable network identities for StatefulSet Pods. Option B is incorrect since LoadBalancer Services expose external IPs and do not provide stable identities for StatefulSet Pods. Option D is incorrect as Ingress manages external HTTP routing and is unrelated to Pod network identity.
"Which statement about the Pod identity in a StatefulSet is TRUE?

A) Pods have interchangeable identities with no stable hostname
B) Pods get unique ordinals starting from 1
C) Hostnames are constructed as $(statefulset name)-$(ordinal)
D) Pod identity changes when rescheduled to a different node",C) Hostnames are constructed as $(statefulset name)-$(ordinal),"The correct answer is C) Hostnames are constructed as $(statefulset name)-$(ordinal). StatefulSet Pods receive stable, unique hostnames based on the StatefulSet name and ordinal index starting at 0. Option A is incorrect because Pods have sticky identities and are not interchangeable. Option B is incorrect since ordinals start at 0, not 1. Option D is incorrect because Pod identity, including hostname and storage, remains stable across rescheduling."
"Which field in a StatefulSet spec controls the minimum seconds a newly created Pod should be ready before being considered available during rolling updates?

A) spec.minReadySeconds
B) spec.terminationGracePeriodSeconds
C) spec.progressDeadlineSeconds
D) spec.activeDeadlineSeconds",A) spec.minReadySeconds,"The correct answer is A) spec.minReadySeconds. This optional field specifies how long a Pod must be ready and stable before considered available, helping to manage rollout progression. Option B is incorrect as terminationGracePeriodSeconds controls pod termination timeout, not readiness. Option C is incorrect because progressDeadlineSeconds applies to Deployments, not StatefulSets. Option D is incorrect since activeDeadlineSeconds limits Pod lifetime."
"Create a StatefulSet named 'web' with 3 replicas using the image 'registry.k8s.io/nginx-slim:0.24', a Headless Service named 'nginx', and a volume claim template requesting 1Gi of storage with storage class 'my-storage-class'. Write the kubectl command to create this StatefulSet from a YAML file named 'statefulset.yaml'.",kubectl apply -f statefulset.yaml,"This command applies the StatefulSet configuration defined in 'statefulset.yaml'. The YAML should include the StatefulSet spec with 3 replicas, the container image 'registry.k8s.io/nginx-slim:0.24', the serviceName set to 'nginx' for the Headless Service, and a volumeClaimTemplates section requesting 1Gi storage with storageClassName 'my-storage-class'. Using 'kubectl apply' ensures the resource is created or updated declaratively. Note that the Headless Service 'nginx' must be created separately before or along with this StatefulSet."
How do you scale a StatefulSet named 'web' down to zero replicas to achieve ordered and graceful termination of its Pods prior to deletion?,kubectl scale statefulset web --replicas=0,"This command scales the StatefulSet 'web' down to 0 replicas, causing the controller to terminate all Pods in reverse ordinal order, which achieves ordered and graceful termination. This step is recommended before deleting a StatefulSet to avoid abrupt Pod termination without order. The '--replicas=0' flag sets the desired replicas to zero. Simply deleting the StatefulSet without scaling down does not guarantee ordered termination of Pods."
"What is the primary purpose of a DaemonSet in Kubernetes?

A) To ensure multiple replicas of a Pod run for load balancing
B) To run a copy of a Pod on all or selected nodes
C) To manage batch jobs that run to completion
D) To expose Pods externally via a network service",B) To run a copy of a Pod on all or selected nodes,"The correct answer is B) To run a copy of a Pod on all or selected nodes. A DaemonSet ensures that all or some nodes run a copy of a particular Pod, which is useful for node-local services like log collectors or monitoring agents. Option A is incorrect because DaemonSets do not manage replicas for load balancing; that's the role of ReplicaSets or Deployments. Option C is incorrect because batch jobs are managed by Jobs, not DaemonSets. Option D is incorrect because exposing Pods externally is handled by Services, not DaemonSets."
"Which field in a DaemonSet spec must exactly match the labels in the Pod template metadata?

A) spec.template.spec.nodeSelector
B) spec.selector
C) metadata.labels
D) spec.template.spec.tolerations",B) spec.selector,"The correct answer is B) spec.selector. The spec.selector is a pod selector that must match the labels defined in spec.template.metadata.labels for the DaemonSet to manage its Pods correctly. Option A is incorrect because nodeSelector is used to select nodes for Pod scheduling, not Pod labels. Option C is incorrect as metadata.labels relate to the DaemonSet object itself, not the Pod selector. Option D is incorrect because tolerations affect scheduling but are unrelated to label matching."
"How can you ensure that DaemonSet Pods are scheduled on control-plane nodes that are marked unschedulable by default?

A) Add a nodeSelector for control-plane nodes
B) Add appropriate tolerations to the Pod template
C) Set the Pod's restartPolicy to Always
D) Set spec.template.spec.schedulerName to 'control-plane-scheduler'",B) Add appropriate tolerations to the Pod template,"The correct answer is B) Add appropriate tolerations to the Pod template. Control-plane nodes typically have taints that prevent normal Pods from scheduling; adding tolerations allows DaemonSet Pods to be scheduled on these nodes. Option A is incorrect because nodeSelector alone won't allow Pods to tolerate taints that prevent scheduling. Option C is incorrect as restartPolicy relates to container restart behavior, not scheduling. Option D is incorrect because changing the scheduler does not bypass taints and tolerations."
"Which of the following tolerations are automatically added by the DaemonSet controller to its Pods?

A) node.kubernetes.io/memory-pressure:NoSchedule
B) node.kubernetes.io/unschedulable:NoSchedule
C) node.kubernetes.io/not-ready:NoExecute
D) All of the above",D) All of the above,"The correct answer is D) All of the above. The DaemonSet controller automatically adds tolerations for various node conditions such as memory pressure, unschedulable status, and not-ready status to ensure DaemonSet Pods can run on nodes even under these conditions. Options A, B, and C are individually correct but incomplete on their own."
Create a DaemonSet named 'fluentd-elasticsearch' in the 'kube-system' namespace using the image 'quay.io/fluentd_elasticsearch/fluentd:v5.0.1' and ensure it tolerates control-plane node taints.,"kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
EOF","This command uses 'kubectl apply -f -' with a here-document to create a DaemonSet resource. The YAML defines the DaemonSet named 'fluentd-elasticsearch' in the 'kube-system' namespace, which runs the specified image. The tolerations allow the Pods to be scheduled on control-plane nodes by tolerating their taints. The selector and template labels match, which is required. This approach ensures the DaemonSet pods run on all nodes including control-plane nodes."
How can you create a DaemonSet from a file named 'daemonset.yaml' located at 'https://k8s.io/examples/controllers/daemonset.yaml'?,kubectl apply -f https://k8s.io/examples/controllers/daemonset.yaml,This command uses 'kubectl apply' with the '-f' flag pointing to the URL of the manifest file. Kubernetes will download the YAML file and create or update the DaemonSet resource as defined. Using 'apply' instead of 'create' allows for idempotent operations and updates. This is the standard way to create resources from remote YAML definitions.
"Which restartPolicy values are allowed for Pods created by a Kubernetes Job?

A) Always or OnFailure
B) Never or Always
C) Never or OnFailure
D) Always only",C) Never or OnFailure,The correct answer is C) Never or OnFailure. Kubernetes Jobs require Pods to have a restartPolicy of either Never or OnFailure to ensure that completed Pods are not restarted indefinitely. Option A) Always or OnFailure is incorrect because Always is not allowed for Job Pods. Option B) Never or Always is incorrect since Always is disallowed. Option D) Always only is incorrect as Always is not permitted for Jobs.
"What happens when you delete a Kubernetes Job?

A) The Job is deleted but Pods remain running
B) The Job and all Pods it created are deleted
C) The Job remains but Pods are deleted
D) The Job is suspended until resumed",B) The Job and all Pods it created are deleted,"The correct answer is B) The Job and all Pods it created are deleted. Deleting a Job cleans up its managed Pods, ensuring no orphan Pods are left running. Option A) is incorrect because Pods do not remain after Job deletion. Option C) is incorrect as the Job itself is deleted, not just Pods. Option D) describes suspending a Job, not deleting it."
"Which of the following statements about Kubernetes Job completions is TRUE?

A) A Job completes after its Pods start running
B) A Job completes after the specified number of Pods terminate successfully
C) A Job completes immediately after creation
D) A Job completes once all Pods fail",B) A Job completes after the specified number of Pods terminate successfully,"The correct answer is B) A Job completes after the specified number of Pods terminate successfully. Jobs track successful completions and only complete when the desired count is reached. Option A) is incorrect because Pod start does not mean completion. Option C) is incorrect as Jobs run to completion. Option D) is incorrect since Jobs complete on success, not failure."
"Create a Kubernetes Job named 'pi-calculation' that runs a single Pod using the image 'perl:5.34.0' with the command to compute π to 2000 places, and ensure the Pod never restarts on failure.","kubectl create job pi-calculation --image=perl:5.34.0 --restart=Never -- perl -Mbignum=bpi -wle ""print bpi(2000)""","This command creates a Job named 'pi-calculation' using 'kubectl create job'. The '--image=perl:5.34.0' flag specifies the container image. The '--restart=Never' flag sets the Pod restart policy to Never, which is required for Jobs. The command after '--' specifies the Perl command to calculate π to 2000 digits. This ensures the Job runs once to completion and does not restart Pods indefinitely."
"How can you list the names of all Pods created by a Job named 'pi' using kubectl?

A) kubectl get pods --selector=job-name=pi
B) kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}'
C) kubectl get pods -l job=pi
D) kubectl get pods --field-selector=metadata.name=pi",B) kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}',"The correct answer is B) kubectl get pods --selector=batch.kubernetes.io/job-name=pi --output=jsonpath='{.items[*].metadata.name}'. This command filters Pods by the label 'batch.kubernetes.io/job-name=pi', which is automatically added to Pods created by the Job. The jsonpath expression extracts the Pod names in a machine-readable format. Option A) is incorrect because 'job-name=pi' is not the correct label key. Option C) is incorrect as 'job=pi' is not the label used by Jobs. Option D) uses field-selector incorrectly; Pods are not selected by metadata.name=pi."
Suspend a Kubernetes Job named 'batch-job' and explain what happens to its active Pods.,"kubectl patch job batch-job -p '{""spec"":{""suspend"":true}}'","This command patches the Job resource named 'batch-job' to set spec.suspend to true, effectively suspending the Job. Suspending a Job deletes its active Pods to prevent further execution until resumed. The JSON patch string '{""spec"":{""suspend"":true}}' updates the suspend field. Suspending is useful to temporarily stop Job execution without deleting the Job resource. Note that when the Job is resumed (spec.suspend set to false), it will recreate Pods as needed."
"What is the purpose of the .spec.ttlSecondsAfterFinished field in a Kubernetes Job manifest?

A) To specify how long the Job controller should retry failed Jobs
B) To define the time-to-live duration before automatically deleting finished Jobs
C) To set the maximum execution time allowed for the Job
D) To specify the interval between Job status updates",B) To define the time-to-live duration before automatically deleting finished Jobs,The correct answer is B) To define the time-to-live duration before automatically deleting finished Jobs. The .spec.ttlSecondsAfterFinished field is used by the TTL-after-finished controller to specify how many seconds after a Job has finished (either Complete or Failed) that the Job object is eligible for automatic cleanup. Option A is incorrect because TTL does not control retries for failed Jobs. Option C is incorrect as TTL does not limit Job execution time but controls lifetime after completion. Option D is incorrect because TTL is unrelated to status update intervals.
"Which of the following statements about the TTL-after-finished controller in Kubernetes is TRUE?

A) It cleans up Jobs only if they have completed successfully
B) Once a Job becomes eligible for cleanup, it is deleted along with its dependent objects
C) The TTL value must be specified at Job creation time and cannot be changed later
D) Cleanup happens immediately after the Job status changes to Complete or Failed","B) Once a Job becomes eligible for cleanup, it is deleted along with its dependent objects","Option B is correct because the TTL-after-finished controller deletes the Job cascadingly, meaning it removes the Job and its dependent resources (like Pods). Option A is incorrect since the controller cleans up Jobs that are either Complete or Failed. Option C is incorrect because the TTL value can be set or modified even after Job creation or completion. Option D is wrong because cleanup happens only after the specified TTL period expires, not immediately upon Job completion."
"What potential issue should be considered when using the TTL-after-finished feature in a Kubernetes cluster?

A) The feature cannot clean up Failed Jobs
B) The TTL-after-finished controller ignores finalizers on Jobs
C) Time skew in the cluster can cause premature or delayed cleanup
D) Jobs with TTL set are never deleted automatically",C) Time skew in the cluster can cause premature or delayed cleanup,"The correct answer is C) Time skew in the cluster can cause premature or delayed cleanup. Because TTL-after-finished relies on timestamps to determine when to delete Jobs, any clock differences between cluster components can affect when Jobs are cleaned up. Option A is incorrect because the feature cleans up both Complete and Failed Jobs. Option B is wrong because Kubernetes honors finalizers on Jobs before deletion. Option D is false as Jobs with TTL set are intended to be deleted automatically after the TTL expires."
"Which of the following is NOT a recommended way to set the TTL for finished Jobs in Kubernetes?

A) Specify .spec.ttlSecondsAfterFinished in the Job manifest before creation
B) Use a mutating admission webhook to dynamically set TTL at Job creation
C) Manually update the .spec.ttlSecondsAfterFinished field on finished Jobs
D) Set the TTL on Pod templates inside the Job to control Job cleanup",D) Set the TTL on Pod templates inside the Job to control Job cleanup,"Option D is correct because TTL for cleanup is controlled at the Job object level via .spec.ttlSecondsAfterFinished, not on Pod templates. Options A, B, and C are all valid ways to set or modify the TTL for Jobs, either before creation, dynamically during creation via webhooks, or after completion by manual update."
"Create a Job named 'backup-job' that runs the image 'busybox' with the command 'sleep 30', and configure it to be automatically deleted 60 seconds after it finishes.","kubectl create job backup-job --image=busybox -- sleep 30
kubectl patch job backup-job -p '{""spec"": {""ttlSecondsAfterFinished"": 60}}'","The first command creates a Job named 'backup-job' using the 'busybox' image and runs the command 'sleep 30'. The second command patches the Job to set the .spec.ttlSecondsAfterFinished field to 60 seconds, which instructs Kubernetes to automatically clean up the Job 60 seconds after it completes or fails. The patch uses a JSON string with the TTL field. This two-step process is necessary because 'kubectl create job' does not directly support setting ttlSecondsAfterFinished at creation."
Update an existing finished Job named 'data-import' to set its TTL so it is deleted 120 seconds after finishing.,"kubectl patch job data-import -p '{""spec"": {""ttlSecondsAfterFinished"": 120}}'","This command updates the existing Job 'data-import' by patching its .spec.ttlSecondsAfterFinished field to 120 seconds. This makes the Job eligible for automatic cleanup 2 minutes after it has finished execution. The command uses the 'kubectl patch' with a JSON patch payload to modify the TTL field. This approach works both for Jobs that are running and already finished. Note that if the previous TTL has already expired, Kubernetes does not guarantee the Job will be retained even if TTL is extended."
"What is the default concurrency policy of a Kubernetes CronJob?

A) Forbid
B) Replace
C) Allow
D) Suspend",C) Allow,"The correct answer is C) Allow. By default, a CronJob allows concurrent executions of Jobs, meaning multiple Jobs created by the same CronJob can run simultaneously. Option A) Forbid is incorrect because it prevents concurrent Job runs by skipping new Jobs if the previous one is still running. Option B) Replace is incorrect as it cancels the currently running Job to start a new one instead. Option D) Suspend is not a concurrency policy; it's a separate field to suspend all future Jobs."
"Which field in a CronJob spec specifies how many successful finished Jobs to keep?

A) .spec.failedJobsHistoryLimit
B) .spec.successfulJobsHistoryLimit
C) .spec.startingDeadlineSeconds
D) .spec.suspend",B) .spec.successfulJobsHistoryLimit,"The correct answer is B) .spec.successfulJobsHistoryLimit. This field defines how many successful finished Jobs the CronJob controller keeps for history; the default is 3. Option A) .spec.failedJobsHistoryLimit is for controlling how many failed Jobs are kept, not successful ones. Option C) .spec.startingDeadlineSeconds defines the deadline for starting a missed Job, unrelated to history. Option D) .spec.suspend is used to suspend execution of all future Jobs, not to control history."
"When specifying the schedule for a CronJob, which of the following is NOT supported?

A) Standard Cron syntax (e.g., ""0 3 * * 1"")
B) Macros like @daily or @weekly
C) Timezone specification using CRON_TZ or TZ variables inside .spec.schedule
D) Step values like */2 in the schedule fields",C) Timezone specification using CRON_TZ or TZ variables inside .spec.schedule,"The correct answer is C) Timezone specification using CRON_TZ or TZ variables inside .spec.schedule. This is not officially supported and should not be used. Instead, the .spec.timeZone field should be used starting from Kubernetes v1.27 to specify the timezone. Option A) is supported as standard cron syntax is required. Option B) is supported as macros like @daily and @weekly are allowed. Option D) is also supported; step values like */2 are valid cron syntax."
Create a CronJob named 'backup' in the default namespace that runs a Job every day at midnight to execute a container using the busybox image which runs the command 'echo Backup started'. The Job should not allow concurrent runs.,"kubectl create cronjob backup --schedule=""0 0 * * *"" --image=busybox -- /bin/sh -c ""echo Backup started"" --concurrency-policy=Forbid","This command creates a CronJob named 'backup' that runs daily at midnight ('0 0 * * *'). The '--image=busybox' sets the container image, and the command '/bin/sh -c ""echo Backup started""' specifies what the container runs. The '--concurrency-policy=Forbid' flag ensures that if the previous Job is still running, new Jobs will be skipped to prevent concurrent runs. Note that the command uses 'kubectl create cronjob' which supports passing the schedule and image directly, and the command is appended after the image. This is a convenient way to create a simple CronJob from the command line."
"Which of the following statements about the .spec.startingDeadlineSeconds field in a CronJob is TRUE?

A) It defines how long a Job is allowed to run before being terminated.
B) It defines a deadline for starting a missed Job; if the Job can't start within the deadline, it is skipped.
C) It suspends the CronJob for the specified number of seconds.
D) It sets how many seconds to wait before retrying a failed Job.","B) It defines a deadline for starting a missed Job; if the Job can't start within the deadline, it is skipped.","The correct answer is B). The .spec.startingDeadlineSeconds field sets a deadline (in seconds) for starting a Job if it misses its scheduled time for any reason. If the Job cannot be started within this deadline, it is skipped and treated as failed. Option A) is incorrect because it doesn't control how long a Job runs. Option C) is incorrect as suspension is controlled by the .spec.suspend field, not this one. Option D) is incorrect because retry behavior is managed differently, not by startingDeadlineSeconds."
"Suspend a CronJob named 'report' in the default namespace so that no new Jobs are started, but keep existing Jobs running.","kubectl patch cronjob report -p '{""spec"":{""suspend"":true}}'","This command patches the CronJob named 'report' by setting its .spec.suspend field to true. This suspends future Job executions without affecting any Jobs that are already running, which will continue to completion. The '-p' flag allows inline JSON patching of the resource. This is the standard way to suspend a CronJob without deleting or recreating it. Note that this suspension only affects new Jobs; existing Jobs are not terminated."
"What is the primary function of a ReplicationController in Kubernetes?

A) To manage container networking
B) To ensure a specified number of pod replicas are running at any time
C) To provide load balancing between pods
D) To manage persistent storage volumes",B) To ensure a specified number of pod replicas are running at any time,"The correct answer is B) To ensure a specified number of pod replicas are running at any time. A ReplicationController monitors the number of pods matching a selector and ensures the desired number is maintained by creating or deleting pods as needed. Option A) is incorrect because networking is handled by other components like Services or CNI plugins. Option C) is incorrect as load balancing is provided by Services, not ReplicationControllers. Option D) is incorrect because persistent storage is managed by PersistentVolume and PersistentVolumeClaim resources."
"Which of the following is a reason to prefer Deployments over ReplicationControllers in modern Kubernetes setups?

A) Deployments are used for managing StatefulSets
B) Deployments configure ReplicaSets and provide advanced rollout features
C) Deployments do not support scaling pods
D) ReplicationControllers have better API support than Deployments",B) Deployments configure ReplicaSets and provide advanced rollout features,The correct answer is B) Deployments configure ReplicaSets and provide advanced rollout features like rolling updates and rollbacks. Deployments are the recommended way to manage replicated pods due to these advanced capabilities. Option A) is incorrect because StatefulSets are a different controller type for stateful applications. Option C) is incorrect since Deployments fully support scaling pods. Option D) is incorrect because ReplicationControllers are legacy and have been largely superseded by Deployments and ReplicaSets.
"When defining a ReplicationController, which statement about the .spec.selector and .spec.template.metadata.labels fields is true?

A) .spec.selector must be different from .spec.template.metadata.labels
B) .spec.selector must be equal to .spec.template.metadata.labels or the API will reject the object
C) .spec.selector is optional and unrelated to .spec.template.metadata.labels
D) .spec.template.metadata.labels must be omitted when using a selector",B) .spec.selector must be equal to .spec.template.metadata.labels or the API will reject the object,The correct answer is B) .spec.selector must be equal to .spec.template.metadata.labels or the API will reject the object. This ensures the ReplicationController can correctly identify and manage its pods. Option A) is incorrect because having differing labels and selector will cause rejection by the API. Option C) is incorrect as the selector is required and tied to the pod labels. Option D) is incorrect because the pod template labels are necessary for pod identification and must be specified.
"How can you delete a ReplicationController but keep its pods running in Kubernetes?

A) kubectl delete replicationcontroller <name>
B) kubectl delete replicationcontroller <name> --cascade=orphan
C) kubectl delete pods --selector=<label>
D) kubectl scale replicationcontroller <name> --replicas=0",B) kubectl delete replicationcontroller <name> --cascade=orphan,"The correct answer is B) kubectl delete replicationcontroller <name> --cascade=orphan. This deletes the ReplicationController object but leaves the pods it manages running by orphaning them (removing the owner reference). Option A) deletes both the ReplicationController and its pods. Option C) deletes pods directly but does not delete the ReplicationController. Option D) scales the ReplicationController's replicas to zero, deleting the pods but not the controller itself."
"Create a ReplicationController named 'nginx-rc' with 3 replicas running the 'nginx' image, exposing container port 80, using a label selector 'app=nginx'.",kubectl run nginx-rc --image=nginx --replicas=3 --port=80 --restart=Always --labels=app=nginx --dry-run=client -o yaml | kubectl apply -f -,"This command uses 'kubectl run' to generate a ReplicationController manifest with the specified parameters: name 'nginx-rc', 3 replicas, image 'nginx', container port 80, and label 'app=nginx'. The '--restart=Always' flag ensures a ReplicationController (instead of a Pod) is created. '--dry-run=client -o yaml' outputs the manifest without applying it immediately, and piping it to 'kubectl apply -f -' creates the resource. Note that newer Kubernetes versions recommend Deployments, but this command still creates a ReplicationController for learning purposes."
"What will happen if multiple ReplicationControllers have overlapping selectors managing pods with the same labels?

A) Kubernetes automatically merges the controllers
B) Only one ReplicationController will manage all pods
C) Both controllers will manage the pods and may interfere with each other
D) The API server will reject one of the ReplicationControllers",C) Both controllers will manage the pods and may interfere with each other,"The correct answer is C) Both controllers will manage the pods and may interfere with each other. Kubernetes does not prevent overlapping selectors, so multiple ReplicationControllers can simultaneously try to manage the same pods, leading to conflicts and unpredictable behavior. Option A) is incorrect because Kubernetes does not merge controllers. Option B) is incorrect since both controllers act independently. Option D) is incorrect because the API server allows overlapping selectors but it is not recommended."
"Which Kubernetes resource is used for automatic horizontal scaling of workloads based on observed resource utilization?

A) VerticalPodAutoscaler (VPA)
B) HorizontalPodAutoscaler (HPA)
C) Cluster Proportional Autoscaler
D) CronScaler",B) HorizontalPodAutoscaler (HPA),"The correct answer is B) HorizontalPodAutoscaler (HPA). The HPA automatically adjusts the number of replicas in a workload based on metrics such as CPU or memory usage. Option A) VerticalPodAutoscaler (VPA) is used for vertical scaling, i.e., adjusting resource requests for pods, not horizontal scaling. Option C) Cluster Proportional Autoscaler scales workloads based on cluster size, not resource usage. Option D) CronScaler is a feature within KEDA used to schedule scaling actions based on time, not automatic scaling based on resource utilization."
"Which of the following statements about VerticalPodAutoscaler (VPA) in Kubernetes is TRUE?

A) VPA is included by default in all Kubernetes clusters.
B) VPA scales the number of pod replicas automatically.
C) VPA requires the Metrics Server to be installed.
D) VPA supports in-place pod resizing since Kubernetes v1.25.",C) VPA requires the Metrics Server to be installed.,"The correct answer is C) VPA requires the Metrics Server to be installed. The VPA needs metrics to recommend resource adjustments, and Metrics Server provides that data. Option A) is incorrect because VPA is not included by default in Kubernetes; it is a separate project that must be installed. Option B) is incorrect because VPA adjusts resource requests (vertical scaling), not the number of replicas (horizontal scaling). Option D) is incorrect because in-place pod resizing is still in beta as of Kubernetes v1.33 and was not available in v1.25."
"What mode of the VerticalPodAutoscaler assigns resource requests only during pod creation and never updates running pods?

A) Auto
B) Recreate
C) Initial
D) Off",C) Initial,"The correct answer is C) Initial. In this mode, the VPA sets resource requests only when pods are created and does not modify resources later. Option A) Auto currently behaves like Recreate, evicting pods to update resources. Option B) Recreate updates resource requests by evicting pods when recommendations change significantly. Option D) Off disables automatic updates; recommendations are provided but not applied."
Create a HorizontalPodAutoscaler named 'webapp-hpa' for a Deployment 'webapp' that scales between 2 and 10 replicas targeting 50% average CPU utilization.,kubectl autoscale deployment webapp --min=2 --max=10 --cpu-percent=50 --name=webapp-hpa,"This command creates a HorizontalPodAutoscaler resource named 'webapp-hpa' for the 'webapp' Deployment. The '--min=2' flag sets the minimum number of pod replicas, '--max=10' sets the maximum number, and '--cpu-percent=50' configures the target average CPU utilization at 50%. The '--name=webapp-hpa' explicitly names the HPA resource. This enables automatic horizontal scaling based on CPU load."
"Which autoscaler would you use to scale Kubernetes system components like cluster-dns based on the number of schedulable nodes?

A) HorizontalPodAutoscaler
B) VerticalPodAutoscaler
C) Cluster Proportional Autoscaler
D) Kubernetes Event Driven Autoscaler (KEDA)",C) Cluster Proportional Autoscaler,"The correct answer is C) Cluster Proportional Autoscaler. This autoscaler adjusts the number of replicas of system components based on the cluster size, such as the number of nodes and cores. Option A) HorizontalPodAutoscaler scales workloads based on resource utilization, not cluster size. Option B) VerticalPodAutoscaler adjusts resource requests for pods, not replicas. Option D) KEDA scales workloads based on external event sources, not cluster node count."
"Using kubectl, how would you manually scale a Deployment named 'api-server' to 5 replicas?",kubectl scale deployment api-server --replicas=5,This command manually scales the 'api-server' Deployment to have 5 replicas. The 'kubectl scale' command is used to change the replica count of scalable resources like Deployments. The '--replicas=5' flag specifies the desired number of pod replicas. This is a manual horizontal scaling operation and does not require editing the deployment manifest directly.
"When applying multiple Kubernetes resource manifests from a single YAML file separated by '---', in what order are the resources created?

A) Resources are created in alphabetical order by kind
B) Resources are created in the order they appear in the manifest file
C) Resources are created randomly
D) Resources are created in reverse order of appearance",B) Resources are created in the order they appear in the manifest file,"The correct answer is B) Resources are created in the order they appear in the manifest file. This is important because resource dependencies can require a certain creation order, like creating a Service before the Deployment it selects. Option A is incorrect as Kubernetes does not sort resources alphabetically when applying YAML files. Option C is incorrect because resource creation order is deterministic, following file order. Option D is incorrect because the order is not reversed."
"What is the recommended practice for organizing Kubernetes resource manifests for a microservice or application tier?

A) Put all unrelated resources in the same file
B) Group all resources related to the same microservice or tier into the same file
C) Use separate namespaces for each resource regardless of relation
D) Store all manifests in a single flat directory without subdirectories",B) Group all resources related to the same microservice or tier into the same file,"The correct answer is B) Group all resources related to the same microservice or tier into the same file. This simplifies deployment and management of related components together. Option A is incorrect because unrelated resources should not be mixed, as it complicates management. Option C is incorrect because namespaces are unrelated to file organization and used for resource isolation. Option D is incorrect as organizing manifests in directories and subdirectories helps maintain clarity and separation."
"Which kubectl flag allows recursive operations on resource manifests within subdirectories when applying or deleting resources?

A) --recursive or -R
B) --all
C) --subdir
D) --deep",A) --recursive or -R,"The correct answer is A) --recursive or -R. This flag instructs kubectl to process all resources in the specified directory and its subdirectories, enabling bulk operations on nested manifests. Option B (--all) is used for selecting all resources of a type but not recursive file traversal. Option C (--subdir) and Option D (--deep) are not valid kubectl flags."
What kubectl command would you use to delete all Deployments and Services labeled with 'app=nginx' in the current namespace?,"kubectl delete deployment,services -l app=nginx",This command uses 'kubectl delete' to remove resources. The resource types 'deployment' and 'services' are specified together separated by a comma. The '-l app=nginx' flag selects resources with the label 'app=nginx'. This is an efficient way to delete multiple resource types filtered by label selectors. It avoids deleting unrelated resources.
"Which of the following statements about updating a Deployment's container image using kubectl is correct?

A) You must delete the Deployment and create a new one with the updated image
B) Updating the Deployment's spec.template.spec.containers[0].image triggers a rolling update
C) You cannot update the image of a Deployment once created
D) You need to scale down replicas to zero before updating the image",B) Updating the Deployment's spec.template.spec.containers[0].image triggers a rolling update,"The correct answer is B) Updating the Deployment's spec.template.spec.containers[0].image triggers a rolling update. Changing the container image in the pod template causes the Deployment controller to progressively replace old Pods with new ones using the updated image, ensuring zero downtime. Option A is incorrect because you do not need to delete the Deployment to update it. Option C is incorrect as image updates are fully supported. Option D is incorrect because scaling to zero is not required and would cause downtime."
"Create a Deployment named 'my-nginx' with 3 replicas using the nginx:1.14.2 image and expose it via a LoadBalancer Service on port 80, using a single YAML file named 'nginx-app.yaml'. Assume the Service should be defined first in the file. What kubectl command applies this configuration from a URL?",kubectl apply -f https://k8s.io/examples/application/nginx-app.yaml,"This command applies the manifest file located at the specified URL, which contains both the Service and Deployment resources separated by '---'. The Service is defined first to ensure proper scheduling of pods behind the Service as they are created. Using 'kubectl apply' allows declarative management of the resources. The '-f' flag specifies the file or URL to use. This is the recommended way to deploy multiple related resources together."
"What is the primary purpose of a Kubernetes Service?

A) To schedule Pods across nodes
B) To expose a set of Pods as a network service with a stable IP and DNS name
C) To store configuration data for Pods
D) To manage Pod lifecycle and restarts",B) To expose a set of Pods as a network service with a stable IP and DNS name,"The correct answer is B) To expose a set of Pods as a network service with a stable IP and DNS name. A Kubernetes Service abstracts a dynamic set of Pods by providing a stable network endpoint, allowing clients to reliably access backend Pods even as they are created or destroyed. Option A) is incorrect because scheduling Pods is handled by the Kubernetes scheduler, not Services. Option C) is incorrect as storing configuration data is the responsibility of ConfigMaps or Secrets. Option D) is incorrect because Pod lifecycle and restarts are managed by controllers like Deployments or ReplicaSets, not Services."
"Which of the following is true about the selector field in a Kubernetes Service?

A) It is mandatory for all Services
B) It determines the set of Pods targeted by the Service
C) It specifies the IP address of the Service
D) It defines the protocol used by the Service",B) It determines the set of Pods targeted by the Service,"The correct answer is B) It determines the set of Pods targeted by the Service. The selector field is used by the Service to identify which Pods are part of its backend by matching labels. Option A) is incorrect because Services can be created without selectors, for example when manually defining endpoints or external backends. Option C) is incorrect because the Service IP is assigned by Kubernetes and not specified by the selector. Option D) is incorrect since the protocol is defined under the ports section, not by the selector."
"When you create a Service without a selector, what must you do to associate backend endpoints?

A) Nothing, Kubernetes automatically associates Pods
B) Manually create EndpointSlice resources with the backend addresses
C) Use a Deployment to create the backend Pods
D) Set the targetPort to 0",B) Manually create EndpointSlice resources with the backend addresses,"The correct answer is B) Manually create EndpointSlice resources with the backend addresses. Without a selector, Kubernetes does not automatically associate Pods, so you must manually define EndpointSlices to map the Service to backend endpoints, which can be internal or external. Option A) is incorrect because automatic association happens only if a selector is specified. Option C) is unrelated because Deployments create Pods but don't link them to selectorless Services automatically. Option D) is invalid as targetPort 0 is not a meaningful configuration."
"Which of the following statements about targetPort in a Service definition is correct?

A) targetPort must always be a numeric port value
B) targetPort defaults to the same value as port if not specified
C) targetPort is the port exposed by the Service to clients
D) targetPort is only used for UDP protocols",B) targetPort defaults to the same value as port if not specified,"The correct answer is B) targetPort defaults to the same value as port if not specified. This means if you define a Service port without explicitly specifying targetPort, it assumes the backend Pod port matches the Service port. Option A) is wrong because targetPort can be a named port (string) defined in the Pod. Option C) is incorrect because port is the Service's exposed port; targetPort is the port on the Pod the Service forwards traffic to. Option D) is false; targetPort applies to all protocols, not just UDP."
"Create a ClusterIP Service named 'my-service' that selects Pods with label app=MyApp, exposes port 80, and forwards traffic to targetPort 9376.",kubectl expose pod -l app=MyApp --name=my-service --port=80 --target-port=9376 --type=ClusterIP,"This command creates a Service of type ClusterIP named 'my-service'. The '-l app=MyApp' flag selects Pods with the label app=MyApp. '--port=80' defines the port exposed by the Service, and '--target-port=9376' forwards traffic to port 9376 on the selected Pods. '--type=ClusterIP' explicitly sets the Service type to ClusterIP, which is also the default. Using 'kubectl expose pod' is appropriate here for selecting Pods by label and creating a Service targeting them."
You want to create a Service named 'external-db' with no selector that exposes TCP port 5432 and maps to targetPort 5432. How do you create this Service using kubectl? Provide the command.,kubectl create service clusterip external-db --tcp=5432:5432 --dry-run=client -o yaml > external-db.yaml && kubectl apply -f external-db.yaml,"This command sequence first creates a Service manifest named 'external-db' of type ClusterIP exposing TCP port 5432 and forwarding to targetPort 5432. The '--dry-run=client' and '-o yaml' flags output the manifest to stdout without creating the Service immediately. This output is redirected to 'external-db.yaml' for inspection or manual editing. Since no selector is specified, the Service will have no selector by default. The second command applies the manifest to create the Service. This approach is recommended because 'kubectl create service' does not provide an option to omit selectors directly, so generating a YAML file allows manual removal of the selector or leaving it unset."
"What is the primary purpose of a Kubernetes Ingress resource?

A) To expose arbitrary TCP and UDP ports externally
B) To expose HTTP and HTTPS routes from outside the cluster to Services within the cluster
C) To manage internal pod-to-pod communication within the cluster
D) To create persistent storage volumes for stateful applications",B) To expose HTTP and HTTPS routes from outside the cluster to Services within the cluster,"The correct answer is B) To expose HTTP and HTTPS routes from outside the cluster to Services within the cluster. An Ingress resource manages external access to HTTP(S) services inside a cluster by defining rules for routing based on hostnames and paths. Option A is incorrect because Ingress does not expose arbitrary TCP or UDP ports; those require Service types like NodePort or LoadBalancer. Option C is incorrect as pod-to-pod communication is handled by the cluster network and Services, not Ingress. Option D is incorrect because persistent storage is managed by PersistentVolume and PersistentVolumeClaim resources, not Ingress."
"Which of the following is true about the Ingress controller in Kubernetes?

A) Creating an Ingress resource automatically deploys an Ingress controller
B) An Ingress controller is optional and not required for Ingress resources to work
C) An Ingress controller fulfills the Ingress resource, typically implementing load balancing and routing
D) Ingress controllers expose arbitrary protocols beyond HTTP and HTTPS","C) An Ingress controller fulfills the Ingress resource, typically implementing load balancing and routing","The correct answer is C) An Ingress controller fulfills the Ingress resource, typically implementing load balancing and routing. An Ingress resource by itself has no effect; it requires an Ingress controller to process the rules and configure the underlying proxy or load balancer. Option A is incorrect because creating an Ingress resource does not deploy the controller—you must deploy it separately. Option B is incorrect; the controller is required for Ingress to function. Option D is incorrect because Ingress only supports HTTP and HTTPS protocols; exposing other protocols requires different Service types."
"In an Ingress resource, what does the 'pathType: Prefix' signify?

A) The path matches exactly and case sensitively
B) The path is matched based on a case-insensitive prefix
C) The path is matched based on a case-sensitive prefix, element-wise split by '/'
D) The path matching behavior is implementation specific and varies by controller","C) The path is matched based on a case-sensitive prefix, element-wise split by '/'","The correct answer is C) The path is matched based on a case-sensitive prefix, element-wise split by '/'. With 'Prefix' pathType, the request path must start with the specified path prefix, matching each path segment case-sensitively. Option A describes the 'Exact' pathType, which requires exact case-sensitive matching. Option B is incorrect because matching is case-sensitive, not insensitive. Option D describes 'ImplementationSpecific' pathType, where the matching behavior depends on the Ingress controller implementation."
"Which statement about the 'defaultBackend' in an Ingress resource is correct?

A) It must always be specified in every Ingress resource
B) It handles requests that do not match any of the specified rules in the Ingress
C) It allows routing traffic to arbitrary TCP/UDP services
D) It can be specified in the Ingress manifest as a backend service for unmatched requests",B) It handles requests that do not match any of the specified rules in the Ingress,"The correct answer is B) It handles requests that do not match any of the specified rules in the Ingress. The defaultBackend is typically configured at the Ingress controller level to serve any unmatched traffic. Option A is incorrect because you do not have to specify defaultBackend in every Ingress resource; sometimes it's configured globally by the controller. Option C is incorrect as Ingress only supports HTTP(S) traffic, not arbitrary TCP/UDP. Option D is incorrect because defaultBackend is usually configured in the controller or as a separate field and is not commonly specified inside the rules in the Ingress manifest."
Create an Ingress resource named 'example-ingress' in the 'default' namespace that routes HTTP traffic with host 'foo.bar.com' and path '/app' to Service 'my-service' on port 8080. Use the 'nginx' ingress class.,"kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-ingress
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /app
        pathType: Prefix
        backend:
          service:
            name: my-service
            port:
              number: 8080
EOF","This command uses 'kubectl apply -f -' to create an Ingress resource from the inline YAML manifest. The manifest defines an Ingress named 'example-ingress' in the 'default' namespace, specifying 'nginx' as the ingressClassName to associate it with the NGINX Ingress controller. The rules section matches HTTP traffic for host 'foo.bar.com' and path '/app' with pathType 'Prefix' (meaning any path starting with '/app'). The backend directs matching requests to the Service named 'my-service' on port 8080. Using 'kubectl apply -f -' allows creating resources from stdin, which is useful for quick resource definitions without separate files."
How would you view detailed information and events related to an existing Ingress resource named 'minimal-ingress' in the current namespace?,kubectl describe ingress minimal-ingress,"The 'kubectl describe ingress minimal-ingress' command outputs detailed information about the Ingress resource named 'minimal-ingress', including its specification, status, associated rules, backend services, annotations, and recent events related to the Ingress. This is useful for debugging and understanding how the Ingress is configured and whether any issues occurred during processing by the Ingress controller. The command targets the current namespace by default unless overridden with the '-n' flag."
"Which statement accurately describes the relationship between an Ingress resource and an Ingress controller in Kubernetes?

A) An Ingress resource automatically works without any controller running in the cluster.
B) An Ingress controller is optional if you use the default kube-controller-manager.
C) An Ingress resource requires an Ingress controller running in the cluster to function.
D) Ingress controllers are started automatically when creating a cluster.",C) An Ingress resource requires an Ingress controller running in the cluster to function.,"The correct answer is C. In Kubernetes, an Ingress resource defines routing rules, but it needs an Ingress controller to implement these rules and route traffic accordingly. Option A is incorrect because an Ingress resource alone does not provide functionality without a controller. Option B is incorrect because the kube-controller-manager does not start or manage Ingress controllers. Option D is incorrect as Ingress controllers are not started automatically with the cluster; they need to be deployed separately."
"How do you specify which Ingress controller should handle an Ingress resource when multiple Ingress controllers are deployed in a cluster?

A) Use the annotation 'kubernetes.io/ingress.class' on the Ingress resource.
B) Use the field 'ingressClassName' in the Ingress resource spec.
C) Label the Ingress resource with the controller's name.
D) You cannot specify this; the first controller installed handles all Ingress resources.",B) Use the field 'ingressClassName' in the Ingress resource spec.,"The correct answer is B. The 'ingressClassName' field in the Ingress spec explicitly specifies which IngressClass (and thus which Ingress controller) should handle the resource. Option A is outdated; the annotation 'kubernetes.io/ingress.class' was used before but is replaced by 'ingressClassName'. Option C is incorrect as labels do not control which controller manages the Ingress. Option D is wrong because Kubernetes allows multiple controllers, and you must specify which one to use if there is more than one."
"What annotation must be set on an IngressClass resource to mark it as the default IngressClass in a Kubernetes cluster?

A) ingressclass.kubernetes.io/is-default-class with value ""true""
B) kubernetes.io/ingress.class with value ""default""
C) ingress.kubernetes.io/default-class with value ""yes""
D) default.ingressclass.kubernetes.io with value ""true""","A) ingressclass.kubernetes.io/is-default-class with value ""true""","The correct answer is A. To mark an IngressClass as the default, you set the annotation 'ingressclass.kubernetes.io/is-default-class' to the string value ""true"". Option B is incorrect because the 'kubernetes.io/ingress.class' annotation is deprecated for this purpose. Option C and D are invalid annotations not recognized by Kubernetes."
Create an Ingress resource named 'web-ingress' in the 'production' namespace that uses the IngressClass named 'nginx-controller' to route traffic for host 'example.com' to a service named 'web-service' on port 80.,"kubectl apply -f - <<EOF
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-ingress
  namespace: production
spec:
  ingressClassName: nginx-controller
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-service
            port:
              number: 80
EOF","This command uses 'kubectl apply -f -' to create an Ingress resource from a YAML manifest supplied via standard input. The Ingress is named 'web-ingress' in the 'production' namespace. The 'ingressClassName: nginx-controller' field specifies that this Ingress should be handled by the Ingress controller associated with the 'nginx-controller' IngressClass. The rule defines HTTP routing for the host 'example.com', forwarding all requests with path prefix '/' to the 'web-service' on port 80. Using the manifest allows precise configuration of complex objects like Ingress."
"Which of the following ingress controllers is maintained by the Kubernetes project itself?

A) Traefik
B) NGINX
C) HAProxy
D) Citrix",B) NGINX,"The correct answer is B. Kubernetes officially supports and maintains the NGINX ingress controller along with AWS and GCE ingress controllers. Option A (Traefik), C (HAProxy), and D (Citrix) are third-party ingress controllers not maintained by the Kubernetes project. They are widely used but maintained by their respective communities or vendors."
Deploy the official NGINX Ingress controller in the 'ingress-nginx' namespace using the default deployment manifest from the Kubernetes community repository.,kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml,"This command deploys the official NGINX Ingress controller using the manifest file hosted in the Kubernetes ingress-nginx GitHub repository. The manifest creates all necessary resources, including the 'ingress-nginx' namespace, controller Deployment, Service, ConfigMaps, and RBAC configurations. Using 'kubectl apply -f' fetches and applies the manifest directly. This is the recommended way to install the NGINX ingress controller in cloud environments."
"Which Kubernetes Gateway API kind is responsible for defining the controller that manages a set of Gateways with common configuration?

A) Gateway
B) HTTPRoute
C) GatewayClass
D) Service",C) GatewayClass,"The correct answer is C) GatewayClass. GatewayClass defines a set of gateways with common configuration and specifies the controller that manages these gateways. Option A) Gateway defines an instance of traffic handling infrastructure but does not define the controller. Option B) HTTPRoute defines HTTP-specific routing rules, not the controller. Option D) Service is unrelated to Gateway API classes and is a core Kubernetes resource for exposing pods."
"In Gateway API, which resource specifies HTTP request routing rules from a Gateway listener to backend network endpoints?

A) GatewayClass
B) HTTPRoute
C) Gateway
D) Ingress",B) HTTPRoute,"The correct answer is B) HTTPRoute. HTTPRoute defines the HTTP-specific rules for routing traffic from a Gateway listener to backend endpoints such as Services. Option A) GatewayClass defines the controller and common configuration for gateways, not routing rules. Option C) Gateway defines the traffic handling infrastructure instance but not the routing rules themselves. Option D) Ingress is the older Kubernetes resource replaced by Gateway API and is not part of Gateway API kinds."
"What is the relationship between a Gateway and GatewayClass in Gateway API?

A) A Gateway can reference multiple GatewayClasses.
B) A GatewayClass is always associated with one Gateway.
C) A Gateway must reference exactly one GatewayClass.
D) A GatewayClass is optional for creating a Gateway.",C) A Gateway must reference exactly one GatewayClass.,The correct answer is C) A Gateway must reference exactly one GatewayClass to specify the controller managing it. Option A) is incorrect because a Gateway cannot reference multiple GatewayClasses. Option B) is wrong since a GatewayClass can be associated with multiple Gateways. Option D) is incorrect because GatewayClass reference is mandatory for a Gateway.
"Which of the following is NOT a design principle of Gateway API?

A) Role-oriented
B) Portable
C) Immutable
D) Extensible",C) Immutable,"The correct answer is C) Immutable. Gateway API design principles include role-oriented, portable, expressive, and extensible, but immutability is not listed as one of them. Option A) Role-oriented is correct as the API models roles managing networking. Option B) Portable refers to the API's support across multiple implementations. Option D) Extensible refers to the ability to link custom resources for granular customization."
Create a Gateway resource named 'my-gateway' that uses the GatewayClass 'example-class' and listens for HTTP traffic on port 8080.,"kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: my-gateway
spec:
  gatewayClassName: example-class
  listeners:
  - name: http
    protocol: HTTP
    port: 8080
EOF",This command applies a Gateway resource manifest using 'kubectl apply -f -' with an inline YAML document. The Gateway named 'my-gateway' references the GatewayClass 'example-class' and defines a listener named 'http' that listens on port 8080 for HTTP traffic. This sets up the traffic handling infrastructure instance accordingly. Using 'kubectl apply' allows the resource to be created or updated idempotently.
"Define an HTTPRoute named 'login-route' that routes HTTP requests with the Host header 'app.example.com' and path prefix '/login' to the Service 'login-service' on port 9090, attached to the Gateway 'my-gateway'.","kubectl apply -f - <<EOF
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: login-route
spec:
  parentRefs:
  - name: my-gateway
  hostnames:
  - ""app.example.com""
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /login
    backendRefs:
    - name: login-service
      port: 9090
EOF","This command creates an HTTPRoute resource named 'login-route' that attaches to the Gateway 'my-gateway' via 'parentRefs'. It specifies matching HTTP requests with Host header 'app.example.com' and a path prefix '/login', routing them to the backend Service 'login-service' on port 9090. Using 'kubectl apply -f -' with inline YAML allows defining complex custom resources interactively or in scripts."
"What is the primary purpose of the EndpointSlice API in Kubernetes?

A) To provide a way to track network endpoints and efficiently scale Services
B) To replace the Pod API for managing container lifecycles
C) To store persistent volume claims for network storage
D) To manage node resource allocation and scheduling",A) To provide a way to track network endpoints and efficiently scale Services,"The correct answer is A) To provide a way to track network endpoints and efficiently scale Services. EndpointSlices store references to network endpoints and help the control plane manage large numbers of backends efficiently. Option B is incorrect because EndpointSlices do not replace the Pod API or manage container lifecycles. Option C is incorrect since persistent volume claims relate to storage, not network endpoints. Option D is incorrect because node resource allocation and scheduling are handled by other components like the scheduler, not EndpointSlices."
"Which of the following statements about EndpointSlices is TRUE?

A) One EndpointSlice object can contain endpoints of both IPv4 and IPv6 address types.
B) The control plane creates EndpointSlices with a maximum of 100 endpoints by default.
C) EndpointSlices cannot be managed by any entity other than the endpoint slice controller.
D) EndpointSlices do not store any topology or node information about endpoints.",B) The control plane creates EndpointSlices with a maximum of 100 endpoints by default.,"B is correct because by default, EndpointSlices have no more than 100 endpoints each, which can be configured up to 1000 via the kube-controller-manager flag. Option A is incorrect; each EndpointSlice represents a single IP address type (IPv4 or IPv6), so a Service with both will have multiple EndpointSlices. Option C is incorrect because other entities can manage EndpointSlices by using the endpointslice.kubernetes.io/managed-by label to avoid conflicts. Option D is incorrect as EndpointSlices store topology information such as nodeName and zone per endpoint."
"What does the 'ready' condition in an EndpointSlice indicate?

A) The endpoint is terminating and should not receive traffic.
B) The endpoint is serving traffic and is not terminating.
C) The endpoint is not ready to serve any requests.
D) The endpoint is in maintenance mode.",B) The endpoint is serving traffic and is not terminating.,"The correct answer is B. The 'ready' condition is essentially a shorthand for the endpoint being both 'serving' and not 'terminating', meaning it is healthy and should receive traffic. Option A is incorrect because 'terminating' endpoints are usually excluded from traffic. Option C is wrong since 'ready' means the endpoint is ready, not the opposite. Option D is unrelated to the EndpointSlice conditions."
"Which label does the endpoint slice controller set on all EndpointSlices it manages to indicate ownership?

A) kubernetes.io/service-name
B) endpointslice.kubernetes.io/managed-by = endpointslice-controller.k8s.io
C) endpointslice.kubernetes.io/owner = kube-controller
D) kubernetes.io/endpoint-owner = service-controller",B) endpointslice.kubernetes.io/managed-by = endpointslice-controller.k8s.io,Option B is correct because the endpoint slice controller uses the label endpointslice.kubernetes.io/managed-by with the value endpointslice-controller.k8s.io to indicate it manages those EndpointSlices. Option A is a label indicating which Service an EndpointSlice belongs to but does not specify the managing controller. Options C and D are incorrect as those labels do not exist in Kubernetes.
"Create an EndpointSlice named 'custom-slice' in the default namespace with IPv4 address type, containing a TCP port named 'http' on port 8080.","kubectl apply -f - <<EOF
apiVersion: discovery.k8s.io/v1
kind: EndpointSlice
metadata:
  name: custom-slice
  namespace: default
addressType: IPv4
ports:
- name: http
  protocol: TCP
  port: 8080
endpoints: []
EOF",This command uses 'kubectl apply -f -' to create an EndpointSlice from a YAML manifest provided inline. The metadata.name sets the EndpointSlice name to 'custom-slice' in the default namespace. The addressType is set to IPv4 as required. The ports section defines a TCP port named 'http' on port 8080. The endpoints list is empty but could be populated with endpoint addresses. This approach is used because EndpointSlice is a custom resource and can't be created with a simple 'kubectl create' command.
How can you configure the maximum number of endpoints per EndpointSlice to 500 in a Kubernetes cluster?,Modify the kube-controller-manager startup flags to include --max-endpoints-per-slice=500 and restart the kube-controller-manager component.,"The kube-controller-manager flag --max-endpoints-per-slice controls how many endpoints each EndpointSlice can contain, with a default of 100 and a maximum allowed of 1000. To set it to 500, you must update the kube-controller-manager's manifest or systemd service configuration to include this flag and then restart the component for the change to take effect. There is no kubectl command to modify this setting dynamically because it is a controller manager configuration."
"Which of the following statements about Kubernetes NetworkPolicies is TRUE?

A) NetworkPolicies can be enforced even if the cluster network plugin does not support them.
B) NetworkPolicies control traffic flow at OSI layer 7 (application layer).
C) A NetworkPolicy applies only if the pod matches the podSelector and specifies the relevant policyTypes.
D) NetworkPolicies can block traffic to and from the node where a Pod is running.",C) A NetworkPolicy applies only if the pod matches the podSelector and specifies the relevant policyTypes.,"The correct answer is C. NetworkPolicies apply to pods selected by the podSelector and only affect traffic directions specified in policyTypes (Ingress, Egress, or both). Option A is incorrect because NetworkPolicies require a network plugin that supports them; otherwise, they have no effect. Option B is wrong because NetworkPolicies work at OSI layers 3 and 4 (IP address and port level), not layer 7. Option D is incorrect because traffic to and from the node where a pod is running is always allowed, regardless of NetworkPolicies."
"When is a Pod considered to be isolated for egress traffic in Kubernetes NetworkPolicies?

A) When any NetworkPolicy in the cluster has egress rules.
B) When there is a NetworkPolicy selecting the Pod with ""Egress"" in policyTypes.
C) When no NetworkPolicy applies to the Pod.
D) When the Pod's namespace has no labels.","B) When there is a NetworkPolicy selecting the Pod with ""Egress"" in policyTypes.","B is correct because a Pod is isolated for egress only if there exists a NetworkPolicy that selects the Pod and includes ""Egress"" in its policyTypes. Option A is incorrect, as NetworkPolicies only apply to Pods they select, not cluster-wide. Option C is wrong because without any NetworkPolicy selecting a Pod for egress, the Pod is non-isolated and all outbound traffic is allowed by default. Option D is irrelevant, as namespace labels do not determine egress isolation."
"Which statement accurately describes how multiple NetworkPolicies affect traffic to a Pod?

A) NetworkPolicies are evaluated in order, and the first matching policy decides the traffic.
B) NetworkPolicies conflict and the most restrictive policy applies.
C) NetworkPolicies are additive; allowed connections are the union of all applicable policies.
D) Only one NetworkPolicy can be applied to a Pod at a time.",C) NetworkPolicies are additive; allowed connections are the union of all applicable policies.,C is correct because all NetworkPolicies applied to a Pod combine their allowed ingress or egress rules additively (union). Option A is incorrect as NetworkPolicies are not ordered and evaluation order does not affect results. Option B is wrong since policies do not conflict; they do not restrict traffic beyond what is allowed by any policy. Option D is incorrect because multiple NetworkPolicies can apply simultaneously to a Pod.
"Which of the following is NOT a valid selector type used in the 'from' or 'to' sections of a NetworkPolicy?

A) podSelector
B) namespaceSelector
C) ipBlock
D) serviceSelector",D) serviceSelector,"D is correct because serviceSelector is not a valid selector type in NetworkPolicy specifications. Valid selectors include podSelector (selects pods in the same namespace), namespaceSelector (selects namespaces), and ipBlock (CIDR IP ranges). Options A, B, and C are valid selectors used to specify allowed sources or destinations in ingress or egress rules."
Create a NetworkPolicy named 'deny-all-egress' in the 'production' namespace that denies all egress traffic from Pods labeled 'app=web'.,kubectl create networkpolicy deny-all-egress --namespace=production --pod-selector=app=web --policy-types=Egress,"This command creates a NetworkPolicy named 'deny-all-egress' in the 'production' namespace targeting Pods with label 'app=web'. The '--policy-types=Egress' flag specifies that the policy applies to egress traffic. Since no egress rules are specified, this policy denies all egress traffic from selected pods (pods become isolated for egress). The '--pod-selector=app=web' flag selects the pods to apply the policy to. This is a concise way to create a deny-all egress NetworkPolicy using kubectl."
Write a kubectl command to apply a NetworkPolicy manifest file named 'networkpolicy.yaml' in the 'default' namespace.,kubectl apply -f networkpolicy.yaml -n default,This command applies the NetworkPolicy resource defined in 'networkpolicy.yaml' to the 'default' namespace. The '-f' flag specifies the filename of the manifest. The '-n default' flag explicitly sets the namespace to 'default' where the NetworkPolicy will be created or updated. Applying a NetworkPolicy manifest requires that the cluster network plugin supports NetworkPolicy enforcement for the changes to take effect.
"Which DNS record type does Kubernetes assign to a normal (non-headless) Service for service discovery within the cluster?

A) CNAME records
B) A and/or AAAA records
C) PTR records
D) TXT records",B) A and/or AAAA records,"The correct answer is B) A and/or AAAA records. Kubernetes assigns A (IPv4) and AAAA (IPv6) DNS records to normal Services, resolving the Service name to its cluster IP address. Option A) CNAME records are not assigned to Services by Kubernetes. Option C) PTR records are used for reverse DNS lookups and are not assigned to Services. Option D) TXT records are used for arbitrary text strings and are not used for Service discovery in Kubernetes."
"For a Pod named 'busybox1' with spec.hostname set to 'busybox-1' and spec.subdomain set to 'busybox-subdomain' in the namespace 'my-namespace', what is the expected fully qualified domain name (FQDN) of the Pod?

A) busybox1.my-namespace.svc.cluster.local
B) busybox-1.busybox-subdomain.my-namespace.svc.cluster.local
C) busybox-subdomain.busybox-1.my-namespace.svc.cluster.local
D) busybox-1.my-namespace.pod.cluster.local",B) busybox-1.busybox-subdomain.my-namespace.svc.cluster.local,"The correct answer is B. When a Pod has spec.hostname set to 'busybox-1' and spec.subdomain set to 'busybox-subdomain', its FQDN is formatted as '<hostname>.<subdomain>.<namespace>.svc.cluster.local'. Option A is incorrect because it uses the Pod name without the subdomain. Option C reverses hostname and subdomain, which is incorrect. Option D uses the old Pod DNS naming convention and omits the subdomain, so it is also incorrect."
"A Pod in the 'test' namespace wants to resolve a Service named 'data' in the 'prod' namespace. Which DNS query should the Pod use?

A) data
B) data.test
C) data.prod
D) data.svc",C) data.prod,"The correct answer is C) data.prod. DNS queries that do not specify a namespace resolve within the Pod's own namespace ('test' in this case), so querying just 'data' returns no result. Specifying 'data.prod' allows the query to resolve the Service named 'data' in the 'prod' namespace. Option A is incorrect because it only queries the local namespace. Option B queries 'data' in the 'test' namespace, which is incorrect. Option D is incomplete and does not specify the namespace, so it fails to resolve the Service in 'prod'."
"What happens if you set 'setHostnameAsFQDN: true' in a Pod spec and the resulting FQDN exceeds 64 characters?

A) The Pod starts normally with the truncated hostname
B) The Pod fails to start and remains in Pending state
C) The Pod starts but hostname commands return an error
D) Kubernetes automatically shortens the FQDN to 64 characters",B) The Pod fails to start and remains in Pending state,"The correct answer is B. The Linux kernel's hostname field is limited to 64 characters. If 'setHostnameAsFQDN: true' causes the FQDN to exceed this length, the Pod fails to start and remains in the Pending (ContainerCreating) state, emitting error events about the FQDN being too long. Option A is incorrect because the Pod does not start normally with truncation. Option C is incorrect because hostname commands do not return errors inside running Pods—they fail to start. Option D is incorrect because Kubernetes does not automatically shorten the FQDN."
"Which of the following statements about headless Services in Kubernetes DNS is correct?

A) They have a ClusterIP assigned like normal Services
B) Their DNS A/AAAA record resolves to the cluster IP
C) Their DNS A/AAAA record resolves to the IPs of all Pods selected by the Service
D) They do not have any DNS records",C) Their DNS A/AAAA record resolves to the IPs of all Pods selected by the Service,"The correct answer is C. Headless Services (with clusterIP: None) do not have a cluster IP; instead, their DNS A and AAAA records resolve directly to the set of IP addresses of the Pods backing the Service. Option A is incorrect because headless Services have no cluster IP. Option B is incorrect because they do not resolve to a cluster IP. Option D is incorrect as headless Services do have DNS records that point to Pod IPs."
Create a headless Service named 'busybox-subdomain' in the 'default' namespace that selects Pods with label 'name=busybox' and exposes port 1234 named 'foo'.,"kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: busybox-subdomain
  namespace: default
spec:
  clusterIP: None
  selector:
    name: busybox
  ports:
  - name: foo
    port: 1234
EOF","This command uses 'kubectl apply -f -' to create a headless Service from the YAML manifest provided via a here-document. Setting 'clusterIP: None' makes the Service headless, which means no cluster IP is assigned and DNS resolves to Pod IPs. The selector matches Pods labeled with 'name=busybox'. The port 1234 is exposed with the name 'foo', enabling SRV records creation. Specifying the namespace as 'default' ensures the Service is created in the correct namespace."
"Which of the following is required to enable IPv4/IPv6 dual-stack networking in a Kubernetes cluster?

A) Kubernetes version 1.18 or later
B) A network plugin that supports dual-stack networking
C) Only cloud provider support is needed, no configuration changes required
D) kube-proxy must be disabled",B) A network plugin that supports dual-stack networking,"The correct answer is B) A network plugin that supports dual-stack networking. Dual-stack support requires the network plugin to handle both IPv4 and IPv6 addressing. Option A is incorrect because dual-stack is supported from Kubernetes 1.20 or later, not 1.18. Option C is incorrect because besides cloud provider support, explicit configuration in kube-apiserver, kube-controller-manager, kube-proxy, and kubelet is required to enable dual-stack. Option D is incorrect since kube-proxy is needed and must be configured with the dual-stack cluster CIDRs."
"How do you configure the kube-apiserver to support dual-stack service IP ranges?

A) --service-cluster-ip-range=<IPv4 CIDR>
B) --service-cluster-ip-range=<IPv6 CIDR>
C) --service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>
D) --cluster-cidr=<IPv4 CIDR>,<IPv6 CIDR>","C) --service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>","The correct answer is C) --service-cluster-ip-range=<IPv4 CIDR>,<IPv6 CIDR>. This flag on the kube-apiserver configures the service IP address ranges for both IPv4 and IPv6, enabling dual-stack services. Option A and B are incorrect because specifying only one CIDR sets single-stack networking. Option D is incorrect because --cluster-cidr is a flag used by kube-controller-manager and kube-proxy, not kube-apiserver."
"What is the effect of setting the Service field .spec.ipFamilyPolicy to 'RequireDualStack'?

A) Allocates only an IPv4 cluster IP for the Service
B) Allocates both IPv4 and IPv6 cluster IPs and fails if dual-stack is not enabled
C) Allocates only an IPv6 cluster IP for the Service
D) Allocates an IP from either IPv4 or IPv6 range, whichever is available",B) Allocates both IPv4 and IPv6 cluster IPs and fails if dual-stack is not enabled,"The correct answer is B) Allocates both IPv4 and IPv6 cluster IPs and fails if dual-stack is not enabled. 'RequireDualStack' ensures the Service must have both IP families assigned, and prevents creation if dual-stack is unsupported. Option A and C are incorrect because those would be single-stack IP allocations. Option D is incorrect because 'RequireDualStack' does not fallback to either IP family; it strictly requires both."
"Which of the following describes the behavior when you create a Service without specifying .spec.ipFamilyPolicy on a dual-stack enabled cluster?

A) The Service will be assigned both IPv4 and IPv6 addresses by default
B) The Service will default to SingleStack using the first service cluster IP range
C) Service creation will fail unless ipFamilyPolicy is specified
D) The Service will randomly choose IPv4 or IPv6 address family",B) The Service will default to SingleStack using the first service cluster IP range,"The correct answer is B) The Service will default to SingleStack using the first service cluster IP range. If .spec.ipFamilyPolicy is not set, Kubernetes assigns the Service a single IP from the first configured service cluster IP range. Option A is incorrect because dual IP assignment requires explicit ipFamilyPolicy settings. Option C is incorrect as ipFamilyPolicy is optional. Option D is incorrect because the selection is deterministic, not random."
Create a ClusterIP Service named 'web-service' in the default namespace that prefers dual-stack IP allocation with IPv6 as the primary IP family and IPv4 as secondary. The Service should target pods with label app=web on port 8080 TCP.,"kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  ipFamilyPolicy: PreferDualStack
  ipFamilies:
  - IPv6
  - IPv4
  selector:
    app: web
  ports:
  - protocol: TCP
    port: 8080
EOF","This kubectl command uses 'apply -f -' to create a Service manifest from standard input. The Service is named 'web-service' with 'PreferDualStack' in .spec.ipFamilyPolicy to allocate both IPv6 and IPv4 addresses if dual-stack is enabled. The ipFamilies array sets IPv6 as primary and IPv4 as secondary, which also controls the .spec.clusterIP selection. Selector matches pods labeled 'app=web', and port 8080 TCP is exposed. This approach ensures the Service uses dual-stack with IPv6 preference."
"Using kubectl, how can you inspect the IP families and ipFamilyPolicy of an existing Service named 'my-service'?",kubectl get svc my-service -o yaml,"This command outputs the full YAML manifest of the Service 'my-service' including the .spec.ipFamilies and .spec.ipFamilyPolicy fields. Viewing the YAML allows you to confirm whether the Service is single-stack or dual-stack and which IP families are configured. Alternative commands like 'kubectl describe svc my-service' may not show detailed ipFamilyPolicy information, so the YAML output is preferred for this inspection."
"What annotation is used to enable Topology Aware Routing on a Kubernetes Service starting with version 1.27?

A) service.kubernetes.io/topology-aware-hints
B) service.kubernetes.io/topology-mode
C) service.kubernetes.io/internal-traffic-policy
D) service.kubernetes.io/zone-preference",B) service.kubernetes.io/topology-mode,"The correct answer is B) service.kubernetes.io/topology-mode. Starting from Kubernetes 1.27, this annotation is used with the value 'Auto' to enable Topology Aware Routing on a Service. Option A) service.kubernetes.io/topology-aware-hints was used prior to version 1.27 and is deprecated. Option C) service.kubernetes.io/internal-traffic-policy is unrelated; it controls whether traffic is routed only to local endpoints. Option D) service.kubernetes.io/zone-preference is not a valid Kubernetes annotation."
"Which of the following conditions would prevent the EndpointSlice controller from assigning topology aware hints?

A) All nodes have topology.kubernetes.io/zone labels and report allocatable CPU
B) There are fewer endpoints than zones in the cluster
C) The Service has more than 3 endpoints per zone
D) All endpoints have zone hints assigned",B) There are fewer endpoints than zones in the cluster,"The correct answer is B) There are fewer endpoints than zones in the cluster. When there are less endpoints than zones, the controller will not assign any topology aware hints. Option A is incorrect because having node labels and CPU info is required for hints to be assigned. Option C is incorrect since having 3 or more endpoints per zone is recommended for the feature to work well. Option D is incorrect because if all endpoints have zone hints, hints can be properly assigned."
"What is a primary benefit of enabling Topology Aware Routing in a multi-zone Kubernetes cluster?

A) It ensures traffic is always routed to the control plane nodes
B) It reduces network latency by preferring endpoints in the same zone
C) It disables load balancing across zones
D) It enforces all traffic to use only one zone's endpoints",B) It reduces network latency by preferring endpoints in the same zone,"The correct answer is B) It reduces network latency by preferring endpoints in the same zone. Topology Aware Routing adjusts routing to favor endpoints that are topologically closer, improving reliability and performance. Option A is incorrect because traffic is routed to workload endpoints, not control plane nodes. Option C is incorrect; it does not disable cross-zone load balancing but prefers same-zone endpoints when possible. Option D is incorrect as traffic can still be routed cross-zone if needed."
Create a Kubernetes Service named 'web-service' of type ClusterIP which enables Topology Aware Routing using the appropriate annotation.,kubectl create service clusterip web-service --tcp=80:80 && kubectl annotate service web-service service.kubernetes.io/topology-mode=Auto --overwrite,"The first command creates a ClusterIP Service named 'web-service' exposing TCP port 80. The second command adds the annotation 'service.kubernetes.io/topology-mode=Auto' to the Service, enabling Topology Aware Routing. The '--overwrite' flag ensures the annotation is set even if it exists previously. This combination enables the EndpointSlice controller and kube-proxy to route traffic preferring same-zone endpoints."
"Which scenario is NOT recommended for enabling Topology Aware Routing on a Service?

A) Incoming traffic evenly distributed across zones
B) Service with 3 or more endpoints per zone
C) Large proportion of traffic originating from a single zone
D) Multi-zone cluster with allocatable CPU info available for nodes",C) Large proportion of traffic originating from a single zone,The correct answer is C) Large proportion of traffic originating from a single zone. This can overload endpoints allocated to that zone and is not recommended. Option A is recommended because even traffic distribution helps the feature work effectively. Option B is recommended as having 3+ endpoints per zone improves balanced allocation. Option D is necessary since the EndpointSlice controller bases allocations on allocatable CPU of nodes in zones.
"What happens if kube-proxy cannot find at least one endpoint with a zone hint matching the zone it is running in?

A) It routes traffic only to endpoints in other zones
B) It disables the Service
C) It falls back to using endpoints from all zones
D) It returns an error and drops traffic",C) It falls back to using endpoints from all zones,"The correct answer is C) It falls back to using endpoints from all zones. If kube-proxy does not find an endpoint hint targeting the zone it runs in, it defaults to cluster-wide routing to avoid losing connectivity. Option A is incorrect because it does not restrict routing to other zones only. Option B is incorrect as the Service remains available. Option D is incorrect as kube-proxy does not drop traffic but falls back gracefully."
"Which Windows networking mode in Kubernetes uses VXLAN encapsulation and allows IP address reuse across different overlay networks?

A) L2bridge
B) Overlay
C) Transparent
D) NAT",B) Overlay,The correct answer is B) Overlay. The Overlay network mode uses VXLAN encapsulation to create isolated virtual container networks and allows IP addresses to be reused across different overlay networks by using VNID tags. Option A) L2bridge attaches containers directly to an external vSwitch but rewrites MAC addresses and does not use VXLAN. Option C) Transparent mode is used with ovn-kubernetes and uses GENEVE or STT tunneling instead of VXLAN. Option D) NAT mode connects containers to an internal vSwitch and rewrites MAC and IP addresses but does not use encapsulation or support overlay networks.
"Which Kubernetes Service type is supported on Windows nodes and allows external access via a static port on each node?

A) ExternalName
B) ClusterIP
C) NodePort
D) Headless Service",C) NodePort,"The correct answer is C) NodePort. NodePort exposes the Service on a static port on each node, including Windows nodes, allowing external access to pods. Option A) ExternalName maps a service to an external DNS name but does not expose ports on nodes. Option B) ClusterIP is internal-only and does not expose ports externally. Option D) Headless Service (a ClusterIP service with no cluster IP) is for direct pod access and is not a Service type supported directly on Windows for external access."
"What component on Windows manages virtual networks, vNICs, namespaces, and policies such as NAT and load balancing for containers?

A) Hyper-V Manager
B) Host Compute Service (HCS)
C) Host Networking Service (HNS)
D) Windows Firewall",C) Host Networking Service (HNS),"The correct answer is C) Host Networking Service (HNS). HNS is responsible for managing networking resources like virtual networks, vNICs, namespaces, NAT rules, and load balancing for Windows containers. Option A) Hyper-V Manager manages virtual machines but not container networking directly. Option B) Host Compute Service (HCS) manages container lifecycle and compute but not networking. Option D) Windows Firewall manages network security but not container networking resources."
Create a Kubernetes Service of type LoadBalancer named 'win-lb' in the namespace 'test' that targets pods with the label 'app=web' on port 80.,kubectl create service loadbalancer win-lb --tcp=80:80 -n test --selector=app=web,This command creates a LoadBalancer Service named 'win-lb' in the 'test' namespace. The '--tcp=80:80' flag maps port 80 of the Service to port 80 on the pods. The '--selector=app=web' flag selects pods with the label 'app=web' as the backend endpoints. Using 'kubectl create service loadbalancer' is the simplest way to create a Service of type LoadBalancer with the specified parameters.
"Which IP Address Management (IPAM) plugin is specific to Azure CNI on Windows nodes?

A) host-local
B) azure-vnet-ipam
C) Windows Server IPAM
D) none",B) azure-vnet-ipam,The correct answer is B) azure-vnet-ipam. This IPAM plugin is specific to the Azure CNI plugin on Windows and integrates with Azure Virtual Network IP address management. Option A) host-local is a generic IPAM plugin used on Windows and Linux but not specific to Azure. Option C) Windows Server IPAM is a fallback IPAM option on Windows if no other IPAM is set. Option D) none is incorrect as there are IPAM plugins supported.
Enable Direct Server Return (DSR) feature in kube-proxy on Windows Server 2019 with Kubernetes v1.33.,kube-proxy --enable-dsr=true,This command runs kube-proxy with the flag '--enable-dsr=true' to enable the Direct Server Return (DSR) feature on Windows Server 2019 with Kubernetes v1.33. DSR allows service traffic source IP preservation and improves performance by letting return traffic bypass the load balancer. It's a beta feature that requires explicit enablement via this flag. Note that kube-proxy must be running with this configuration on Windows nodes to use DSR.
"Which of the following is true about the allocation of Service ClusterIP addresses in Kubernetes?

A) ClusterIP addresses are always statically assigned by the user.
B) The control plane dynamically assigns ClusterIP addresses from the configured range unless a static IP is specified.
C) ClusterIP addresses can be duplicated across different Services in the cluster.
D) Services of type ClusterIP do not require a virtual IP address.",B) The control plane dynamically assigns ClusterIP addresses from the configured range unless a static IP is specified.,"Option B is correct because Kubernetes assigns ClusterIP addresses dynamically from the configured service IP range unless a user explicitly specifies a static IP within that range. Option A is incorrect as static assignment is optional, not mandatory. Option C is incorrect since every ClusterIP must be unique cluster-wide, and duplication causes errors. Option D is incorrect because ClusterIP Services specifically provide a cluster-scoped virtual IP to expose applications."
"Why might you want to statically assign a ClusterIP to a Service in Kubernetes?

A) To enable the Service to handle external traffic.
B) To ensure a Service has a well-known, reserved IP address within the cluster.
C) To prevent the Service from being load balanced.
D) To allow the Service to automatically scale with IP allocation.","B) To ensure a Service has a well-known, reserved IP address within the cluster.","Option B is correct because static ClusterIP assignment is used to reserve IP addresses for Services that need to be reliably addressed, such as the cluster DNS Service. Option A is incorrect as external traffic handling is related to Service type LoadBalancer or NodePort, not static IP assignment. Option C is incorrect since load balancing behavior is independent of static IP assignment. Option D is incorrect because scaling does not affect or require static IP allocation."
"How does Kubernetes reduce the risk of ClusterIP address conflicts when using static IP assignments?

A) By disabling dynamic allocation when static IPs are used.
B) By dividing the ClusterIP range into two bands and using the upper band for dynamic allocation.
C) By allowing IP address reuse after 24 hours.
D) By reserving the entire IP range exclusively for static IPs.",B) By dividing the ClusterIP range into two bands and using the upper band for dynamic allocation.,"Option B is correct because Kubernetes splits the ClusterIP range into a static band (lower range) and a dynamic band (upper range), allocating dynamic IPs from the upper band by default. This separation reduces collision risks for statically assigned IPs in the lower band. Option A is incorrect as dynamic allocation is not disabled by static IP usage. Option C is incorrect because IP reuse timing is not the mechanism used. Option D is incorrect as the entire IP range is not reserved solely for static IPs."
"Given a Kubernetes cluster with the Service CIDR 10.96.0.0/24, what is the size of the static IP allocation band according to Kubernetes' allocation strategy?

A) 254 addresses
B) 16 addresses
C) 238 addresses
D) 256 addresses",B) 16 addresses,"Option B is correct. For the 10.96.0.0/24 CIDR, Kubernetes calculates the static IP band size as min(max(16, 256/16), 256) = min(16,256) = 16 addresses. Option A (254) is the total usable IPs in the /24 minus network and broadcast addresses. Option C (238) corresponds to the dynamic band size, not the static. Option D (256) is the total number of IPs in the /24 block including network and broadcast addresses."
"Create a Service named 'custom-dns' in the 'kube-system' namespace with a static ClusterIP of 10.96.0.10, exposing UDP and TCP ports 53 targeting port 53 on Pods labeled 'k8s-app=kube-dns'. The Service type should be ClusterIP.",kubectl create service clusterip custom-dns --tcp=53:53 --udp=53:53 --cluster-ip=10.96.0.10 -n kube-system --dry-run=client -o yaml | kubectl label -f - k8s-app=kube-dns --local -o yaml | kubectl apply -f -,"This command creates a ClusterIP Service named 'custom-dns' in the 'kube-system' namespace with the static IP 10.96.0.10. The '--tcp=53:53' and '--udp=53:53' flags expose port 53 for both protocols. '--cluster-ip=10.96.0.10' assigns the static IP. The dry-run with output to YAML and label application simulates adding the selector 'k8s-app=kube-dns' since 'kubectl create service' does not support the selector flag directly. The final 'kubectl apply -f -' submits the properly labeled Service manifest. Note: In practice, creating such a Service with correct selectors is often done via a full YAML manifest for clarity."
Which command lists all Services along with their ClusterIP addresses in the default namespace?,kubectl get svc -o wide,"The 'kubectl get svc' command lists Services in the current namespace (default if none specified). The '-o wide' option adds extra columns including the ClusterIP address. This allows quick viewing of Service names, types, ClusterIP, and ports. Alternative commands like 'kubectl get svc -o jsonpath' could extract ClusterIPs but are more complex. The '-o wide' is the simplest method."
"What is the effect of setting the Service's .spec.internalTrafficPolicy field to Local in Kubernetes?

A) It routes traffic only to endpoints within the same node as the originating Pod.
B) It routes traffic only to external endpoints outside the cluster.
C) It disables kube-proxy from routing traffic.
D) It routes traffic to all endpoints in the cluster regardless of node locality.",A) It routes traffic only to endpoints within the same node as the originating Pod.,"The correct answer is A) It routes traffic only to endpoints within the same node as the originating Pod. Setting .spec.internalTrafficPolicy to Local instructs kube-proxy to restrict routing for cluster internal traffic to node-local endpoints, improving performance and reducing network overhead. Option B is incorrect because internalTrafficPolicy does not affect external endpoints; it only manages cluster internal traffic. Option C is wrong since kube-proxy still performs routing but filters endpoints based on node locality. Option D is the default behavior when internalTrafficPolicy is set to Cluster or unset, not when it is set to Local."
"Which Kubernetes version introduced the stable feature of Service Internal Traffic Policy?

A) v1.20
B) v1.22
C) v1.26
D) v1.28",C) v1.26,The correct answer is C) v1.26. The Service Internal Traffic Policy feature reached stable status in Kubernetes version 1.26. Option A and B are incorrect as the feature was not stable or introduced in those versions. Option D is incorrect since v1.28 is a future version beyond the stable introduction point.
"What happens if a Pod is on a node that has no endpoints for a Service with internalTrafficPolicy set to Local?

A) The Pod can still access endpoints on other nodes.
B) The Service acts as if it has zero endpoints for that Pod's node.
C) The Pod receives traffic load-balanced across all cluster endpoints.
D) The Service forwards traffic externally.",B) The Service acts as if it has zero endpoints for that Pod's node.,"The correct answer is B) The Service acts as if it has zero endpoints for that Pod's node. When internalTrafficPolicy is Local, kube-proxy only considers node-local endpoints. If none exist on the Pod's node, the Service behaves as if no endpoints are available for that node, meaning traffic is not routed elsewhere. Option A is incorrect because it contradicts the behavior of internalTrafficPolicy set to Local. Option C is the default when internalTrafficPolicy is Cluster or unset. Option D is incorrect as this setting does not cause forwarding of traffic outside the cluster."
"Create a ClusterIP Service named 'my-service' exposing TCP port 80 targeting port 9376, selecting Pods with label 'app.kubernetes.io/name=MyApp', and enforcing internal traffic policy to local node endpoints.","kubectl apply -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
  - protocol: TCP
    port: 80
    targetPort: 9376
  internalTrafficPolicy: Local
EOF",This command uses 'kubectl apply -f -' with a here-document to create a Service resource. The Service is named 'my-service' with a selector matching Pods labeled 'app.kubernetes.io/name=MyApp'. It exposes port 80 (TCP) and forwards traffic to targetPort 9376 on selected Pods. The key field 'internalTrafficPolicy: Local' ensures kube-proxy routes cluster internal traffic only to endpoints local to the node where the request originates. This reduces cross-node network hops for internal Pod communication. Using 'apply' is appropriate for declarative management.
"Which of the following is NOT a benefit of enabling Service Internal Traffic Policy with the Local setting?

A) Reduced network latency for intra-node Pod communication.
B) Improved reliability by avoiding cluster network hops.
C) Ability to route external client traffic only to local node endpoints.
D) Potential cost savings by limiting intra-cluster traffic to nodes.",C) Ability to route external client traffic only to local node endpoints.,"The correct answer is C) Ability to route external client traffic only to local node endpoints. Service Internal Traffic Policy applies only to internal cluster traffic originating from Pods within the cluster, not external client traffic. Options A, B, and D are benefits of setting internalTrafficPolicy to Local because it reduces cross-node network hops (lower latency), improves reliability by avoiding cluster network paths, and can reduce network costs by limiting traffic to the node."
Configure an existing Service named 'backend' to restrict internal traffic to node-local endpoints without changing other Service fields.,"kubectl patch service backend -p '{""spec"":{""internalTrafficPolicy"":""Local""}}'",This command uses 'kubectl patch' to modify the 'backend' Service resource by adding or updating the 'spec.internalTrafficPolicy' field to 'Local'. The '-p' flag specifies the patch in JSON format. This approach minimally updates only the intended field without affecting other Service specifications. It's useful for quickly enabling internal traffic locality on an existing Service without recreating or applying a full manifest.
"Which of the following statements about Kubernetes volumes is TRUE?

A) Data in ephemeral volumes is preserved even after the Pod is deleted.
B) Persistent volumes exist only as long as the Pod exists.
C) Data in volumes is lost when containers restart.
D) Volumes provide shared filesystem access between containers in the same Pod.",D) Volumes provide shared filesystem access between containers in the same Pod.,The correct answer is D. Kubernetes volumes allow multiple containers in the same Pod to share data via a filesystem. Option A is incorrect because ephemeral volumes are deleted when the Pod is removed. Option B is wrong as persistent volumes exist beyond the lifetime of a Pod and are not deleted when Pods are terminated. Option C is false because data in volumes is preserved across container restarts within a Pod.
"What happens to an emptyDir volume when a Pod is removed from a node?

A) The data persists on the node.
B) The data in the emptyDir is permanently deleted.
C) The data is synchronized to other nodes.
D) The data is backed up automatically to persistent storage.",B) The data in the emptyDir is permanently deleted.,The correct answer is B. An emptyDir volume is created when a Pod is assigned to a node and is deleted permanently when the Pod is removed from that node. Option A is incorrect because the data does not persist beyond the Pod lifecycle. Option C is wrong as emptyDir does not synchronize data across nodes. Option D is false because emptyDir volumes do not support automatic backup to persistent storage.
"Which volume type would you use to inject configuration data from a ConfigMap into a Pod?

A) emptyDir
B) configMap
C) persistentVolumeClaim
D) downwardAPI",B) configMap,"The correct answer is B. The configMap volume type allows configuration data stored in a ConfigMap to be mounted into a Pod as files. Option A (emptyDir) is for ephemeral scratch space, not config data. Option C (persistentVolumeClaim) is for persistent storage volumes, unrelated to ConfigMaps. Option D (downwardAPI) exposes Pod metadata and status, not ConfigMap data."
"Which statement about mounting volumes in Kubernetes containers is TRUE?

A) A volume can be mounted inside other volumes.
B) Each container must independently specify where to mount the volumes it uses.
C) Volumes can be mounted with hard links to different volumes.
D) Volume mounts are optional if volumes are defined in the Pod spec.",B) Each container must independently specify where to mount the volumes it uses.,"The correct answer is B. Each container in a Pod must declare volume mounts independently, specifying the mount path for each volume it requires. Option A is incorrect because volumes cannot be mounted inside other volumes. Option C is false since volumes cannot contain hard links to files in other volumes. Option D is wrong because defining a volume alone does not automatically mount it inside containers."
"Create a Pod named 'configmap-pod' running the busybox:1.28 image with a ConfigMap volume named 'config-vol' mounted at '/etc/config', using the ConfigMap named 'log-config'. The pod should run a command that prints 'The app is running!' and then sleep indefinitely.","kubectl run configmap-pod --image=busybox:1.28 --restart=Never --overrides='{ ""apiVersion"": ""v1"", ""kind"": ""Pod"", ""metadata"": { ""name"": ""configmap-pod"" }, ""spec"": { ""containers"": [{ ""name"": ""test"", ""image"": ""busybox:1.28"", ""command"": [""sh"", ""-c"", ""echo \""The app is running!\"" && tail -f /dev/null""], ""volumeMounts"": [{ ""name"": ""config-vol"", ""mountPath"": ""/etc/config"" }] }], ""volumes"": [{ ""name"": ""config-vol"", ""configMap"": { ""name"": ""log-config"" } }] } }'","This kubectl run command creates a Pod named 'configmap-pod' with the busybox:1.28 image. The '--restart=Never' flag ensures it creates a Pod, not a Deployment. The '--overrides' JSON specifies the Pod spec including container command, volume mount at '/etc/config', and the ConfigMap volume named 'config-vol' referencing the 'log-config' ConfigMap. The container runs a shell command to print a message then tails /dev/null to keep running."
Explain what happens to data stored in a downwardAPI volume when the Pod's metadata changes and the volume is mounted using subPath.,"When a downwardAPI volume is mounted using subPath, the container does not receive updates to the Pod metadata files if the metadata changes during the Pod lifecycle.","The downwardAPI volume exposes Pod metadata as files. However, when using subPath to mount the volume into a container, these files are not updated dynamically if the metadata changes. This means the container sees a static snapshot of the metadata at Pod startup. To receive updates, the volume must be mounted without subPath. This behavior is important to know when using downwardAPI volumes for dynamic Pod info."
"Which Kubernetes resource represents a piece of storage in the cluster with a lifecycle independent of any Pod?

A) PersistentVolumeClaim
B) PersistentVolume
C) StorageClass
D) Volume",B) PersistentVolume,"The correct answer is B) PersistentVolume. A PersistentVolume (PV) is a cluster resource representing storage that exists independently of any Pod that uses it. Option A) PersistentVolumeClaim is a user request for storage, not the storage itself. Option C) StorageClass defines how volumes are dynamically provisioned but is not the storage itself. Option D) Volume is a general term for storage attached to Pods but does not have a lifecycle independent of Pods."
"What is required for dynamic provisioning of PersistentVolumes in Kubernetes?

A) PVC must specify a StorageClass and the administrator must configure that StorageClass for dynamic provisioning
B) PVC must be bound to a statically created PV
C) PVC must have access mode ReadOnlyMany
D) PVC must be deleted before dynamic provisioning can occur",A) PVC must specify a StorageClass and the administrator must configure that StorageClass for dynamic provisioning,The correct answer is A) PVC must specify a StorageClass and the administrator must configure that StorageClass for dynamic provisioning. Dynamic provisioning happens only when the PVC requests a StorageClass and the cluster admin has configured that class for dynamic provisioning. Option B is incorrect because dynamic provisioning happens when no matching static PV exists. Option C is irrelevant to provisioning method. Option D is incorrect; PVC deletion is unrelated to provisioning.
"What happens to a PersistentVolume when its reclaim policy is set to 'Retain' and the associated PersistentVolumeClaim is deleted?

A) The PV is deleted along with its storage asset
B) The PV remains but is marked Released and data remains on the volume
C) The PV is recycled and data automatically wiped
D) The PV is immediately available for a new claim",B) The PV remains but is marked Released and data remains on the volume,"The correct answer is B) The PV remains but is marked Released and data remains on the volume. The 'Retain' reclaim policy means manual reclamation is required; the PV is not deleted automatically, and data remains until cleaned up manually. Option A is the behavior of 'Delete' reclaim policy. Option C describes 'Recycle', which is deprecated and not automatic in 'Retain'. Option D is incorrect because the PV is not available until manually cleaned and re-created."
"Which feature in Kubernetes protects PersistentVolumeClaims and PersistentVolumes from being deleted while they are in active use?

A) StorageClass
B) Access Modes
C) Storage Object in Use Protection
D) Reclaim Policy",C) Storage Object in Use Protection,"The correct answer is C) Storage Object in Use Protection. This feature prevents PVCs and PVs from being deleted while they are actively used by Pods, postponing deletion until they are no longer in use. Option A) StorageClass is about provisioning volumes. Option B) Access Modes define how volumes can be mounted, not deletion protection. Option D) Reclaim Policy controls what happens after a volume is released, not protection during use."
Create a PersistentVolumeClaim named 'mypvc' in the default namespace requesting 10Gi of storage with access mode ReadWriteOnce using the StorageClass 'fast-storage'.,kubectl create pvc mypvc --storage-class=fast-storage --access-modes=ReadWriteOnce --resources=requests.storage=10Gi,This command creates a PersistentVolumeClaim named 'mypvc' in the default namespace. The '--storage-class=fast-storage' flag specifies the StorageClass for dynamic provisioning or binding. '--access-modes=ReadWriteOnce' requests the volume to be mounted as read-write by a single node. '--resources=requests.storage=10Gi' requests 10Gi of storage. This PVC can then be bound to a matching PersistentVolume or dynamically provisioned.
Describe the command to check if a PersistentVolumeClaim named 'hostpath' is protected by the Storage Object in Use Protection feature.,kubectl describe pvc hostpath,"This command shows detailed information about the 'hostpath' PVC. In the output, check for the 'Finalizers' field containing 'kubernetes.io/pvc-protection' and the status 'Terminating' to confirm protection is active. This means the PVC cannot be deleted while in use by a Pod. No additional flags are needed for this inspection."
"Which of the following volume sources cannot be included in a Kubernetes projected volume?

A) secret
B) configMap
C) persistentVolumeClaim
D) downwardAPI",C) persistentVolumeClaim,"The correct answer is C) persistentVolumeClaim. Projected volumes combine several volume sources into the same directory, but supported sources include secret, configMap, downwardAPI, serviceAccountToken, and clusterTrustBundle. PersistentVolumeClaims are not supported as a source in projected volumes. Option A) secret, B) configMap, and D) downwardAPI are all valid sources for projected volumes."
"In a projected volume, where can you specify the default file permission mode for all projections?

A) At each individual volume source under 'mode'
B) Only at the projected volume level under 'defaultMode'
C) At the container security context
D) In the Pod metadata annotations",B) Only at the projected volume level under 'defaultMode',"The correct answer is B) Only at the projected volume level under 'defaultMode'. When using projected volumes, the defaultMode parameter can only be applied at the projected volume level, not for each individual volume source. However, you can override this default by explicitly setting the 'mode' for individual projections. Option A is incorrect because 'mode' cannot be specified at the volume source level for defaultMode. Option C is unrelated to file mode in projected volumes. Option D is incorrect because Pod annotations do not control volume file permissions."
"Which statement about serviceAccountToken projected volumes is TRUE?

A) The audience field is mandatory and must match the API server identifier.
B) expirationSeconds must be at least 3600 seconds.
C) The token is mounted at a path relative to the projected volume mount.
D) serviceAccountToken projected volumes can inject tokens from any namespace.",C) The token is mounted at a path relative to the projected volume mount.,"The correct answer is C) The token is mounted at a path relative to the projected volume mount. The 'path' field specifies a relative path inside the projected volume where the token file will be placed. Option A is incorrect because the 'audience' field is optional and defaults to the API server identifier. Option B is incorrect because expirationSeconds defaults to 3600 seconds but must be at least 600 seconds (10 minutes). Option D is incorrect because all sources, including serviceAccountToken, must be in the same namespace as the Pod."
"When using clusterTrustBundle projected volumes in Kubernetes v1.33, which of the following is REQUIRED to enable this feature?

A) Enable the ClusterTrustBundle feature gate and set --runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true on the kube-apiserver
B) Annotate the Pod with clusterTrustBundle=true
C) Use the default enabled state without any configuration
D) Use a ConfigMap to hold the clusterTrustBundle certificates",A) Enable the ClusterTrustBundle feature gate and set --runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true on the kube-apiserver,"The correct answer is A) Enable the ClusterTrustBundle feature gate and set --runtime-config=certificates.k8s.io/v1beta1/clustertrustbundles=true on the kube-apiserver. In Kubernetes v1.33, clusterTrustBundle projected volumes are beta and disabled by default, requiring explicit enabling via feature gates and runtime-config flags on the API server. Option B is incorrect because Pod annotations do not enable this feature. Option C is wrong since the feature is disabled by default. Option D is incorrect because clusterTrustBundles are special resources, not ConfigMaps."
"Create a Pod named 'projected-pod' that mounts a projected volume combining a secret named 'mysecret' (key: 'username' mapped to path 'user'), a ConfigMap named 'myconfig' (key: 'config' mapped to 'app-config'), and a downwardAPI volume exposing the Pod's labels at 'labels'. Mount the projected volume at '/mnt/projected' as read-only, using the busybox image and command 'sleep 3600'.","kubectl run projected-pod --image=busybox --command -- sleep 3600 --dry-run=client -o yaml > projected-pod.yaml

# Then edit projected-pod.yaml to add the volumes and volumeMounts as below:

# volumes:
# - name: projected-volume
#   projected:
#     sources:
#     - secret:
#         name: mysecret
#         items:
#         - key: username
#           path: user
#     - configMap:
#         name: myconfig
#         items:
#         - key: config
#           path: app-config
#     - downwardAPI:
#         items:
#         - path: labels
#           fieldRef:
#             fieldPath: metadata.labels

# containers:
# - name: busybox
#   image: busybox
#   command:
#   - sleep
#   - ""3600""
#   volumeMounts:
#   - name: projected-volume
#     mountPath: /mnt/projected
#     readOnly: true

kubectl apply -f projected-pod.yaml","This task cannot be completed with a single kubectl run command because projected volumes require detailed volume and volumeMount specifications. The recommended approach is to generate a Pod manifest using 'kubectl run' with '--dry-run=client -o yaml', then edit the manifest to include a projected volume combining the secret, configMap, and downwardAPI sources as specified. The projected volume is mounted at '/mnt/projected' with readOnly: true. Finally, apply the manifest with 'kubectl apply -f'. This approach ensures all volume sources are correctly projected into the Pod."
"What effect does setting 'optional: true' have on a clusterTrustBundle projected volume source?

A) The Pod fails to start if the ClusterTrustBundle is missing.
B) The Pod starts with an empty file if no matching ClusterTrustBundle is found.
C) It disables automatic updates of the certificates in the projected file.
D) It causes the kubelet to ignore the ClusterTrustBundle feature gate.",B) The Pod starts with an empty file if no matching ClusterTrustBundle is found.,"The correct answer is B) The Pod starts with an empty file if no matching ClusterTrustBundle is found. By default, the kubelet prevents the Pod from starting if the specified ClusterTrustBundle object is missing. Setting 'optional: true' overrides this behavior, allowing the Pod to start and creating an empty file at the specified path. Option A is the default behavior without 'optional: true'. Option C is incorrect because certificate updates continue. Option D is incorrect because feature gates must still be enabled regardless of this flag."
"Which of the following best describes an ephemeral volume in Kubernetes?

A) A volume that persists data across Pod restarts and is backed by PersistentVolumeClaim.
B) A volume that is created and deleted along with the Pod and does not persist data beyond the Pod's lifecycle.
C) A volume that stores data permanently on a network-attached storage.
D) A volume type that only supports read-only access and cannot be used for scratch data.",B) A volume that is created and deleted along with the Pod and does not persist data beyond the Pod's lifecycle.,"The correct answer is B. Ephemeral volumes are designed to live and die with the Pod, meaning the data stored in them does not persist after the Pod is deleted or restarted. Option A is incorrect because persistent volumes backed by PersistentVolumeClaims are designed to persist data beyond the Pod lifecycle. Option C is incorrect because ephemeral volumes do not necessarily use network-attached storage and do not provide permanent storage. Option D is incorrect because ephemeral volumes can be used for scratch data and are not limited to read-only access."
"Which of the following volume types is NOT considered a local ephemeral volume managed by kubelet?

A) emptyDir
B) configMap
C) secret
D) CSI ephemeral volumes",D) CSI ephemeral volumes,"The correct answer is D. CSI ephemeral volumes are provided by third-party CSI drivers and are not managed by kubelet as local ephemeral storage. Options A, B, and C (emptyDir, configMap, secret) are local ephemeral volumes managed by kubelet on each node. Thus, they are considered local ephemeral storage, whereas CSI ephemeral volumes rely on external CSI drivers."
"What is a key limitation of CSI ephemeral volumes compared to generic ephemeral volumes?

A) CSI ephemeral volumes support snapshotting and cloning while generic ephemeral volumes do not.
B) CSI ephemeral volumes support storage capacity aware Pod scheduling but generic ephemeral volumes do not.
C) CSI ephemeral volumes cannot be used with dynamic provisioning unlike generic ephemeral volumes.
D) CSI ephemeral volumes are managed by kubelet directly, unlike generic ephemeral volumes.",C) CSI ephemeral volumes cannot be used with dynamic provisioning unlike generic ephemeral volumes.,"The correct answer is C. Some CSI drivers written specifically for CSI ephemeral volumes do not support dynamic provisioning, which is required for generic ephemeral volumes. Option A is incorrect because generic ephemeral volumes support snapshotting and cloning, but CSI ephemeral volumes may not. Option B is incorrect because both types currently do not support storage capacity aware Pod scheduling. Option D is incorrect because CSI ephemeral volumes are managed via CSI drivers, not by kubelet directly."
"How is the name of the PersistentVolumeClaim (PVC) created for a generic ephemeral volume determined?

A) It is randomly generated by Kubernetes.
B) It is the same as the Pod name.
C) It is a combination of the Pod name and volume name separated by a hyphen.
D) It is defined explicitly in the Pod manifest by the user.",C) It is a combination of the Pod name and volume name separated by a hyphen.,The correct answer is C. Kubernetes deterministically names the PVC for a generic ephemeral volume by concatenating the Pod name and the volume name with a hyphen. Option A is incorrect because the name is not random. Option B is incorrect because the PVC name includes both Pod and volume names. Option D is incorrect because the PVC is automatically created by Kubernetes and not explicitly named by the user.
"Create a Pod named 'my-csi-app' using the busybox:1.28 image, which mounts a CSI ephemeral volume named 'my-csi-inline-vol' at path '/data' using the CSI driver 'inline.storage.kubernetes.io' with a volume attribute 'foo=bar'. The container should run the command 'sleep 1000000'.","kubectl run my-csi-app --image=busybox:1.28 --command -- sleep 1000000 --overrides='{""apiVersion"":""v1"",""spec"":{""volumes"":[{""name"":""my-csi-inline-vol"",""csi"":{""driver"":""inline.storage.kubernetes.io"",""volumeAttributes"":{""foo"":""bar""}}}],""containers"":[{""name"":""my-csi-app"",""image"":""busybox:1.28"",""command"":[""sleep"",""1000000""],""volumeMounts"":[{""mountPath"":""/data"",""name"":""my-csi-inline-vol""}]}]}}'","This command uses 'kubectl run' to create a Pod named 'my-csi-app' with the specified image and command. The '--overrides' flag injects the Pod spec JSON to define the CSI ephemeral volume with the driver 'inline.storage.kubernetes.io' and volume attribute 'foo=bar'. The container mounts this volume at '/data'. This approach is necessary because 'kubectl run' does not support the full volume spec natively, so JSON overrides are used to specify the CSI volume inline."
Create a Pod named 'my-app' with a generic ephemeral volume called 'scratch-volume' using a volumeClaimTemplate requesting 1Gi storage with access mode ReadWriteOnce and storageClassName 'scratch-storage-class'. The Pod should mount the volume at '/scratch' and run 'sleep 1000000' using the busybox:1.28 image.,"kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - name: my-frontend
    image: busybox:1.28
    command: [""sleep"", ""1000000""]
    volumeMounts:
    - mountPath: /scratch
      name: scratch-volume
  volumes:
  - name: scratch-volume
    ephemeral:
      volumeClaimTemplate:
        metadata:
          labels:
            type: my-frontend-volume
        spec:
          accessModes: [""ReadWriteOnce""]
          storageClassName: scratch-storage-class
          resources:
            requests:
              storage: 1Gi
EOF","This command uses 'kubectl apply' with a here-document to create a Pod manifest inline. The Pod named 'my-app' includes a generic ephemeral volume named 'scratch-volume' defined via 'volumeClaimTemplate' which dynamically creates a PVC with the specified storage requests and class. The container mounts this volume at '/scratch' and runs the command 'sleep 1000000'. Using generic ephemeral volumes allows the Pod to have ephemeral storage backed by dynamically provisioned storage, automatically cleaned up when the Pod is deleted."
"What is the purpose of a StorageClass in Kubernetes?

A) To define network policies for pods
B) To describe classes of storage with different quality-of-service or backup policies
C) To manage user access control
D) To schedule pods on specific nodes",B) To describe classes of storage with different quality-of-service or backup policies,"The correct answer is B) To describe classes of storage with different quality-of-service or backup policies. A StorageClass provides a way for administrators to define different storage types or policies, such as performance levels or backup settings. Option A is incorrect because StorageClasses do not define network policies. Option C is incorrect as StorageClasses do not manage user access control. Option D is wrong because StorageClasses are unrelated to pod scheduling."
"Which reclaimPolicy is the default when not specified in a StorageClass?

A) Retain
B) Delete
C) Recycle
D) None",B) Delete,"The correct answer is B) Delete. When a reclaimPolicy is not specified in a StorageClass, it defaults to Delete, meaning PersistentVolumes created by the StorageClass are deleted when released. Option A) Retain is incorrect because it must be explicitly set and it retains the volume after release. Option C) Recycle is deprecated and not a valid default. Option D) None is invalid."
"What does setting the volumeBindingMode to WaitForFirstConsumer achieve?

A) Immediate dynamic provisioning of volumes upon PVC creation
B) Delays volume binding and provisioning until a Pod using the PVC is scheduled
C) Prevents volume expansion
D) Automatically deletes the volume when Pod is deleted",B) Delays volume binding and provisioning until a Pod using the PVC is scheduled,"The correct answer is B) Delays volume binding and provisioning until a Pod using the PVC is scheduled. This ensures volumes are provisioned in the correct topology based on Pod scheduling constraints. Option A) Immediate is the default mode, which binds volumes as soon as the PVC is created. Option C) is unrelated to volumeBindingMode. Option D) describes reclaimPolicy behavior, not volumeBindingMode."
"Which of the following statements about default StorageClasses is TRUE?

A) Multiple StorageClasses marked as default will cause PVC creation to fail
B) If no default StorageClass exists, PVCs without storageClassName remain unbound
C) The annotation 'storageclass.kubernetes.io/is-default-class' must be set to 'false' to mark a default class
D) PVCs without storageClassName will never get updated if a default StorageClass becomes available","B) If no default StorageClass exists, PVCs without storageClassName remain unbound","The correct answer is B) If no default StorageClass exists, PVCs without storageClassName remain unbound until a default is set. Option A is incorrect because Kubernetes allows multiple default classes but selects the most recently created one. Option C is wrong because the annotation should be set to 'true' to mark a default. Option D is incorrect; PVCs without storageClassName are updated when a default StorageClass appears, except those explicitly set to an empty string ('')."
Create a StorageClass named 'fast-storage' using the provisioner 'csi.example.com' with volume expansion enabled and a reclaim policy of Retain.,"kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-storage
provisioner: csi.example.com
reclaimPolicy: Retain
allowVolumeExpansion: true
EOF",This command creates a StorageClass resource named 'fast-storage' with the specified provisioner 'csi.example.com'. The 'reclaimPolicy: Retain' ensures volumes are not deleted when released. The 'allowVolumeExpansion: true' flag enables resizing of volumes dynamically. Using 'kubectl apply -f -' with a here-document allows inline creation of the resource. This method is common for creating complex resources without files.
How would you mark an existing StorageClass named 'standard' as the default StorageClass in your cluster?,"kubectl patch storageclass standard -p '{""metadata"":{""annotations"":{""storageclass.kubernetes.io/is-default-class"":""true""}}}'","This command patches the 'standard' StorageClass resource to add or update the annotation 'storageclass.kubernetes.io/is-default-class' to 'true', marking it as the default. This default StorageClass will be used for PVCs that do not specify a storageClassName. The '-p' flag passes the patch as a JSON string. This is a quick way to update annotations without editing the full resource."
"Which of the following statements about VolumeAttributesClass in Kubernetes is TRUE?

A) VolumeAttributesClass is enabled by default in Kubernetes v1.31.
B) VolumeAttributesClass parameters are mutable after creation.
C) VolumeAttributesClass requires the CSI driver to implement the ModifyVolume API for volume modifications.
D) VolumeAttributesClass can be used with any volume plugin, including in-tree plugins.",C) VolumeAttributesClass requires the CSI driver to implement the ModifyVolume API for volume modifications.,"The correct answer is C. VolumeAttributesClass can only be used where the relevant CSI driver implements the ModifyVolume API, enabling modification of volumes. Option A is incorrect because the feature is in beta in v1.31 and disabled by default. Option B is incorrect since parameters of an existing VolumeAttributesClass are immutable after creation. Option D is incorrect because VolumeAttributesClass works only with volumes backed by CSI drivers, not in-tree volume plugins."
"To enable the VolumeAttributesClass feature gate and the storage.k8s.io/v1beta1 API group on the kube-apiserver, which command line arguments are required?

A) --feature-gates=VolumeAttributesClass=true --runtime-config=storage.k8s.io/v1beta1=true
B) --feature-gates=VolumeAttributesClass=false --runtime-config=storage.k8s.io/v1alpha1=true
C) --enable-feature=VolumeAttributesClass --enable-api=storage.k8s.io/v1beta1
D) --feature-gates=VolumeAttributesClass=true --enable-api=storage.k8s.io/v1beta1",A) --feature-gates=VolumeAttributesClass=true --runtime-config=storage.k8s.io/v1beta1=true,The correct answer is A. Enabling the VolumeAttributesClass beta feature requires setting the feature gate VolumeAttributesClass=true and enabling the storage.k8s.io/v1beta1 API group via the runtime-config parameter. Option B is incorrect as it disables the feature gate and references the wrong API version. Options C and D use incorrect flag names that the kube-apiserver does not recognize.
"What is the purpose of the driverName field in a VolumeAttributesClass object?

A) It specifies the Kubernetes scheduler plugin to use.
B) It identifies the CSI driver used for provisioning and resizing volumes.
C) It determines the node selector for volume placement.
D) It defines the container image used by the provisioner.",B) It identifies the CSI driver used for provisioning and resizing volumes.,The correct answer is B. The driverName field specifies the CSI driver name that will be used by the provisioner and resizer to create and modify PersistentVolumes. Option A is incorrect as driverName has nothing to do with the Kubernetes scheduler. Option C is incorrect because node selectors are unrelated to this field. Option D is incorrect since driverName does not define container images.
Create a VolumeAttributesClass named 'gold' for the CSI driver 'pd.csi.storage.gke.io' with parameters iops=4000 and throughput=60.,"kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1beta1
kind: VolumeAttributesClass
metadata:
  name: gold
driverName: pd.csi.storage.gke.io
parameters:
  iops: ""4000""
  throughput: ""60""
EOF","This command uses 'kubectl apply -f -' to create a VolumeAttributesClass resource by reading the manifest from standard input. The manifest defines the apiVersion as storage.k8s.io/v1beta1, kind as VolumeAttributesClass, and sets metadata.name to 'gold'. The driverName is set to the CSI driver 'pd.csi.storage.gke.io', and the parameters field specifies iops and throughput values as strings. Proper quoting ensures numeric values are treated as strings, which is required by Kubernetes API."
"Which of the following is TRUE about parameters in a VolumeAttributesClass?

A) They can have up to 1024 keys.
B) Parameters are mutable after the VolumeAttributesClass is created.
C) Parameters describe volume characteristics and their defaults are used if omitted.
D) Parameter names are fixed by Kubernetes and cannot be customized.",C) Parameters describe volume characteristics and their defaults are used if omitted.,"The correct answer is C. Parameters describe specific volume attributes (like IOPS or throughput) and if omitted during provisioning, default values may be used depending on the CSI driver implementation. Option A is incorrect because the maximum number of parameters is 512, not 1024. Option B is incorrect as parameters are immutable after creation. Option D is incorrect since parameter keys are defined by the CSI driver and administrators and are not fixed by Kubernetes."
Update an existing PersistentVolumeClaim named 'test-pv-claim' to switch its VolumeAttributesClass from 'silver' to 'gold'.,"kubectl patch pvc test-pv-claim -p '{""spec"":{""volumeAttributesClassName"":""gold""}}'","This command patches the PersistentVolumeClaim resource named 'test-pv-claim' by updating the 'spec.volumeAttributesClassName' field to 'gold'. The -p flag allows specifying a JSON patch inline. This updates the PVC to request the new VolumeAttributesClass 'gold', enabling volume modification if supported. This approach avoids editing the entire manifest and is efficient for small updates."
"Which Kubernetes API object is responsible for enabling dynamic volume provisioning by specifying the volume plugin and its parameters?

A) PersistentVolumeClaim
B) PersistentVolume
C) StorageClass
D) VolumeBindingMode",C) StorageClass,"The correct answer is C) StorageClass. StorageClass objects define which provisioner (volume plugin) to use and the parameters for dynamic volume provisioning. Option A) PersistentVolumeClaim is used by users to request storage but does not specify provisioning details. Option B) PersistentVolume represents actual storage resources but is not used to configure dynamic provisioning. Option D) VolumeBindingMode is related to where volumes are bound in topology-aware provisioning, not the main API object for dynamic provisioning."
"How can a cluster administrator designate a StorageClass as the default for dynamic provisioning?

A) Set the storageClassName field in PersistentVolumeClaim to 'default'
B) Add the annotation storageclass.kubernetes.io/is-default-class: ""true"" to the StorageClass
C) Name the StorageClass 'default'
D) Enable the VolumeBindingMode in the StorageClass","B) Add the annotation storageclass.kubernetes.io/is-default-class: ""true"" to the StorageClass","The correct answer is B) Add the annotation storageclass.kubernetes.io/is-default-class: ""true"" to the StorageClass. This marks the StorageClass as the cluster's default for volume provisioning. Option A) is incorrect because PersistentVolumeClaim sets storageClassName to select a class, not to define defaults. Option C) naming a StorageClass 'default' does not automatically make it the default class. Option D) VolumeBindingMode controls binding behavior and is unrelated to setting defaults."
"Prior to Kubernetes v1.9, how was the storage class specified in a PersistentVolumeClaim?

A) storageClassName field
B) storage-class annotation
C) volume.beta.kubernetes.io/storage-class annotation
D) metadata.name field",C) volume.beta.kubernetes.io/storage-class annotation,"The correct answer is C) volume.beta.kubernetes.io/storage-class annotation. Before v1.9, this annotation was used to specify the storage class in PVCs. Option A) storageClassName field is the current recommended way since v1.9. Option B) storage-class annotation is not a valid annotation. Option D) metadata.name is the name of the PVC itself, not related to storage class selection."
"What happens to a dynamically provisioned PersistentVolume when its associated PersistentVolumeClaim is deleted?

A) The PersistentVolume remains bound and must be deleted manually
B) The PersistentVolume is automatically deleted
C) The PersistentVolume is retained but unbound
D) The PersistentVolume is converted to ReadOnly mode",B) The PersistentVolume is automatically deleted,"The correct answer is B) The PersistentVolume is automatically deleted. In dynamic provisioning, the lifecycle of the volume is tied to the PVC, and deletion of the claim typically triggers deletion of the volume. Option A) is incorrect because manual deletion is not needed for dynamically provisioned volumes. Option C) retention happens only if reclaimPolicy is set to Retain, which is not the default for dynamic provisioning. Option D) volumes are not converted to ReadOnly mode upon claim deletion."
"Create a PersistentVolumeClaim named 'fast-claim' requesting 50Gi of storage with access mode ReadWriteOnce, using the StorageClass named 'fast'.","kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fast-claim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast
  resources:
    requests:
      storage: 50Gi
EOF","This command uses 'kubectl apply -f -' with a here-document to create a PersistentVolumeClaim named 'fast-claim'. The spec requests 50Gi of storage with access mode ReadWriteOnce, and specifies the StorageClass 'fast' for dynamic provisioning. Using 'kubectl apply' with inline YAML allows precise control over PVC creation matching certification scenarios."
How can a cluster administrator verify which StorageClass is currently marked as the default in the cluster?,"kubectl get storageclass -o jsonpath='{range .items[?(@.metadata.annotations.storageclass\.kubernetes\.io/is-default-class==""true"")]}{.metadata.name}{""\n""}{end}'","This command lists all StorageClass objects and filters those with the annotation 'storageclass.kubernetes.io/is-default-class' set to 'true' using JSONPath. It outputs the names of the default StorageClasses. This method is important because multiple StorageClasses can be marked as default, and administrators need to identify which are currently set. The escaping of the dot in the annotation key is necessary for JSONPath syntax."
"Which Kubernetes API resource represents a user request to create a volume snapshot?

A) VolumeSnapshotContent
B) VolumeSnapshot
C) VolumeSnapshotClass
D) PersistentVolumeClaim",B) VolumeSnapshot,"The correct answer is B) VolumeSnapshot. This resource represents a user's request to create a snapshot of a volume, analogous to how PersistentVolumeClaim requests a PersistentVolume. Option A) VolumeSnapshotContent is incorrect because it represents the actual snapshot content on the storage system and is managed by administrators or controllers. Option C) VolumeSnapshotClass defines parameters for snapshot creation but does not represent the snapshot itself. Option D) PersistentVolumeClaim is related to requesting persistent storage volumes, not snapshots."
"Which component is responsible for watching VolumeSnapshot and VolumeSnapshotContent objects and managing their lifecycle in Kubernetes?

A) kube-scheduler
B) CSI driver
C) snapshot controller
D) kube-controller-manager",C) snapshot controller,"The correct answer is C) snapshot controller. This controller watches VolumeSnapshot and VolumeSnapshotContent objects and handles their creation and deletion, managing the binding between these resources. Option A) kube-scheduler is responsible for scheduling pods onto nodes, unrelated to snapshots. Option B) CSI driver provides storage capabilities and snapshot support but relies on the snapshot controller for lifecycle management. Option D) kube-controller-manager runs various controllers but does not directly manage snapshot lifecycle; the snapshot controller is a separate specialized controller."
"Which of the following statements about VolumeSnapshotClass in Kubernetes is TRUE?

A) It is used to request a pre-provisioned snapshot.
B) It specifies storage provider-specific parameters for snapshot creation.
C) It is a core API resource.
D) It replaces StorageClass for PersistentVolumeClaims.",B) It specifies storage provider-specific parameters for snapshot creation.,"The correct answer is B) It specifies storage provider-specific parameters for snapshot creation. VolumeSnapshotClass allows users to define attributes for snapshot behavior specific to the CSI driver. Option A) is incorrect because VolumeSnapshotClass does not request pre-provisioned snapshots; it is used mainly for dynamic provisioning. Option C) is incorrect because VolumeSnapshotClass is a CRD, not part of the core API. Option D) is incorrect because VolumeSnapshotClass does not replace StorageClass for PersistentVolumeClaims; they serve different purposes."
"What happens if a PersistentVolumeClaim that is the source for a VolumeSnapshot is deleted while the snapshot is still being created?

A) The PersistentVolumeClaim is immediately deleted, causing snapshot failure.
B) The deletion of PersistentVolumeClaim is postponed until the snapshot is ready or aborted.
C) The snapshot creation is canceled automatically.
D) The PersistentVolumeClaim is duplicated to preserve data.",B) The deletion of PersistentVolumeClaim is postponed until the snapshot is ready or aborted.,The correct answer is B) The deletion of PersistentVolumeClaim is postponed until the snapshot is ready or aborted. This protection prevents data loss by ensuring the source PVC is not removed during snapshot creation. Option A) is incorrect because immediate deletion would jeopardize snapshot integrity. Option C) is incorrect as snapshot creation is not automatically canceled. Option D) is incorrect because Kubernetes does not duplicate PVCs in this scenario.
Create a VolumeSnapshot named 'db-backup' in the 'prod' namespace that dynamically takes a snapshot from a PersistentVolumeClaim named 'db-pvc' using the VolumeSnapshotClass 'fast-snapshots'.,kubectl create volumesnapshot db-backup --namespace=prod --source-pvc=db-pvc --snapshot-class=fast-snapshots,"This command creates a VolumeSnapshot resource named 'db-backup' in the 'prod' namespace. The '--source-pvc=db-pvc' flag specifies the PersistentVolumeClaim to snapshot. The '--snapshot-class=fast-snapshots' flag specifies the VolumeSnapshotClass to use for dynamic provisioning of the snapshot. This command uses the kubectl plugin for volume snapshots, which is available in newer kubectl versions. Note that the VolumeSnapshot CRDs and snapshot controller must be installed and the CSI driver must support snapshots."
"A cluster administrator wants to create a pre-provisioned volume snapshot content named 'pre-snap-content' for a snapshot with snapshot handle '12345-abcde' on the CSI driver 'csi.example.com', which should be deleted when the VolumeSnapshot is deleted. Write the YAML manifest snippet for the spec section of this VolumeSnapshotContent.","spec:
  deletionPolicy: Delete
  driver: csi.example.com
  source:
    snapshotHandle: ""12345-abcde""
  sourceVolumeMode: Filesystem
  volumeSnapshotRef:
    name: <snapshot-name>
    namespace: <snapshot-namespace>","This YAML snippet defines the spec for a pre-provisioned VolumeSnapshotContent. The 'deletionPolicy: Delete' ensures the underlying snapshot is deleted when the VolumeSnapshot is deleted. 'driver' specifies the CSI driver name. 'source.snapshotHandle' is the unique ID of the snapshot on the storage backend. 'sourceVolumeMode' indicates the volume mode, commonly 'Filesystem'. 'volumeSnapshotRef' points to the VolumeSnapshot resource that is bound to this content; placeholders for name and namespace must be replaced accordingly. This manifest must be applied by the cluster administrator before users can bind VolumeSnapshots to it."
"Which of the following fields is mandatory when creating a VolumeSnapshotClass in Kubernetes?

A) volumeSnapshotRef
B) driver
C) storageClassName
D) accessModes",B) driver,"The correct answer is B) driver. The driver field specifies the CSI volume plugin used for provisioning VolumeSnapshots and is mandatory in VolumeSnapshotClass. Option A) volumeSnapshotRef is not a field in VolumeSnapshotClass. Option C) storageClassName is a field related to StorageClass, not VolumeSnapshotClass. Option D) accessModes is a field used in PersistentVolume or PersistentVolumeClaim, not in VolumeSnapshotClass."
"What happens if multiple VolumeSnapshotClass objects with the same CSI driver are marked as default in a Kubernetes cluster?

A) Kubernetes selects one at random to use
B) VolumeSnapshot creation fails due to ambiguity
C) Kubernetes merges parameters from all default classes
D) The first created default class is always used",B) VolumeSnapshot creation fails due to ambiguity,"The correct answer is B) VolumeSnapshot creation fails due to ambiguity. Kubernetes requires only one default VolumeSnapshotClass per CSI driver; if multiple defaults exist for the same driver, it cannot determine which to use, causing failure. Option A is incorrect because Kubernetes does not randomly pick. Option C is incorrect as Kubernetes does not merge parameters. Option D is incorrect because there is no precedence by creation time."
"Which deletionPolicy setting on a VolumeSnapshotClass ensures that the underlying storage snapshot remains after the VolumeSnapshot object is deleted?

A) Delete
B) Retain
C) Keep
D) Preserve",B) Retain,"The correct answer is B) Retain. When deletionPolicy is set to Retain, deleting the VolumeSnapshot object does not delete the underlying storage snapshot or the VolumeSnapshotContent. Option A) Delete causes the underlying snapshot to be deleted with the VolumeSnapshotContent. Options C) Keep and D) Preserve are not valid deletionPolicy values."
"How can an administrator specify a default VolumeSnapshotClass for a particular CSI driver?

A) By setting the annotation snapshot.storage.kubernetes.io/is-default-class: ""true"" on the VolumeSnapshotClass
B) By naming the VolumeSnapshotClass with the prefix 'default-'
C) By setting deletionPolicy to 'Default'
D) By specifying default: true in the parameters section","A) By setting the annotation snapshot.storage.kubernetes.io/is-default-class: ""true"" on the VolumeSnapshotClass","The correct answer is A) By setting the annotation snapshot.storage.kubernetes.io/is-default-class: ""true"" on the VolumeSnapshotClass. This annotation marks the VolumeSnapshotClass as default for its CSI driver. Option B is incorrect because naming conventions do not influence default status. Option C is incorrect because deletionPolicy controls snapshot deletion behavior, not default status. Option D is incorrect because 'default: true' is not a recognized parameter for default designation."
Create a VolumeSnapshotClass named 'fast-snapclass' for the CSI driver 'fast.csi.driver' with deletionPolicy set to Retain and mark it as the default snapshot class.,"kubectl apply -f - <<EOF
apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: fast-snapclass
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: ""true""
driver: fast.csi.driver
deletionPolicy: Retain
parameters: {}
EOF","This command uses 'kubectl apply -f -' to create a VolumeSnapshotClass resource from the inline YAML manifest. The metadata.name is set to 'fast-snapclass'. The annotation 'snapshot.storage.kubernetes.io/is-default-class: ""true""' marks it as default for the CSI driver. The driver field is set to 'fast.csi.driver' as required. The deletionPolicy is set to 'Retain' to keep snapshots after deletion. Empty parameters are specified as '{}', which can be customized if needed."
"If a user creates a VolumeSnapshot without specifying a VolumeSnapshotClass, how does Kubernetes select which VolumeSnapshotClass to use?",Kubernetes automatically selects the default VolumeSnapshotClass whose CSI driver matches the CSI driver of the PVC's StorageClass.,"When a VolumeSnapshot is created without specifying a VolumeSnapshotClass, Kubernetes finds the default VolumeSnapshotClass that has the same CSI driver as the PersistentVolumeClaim's StorageClass. This design allows multiple default VolumeSnapshotClass objects to coexist for different CSI drivers. Optionally specifying the class overrides this behavior. This automatic matching ensures snapshots are created using the appropriate CSI driver."
"Which of the following statements about CSI Volume Cloning in Kubernetes is TRUE?

A) Cloning a PVC can be done across different namespaces.
B) Cloning support is available for all volume types, including non-CSI volumes.
C) The source PVC must be bound and available before cloning.
D) The cloned PVC automatically remains linked to the source PVC for synchronization.",C) The source PVC must be bound and available before cloning.,"Option C is correct because Kubernetes requires the source PersistentVolumeClaim (PVC) to be bound and available (not in use) before it can be cloned. Option A is incorrect because cloning can only be done within the same namespace; cross-namespace cloning is not supported. Option B is incorrect because cloning support is only available for CSI drivers that implement volume cloning, and not for non-CSI volumes. Option D is incorrect because the cloned PVC is an independent object and is not linked to the source PVC after creation; changes to one do not affect the other."
"When creating a cloned PVC in Kubernetes using CSI Volume Cloning, which condition must be met regarding the volume mode?

A) The source PVC must be in filesystem mode, the clone can be block mode.
B) The clone must have the same VolumeMode setting as the source PVC.
C) VolumeMode does not need to match between source and clone.
D) VolumeMode is ignored when cloning PVCs.",B) The clone must have the same VolumeMode setting as the source PVC.,Option B is correct because Kubernetes enforces that the VolumeMode setting (Filesystem or Block) must be the same between the source PVC and the cloned PVC. Option A is incorrect because the source and clone cannot differ in VolumeMode. Option C and D are incorrect as VolumeMode is an important attribute that must match for successful cloning.
"Which of the following is TRUE about Storage Classes when cloning a PVC using CSI Volume Cloning?

A) Cloning can only be done if the source and destination PVCs use the same Storage Class.
B) Cloning can be done using a different Storage Class for the destination PVC.
C) Storage Class must always be explicitly specified during cloning.
D) Cloning is not supported with dynamic provisioners.",B) Cloning can be done using a different Storage Class for the destination PVC.,Option B is correct because Kubernetes allows the destination PVC to use a different Storage Class than the source PVC when cloning. Option A is incorrect because same Storage Class is not a requirement. Option C is incorrect since the Storage Class can be omitted to use the default Storage Class. Option D is incorrect because cloning requires dynamic provisioners and is not supported with static provisioning.
"Create a PersistentVolumeClaim named 'clone-pvc' in the namespace 'test-ns' that clones an existing PVC named 'source-pvc', requesting 10Gi of storage, using the Storage Class 'fast-storage' and ReadWriteOnce access mode.","kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: clone-pvc
  namespace: test-ns
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: fast-storage
  resources:
    requests:
      storage: 10Gi
  dataSource:
    kind: PersistentVolumeClaim
    name: source-pvc
EOF","This command creates a YAML manifest inline and applies it via kubectl. The PersistentVolumeClaim named 'clone-pvc' is defined in the 'test-ns' namespace. The 'accessModes' specifies ReadWriteOnce. 'storageClassName' is set to 'fast-storage'. The storage request is 10Gi which must be equal or larger than the source PVC capacity. The 'dataSource' field references the existing PVC 'source-pvc' in the same namespace, triggering the cloning feature. Using 'kubectl apply -f -' with a heredoc allows creating this manifest without a separate file."
"Which of the following is NOT a requirement for CSI Volume Cloning in Kubernetes?

A) The CSI driver must support volume cloning.
B) The volume to be cloned must be dynamically provisioned.
C) The source PVC can be in any namespace.
D) The size requested in the clone PVC must be equal to or larger than the source PVC.",C) The source PVC can be in any namespace.,"Option C is the correct answer because it is NOT a requirement; in fact, cloning is restricted to PVCs within the same namespace. Option A is a requirement because only CSI drivers that implement cloning support can be used. Option B is correct because cloning only works with dynamically provisioned volumes. Option D is also a requirement as the requested clone size must be equal or greater than the source PVC size."
"Create a PersistentVolumeClaim named 'block-clone' in namespace 'dev', cloning from an existing PVC 'block-source' that uses Block volume mode, requests 20Gi storage, and uses Storage Class 'block-storage'. Use ReadWriteOnce access mode.","kubectl apply -f - <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: block-clone
  namespace: dev
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: block-storage
  volumeMode: Block
  resources:
    requests:
      storage: 20Gi
  dataSource:
    kind: PersistentVolumeClaim
    name: block-source
EOF","This command creates a PVC named 'block-clone' that clones an existing PVC 'block-source' in the same namespace 'dev'. The 'volumeMode: Block' field specifies that the volume should be in block mode, matching the source's volume mode as required. The storage request is 20Gi, which must be equal or larger than the source's capacity. The 'dataSource' field references the source PVC to enable cloning. This ensures the clone has the same content as the source but is an independent volume."
"Which of the following conditions must be met for the Kubernetes scheduler to consider storage capacity when scheduling a Pod?

A) The Pod uses a volume with Immediate volume binding mode and the CSIDriver has StorageCapacity set to true.
B) The Pod uses a volume that is already created and the StorageClass does not reference a CSI driver.
C) The Pod uses a volume that has not been created yet, with a StorageClass referencing a CSI driver that has StorageCapacity enabled, and uses WaitForFirstConsumer volume binding mode.
D) The Pod uses a CSI ephemeral volume and the CSIDriver has StorageCapacity set to false.","C) The Pod uses a volume that has not been created yet, with a StorageClass referencing a CSI driver that has StorageCapacity enabled, and uses WaitForFirstConsumer volume binding mode.","The correct answer is C. Storage capacity tracking is used by the scheduler only when a Pod uses a volume that has not yet been created, the StorageClass references a CSI driver with StorageCapacity set to true, and uses the WaitForFirstConsumer binding mode. Option A is incorrect because Immediate volume binding mode does not involve scheduler storage capacity checks; the volume is created independently of Pod scheduling. Option B is incorrect since storage capacity tracking is not used for volumes already created or those not using CSI drivers. Option D is incorrect because CSI ephemeral volumes are scheduled without considering storage capacity, regardless of the StorageCapacity field."
"What is the function of the CSIStorageCapacity objects in Kubernetes?

A) They store persistent volume claim specifications for CSI drivers.
B) They provide capacity information per storage class and node topology, produced by CSI drivers.
C) They define the CSIDriver custom resource specifications.
D) They track ephemeral volume usage across all nodes.","B) They provide capacity information per storage class and node topology, produced by CSI drivers.","The correct answer is B. CSIStorageCapacity objects are produced by CSI drivers and contain capacity information about storage available per storage class and node topology. Option A is incorrect because PersistentVolumeClaims are separate resources and not stored in CSIStorageCapacity objects. Option C is incorrect since CSIDriverSpec.StorageCapacity is a field within the CSIDriver object, not the CSIStorageCapacity object itself. Option D is incorrect because ephemeral volumes do not use storage capacity tracking."
"If a Pod uses multiple volumes with WaitForFirstConsumer binding mode, what is a potential scheduling limitation?

A) The Pod will always be scheduled successfully without retries.
B) One volume might be created in a topology segment that lacks enough capacity for other volumes, causing permanent scheduling failure.
C) The scheduler ignores storage capacity for multiple volumes.
D) The CSI driver automatically expands capacity when needed.","B) One volume might be created in a topology segment that lacks enough capacity for other volumes, causing permanent scheduling failure.","The correct answer is B. When multiple WaitForFirstConsumer volumes are used, scheduling can fail permanently if one volume is created in a segment with insufficient capacity for others. Option A is false because scheduling is not guaranteed to succeed on the first try. Option C is incorrect as the scheduler does consider storage capacity, but only in a simplistic manner and with limitations. Option D is incorrect because CSI drivers do not automatically expand capacity; manual intervention is required."
"Which statement best describes scheduling behavior for CSI ephemeral volumes?

A) The scheduler uses CSIStorageCapacity information to schedule Pods with ephemeral volumes.
B) Scheduling ignores storage capacity since ephemeral volumes are local and lightweight.
C) Ephemeral volumes require manual node selection by the user.
D) Ephemeral volumes use WaitForFirstConsumer binding mode exclusively.",B) Scheduling ignores storage capacity since ephemeral volumes are local and lightweight.,"The correct answer is B. CSI ephemeral volumes are assumed to be local to nodes and lightweight, so the scheduler does not consider storage capacity for them. Option A is incorrect because ephemeral volumes do not use storage capacity tracking. Option C is incorrect since scheduling is automatic, not manual. Option D is incorrect because ephemeral volumes do not use WaitForFirstConsumer binding mode; they have a different lifecycle."
Enable storage capacity tracking for a CSI driver named 'my-csi-driver'. Write the kubectl command to patch the CSIDriver object to set StorageCapacity to true.,"kubectl patch csidriver my-csi-driver -p '{""spec"":{""storageCapacity"":true}}' --type=merge","This command patches the CSIDriver resource named 'my-csi-driver' by setting the 'storageCapacity' field in its spec to true. The '-p' flag passes a JSON patch to update the resource, and '--type=merge' indicates a strategic merge patch. Enabling 'storageCapacity' tells Kubernetes scheduler to consider storage capacity for volumes managed by this CSI driver. Without this flag set to true, storage capacity tracking is disabled for that driver."
Create a CSIStorageCapacity object named 'csi-capacity-example' in the 'default' namespace for storage class 'fast-storage' with 500Gi capacity accessible only to node 'node01'. Assume the CSI driver name is 'my-csi-driver.example.com'. Provide the kubectl command to apply the YAML manifest.,"kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1beta1
kind: CSIStorageCapacity
metadata:
  name: csi-capacity-example
  namespace: default
capacity: ""500Gi""
storageClassName: fast-storage
driver: my-csi-driver.example.com
topology:
  node: node01
EOF","This command uses 'kubectl apply -f -' to create a CSIStorageCapacity object from the inline YAML manifest. The manifest specifies the capacity as 500Gi, the storage class name 'fast-storage', and the CSI driver name 'my-csi-driver.example.com'. The topology field restricts access to node 'node01' only, indicating that this capacity is available exclusively on that node. This object informs the scheduler about available storage capacity topology-wise."
"What is the default maximum number of Amazon Elastic Block Store (EBS) volumes that can be attached to a single Kubernetes Node?

A) 16
B) 25
C) 39
D) 64",C) 39,"The correct answer is C) 39. Kubernetes has a default maximum limit of 39 EBS volumes attached per Node. Option A) 16 is the default limit for Google Persistent Disk and Azure Disk, not EBS. Option B) 25 applies only to certain Amazon EC2 instance types (M5, C5, R5, T3, Z1D), but is not the default maximum for all nodes. Option D) 64 is the maximum number of disks for some Azure VM types, not EBS."
"Which feature gate must be enabled to allow CSI drivers to dynamically adjust the maximum number of volumes that can be attached to a Node at runtime?

A) DynamicVolumeLimits
B) MutableCSINodeAllocatableCount
C) NodeVolumeLimitAdjuster
D) CSIExpandVolumes",B) MutableCSINodeAllocatableCount,"The correct answer is B) MutableCSINodeAllocatableCount. This alpha feature gate allows CSI drivers to dynamically update volume limits. Option A) DynamicVolumeLimits refers to a different stable feature available since Kubernetes v1.17 but does not enable dynamic adjustment at runtime. Option C) NodeVolumeLimitAdjuster is a fictitious name. Option D) CSIExpandVolumes is related to volume resizing, not volume attachment limits."
"Which Kubernetes component honors the volume limits advertised by a CSI driver using the NodeGetInfo call?

A) kubelet
B) kube-controller-manager
C) kube-scheduler
D) kube-proxy",C) kube-scheduler,The correct answer is C) kube-scheduler. It enforces volume attachment limits and respects the maximum volumes per Node as reported by the CSI driver via NodeGetInfo. Option A) kubelet manages volume mounts on the Node but does not enforce scheduling limits. Option B) kube-controller-manager manages controllers but does not directly handle volume limit enforcement. Option D) kube-proxy manages network proxying and is unrelated to volume limits.
"You want to change the default maximum number of volumes per Node for the Kubernetes scheduler to 50 for EBS volumes. Which environment variable must you set before starting the scheduler?

A) MAX_VOLUME_ATTACHMENTS
B) KUBE_MAX_PD_VOLS
C) SCHEDULER_VOLUME_LIMIT
D) EBS_VOLUME_LIMIT",B) KUBE_MAX_PD_VOLS,The correct answer is B) KUBE_MAX_PD_VOLS. This environment variable configures the maximum number of volumes the scheduler considers attachable per Node. Option A) MAX_VOLUME_ATTACHMENTS is incorrect as it is not a recognized env variable in Kubernetes scheduler. Option C) SCHEDULER_VOLUME_LIMIT and D) EBS_VOLUME_LIMIT do not exist in the Kubernetes context.
Create a CSIDriver resource named 'hostpath.csi.k8s.io' that configures the CSI driver to update its Node allocatable volume count every 30 seconds.,"kubectl apply -f - <<EOF
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: hostpath.csi.k8s.io
spec:
  nodeAllocatableUpdatePeriodSeconds: 30
EOF","This command uses 'kubectl apply' with a here-document to create a CSIDriver resource named 'hostpath.csi.k8s.io'. The 'nodeAllocatableUpdatePeriodSeconds' field in the spec is set to 30, meaning kubelet will call the CSI driver's NodeGetInfo endpoint every 30 seconds to refresh volume limits. This field must be at least 10 seconds, and this feature requires the MutableCSINodeAllocatableCount feature gate to be enabled."
"How many volumes can be attached to a Google Compute Engine Node of a type that supports dynamic volume limits, according to Kubernetes documentation?

A) 16
B) 39
C) 64
D) Up to 127",D) Up to 127,"The correct answer is D) Up to 127. Kubernetes dynamically determines the maximum number of volumes that can be attached based on the Node type. For Google Compute Engine Nodes, some types support up to 127 volumes. Options A) 16 and C) 64 are default limits for other cloud providers or VM types. Option B) 39 is the default for Amazon EBS, not Google Compute Engine."
"Which Kubernetes feature gate must be enabled to use Volume Health Monitoring from the node side for CSI volumes?

A) VolumeSnapshotDataSource
B) CSIVolumeHealth
C) PersistentLocalVolumes
D) StorageObjectInUseProtection",B) CSIVolumeHealth,"The correct answer is B) CSIVolumeHealth. This feature gate enables Volume Health Monitoring on the node side, allowing kubelet to detect and report abnormal volume conditions for CSI volumes. Option A) VolumeSnapshotDataSource is unrelated to volume health monitoring; it concerns volume snapshotting. Option C) PersistentLocalVolumes relates to managing persistent local volumes, not health monitoring. Option D) StorageObjectInUseProtection is about protection of storage objects during deletion, not health monitoring."
"What happens when the External Health Monitor controller detects a node failure event with enable-node-watcher set to true?

A) It deletes all Pods on the failed node
B) It reports an Event on the related PersistentVolumeClaim
C) It automatically migrates the PVC to another node
D) It restarts the CSI driver on the node",B) It reports an Event on the related PersistentVolumeClaim,"The correct answer is B) It reports an Event on the related PersistentVolumeClaim (PVC). When enable-node-watcher is set to true, the External Health Monitor watches for node failure events and reports an Event on PVCs used by Pods on the failed node. Option A) is incorrect because the controller does not delete Pods. Option C) is incorrect as Kubernetes does not automatically migrate PVCs; manual intervention or higher-level controllers handle that. Option D) is incorrect because the health monitor does not restart CSI drivers."
"Which Kubernetes component exposes Volume Health information as metrics including 'kubelet_volume_stats_health_status_abnormal'?

A) kube-apiserver
B) kube-controller-manager
C) kubelet
D) etcd",C) kubelet,"The correct answer is C) kubelet. The kubelet on the node side exposes Volume Health information as metrics, including the 'kubelet_volume_stats_health_status_abnormal' metric with labels for namespace and persistentvolumeclaim. Option A) kube-apiserver is the API server and does not expose these metrics directly. Option B) kube-controller-manager manages controllers but does not expose volume health metrics. Option D) etcd is the cluster's key-value store and unrelated to metrics exposure."
"A CSI driver supports Volume Health Monitoring from the controller side. What Kubernetes resource will show events if an abnormal volume condition is detected?

A) Pod
B) PersistentVolumeClaim
C) Node
D) PersistentVolume",B) PersistentVolumeClaim,"The correct answer is B) PersistentVolumeClaim (PVC). When a CSI driver supports volume health monitoring from the controller side, abnormal conditions are reported as Events on the PVC associated with the volume. Option A) Pod is incorrect because controller-side monitoring reports events on PVC, not Pods. Option C) Node is incorrect since node events are monitored separately. Option D) PersistentVolume is a cluster resource but events for volume health monitoring are reported on PVCs, which are user-facing claims."
Create a PersistentVolumeClaim named 'data-pvc' in the default namespace requesting 10Gi of storage with access mode ReadWriteOnce and storage class 'fast-storage'.,kubectl create pvc data-pvc --storage=10Gi --access-modes=ReadWriteOnce --storage-class=fast-storage,This command creates a PersistentVolumeClaim named 'data-pvc' in the default namespace using 'kubectl create pvc'. The '--storage=10Gi' flag specifies the requested storage size. The '--access-modes=ReadWriteOnce' flag sets the access mode allowing the volume to be mounted as read-write by a single node. The '--storage-class=fast-storage' flag requests a specific StorageClass named 'fast-storage'. This PVC can then be used by Pods to mount storage volumes.
Enable the External Health Monitor controller's node failure event watching feature in a Kubernetes cluster.,"kubectl patch deployment external-health-monitor-controller -n kube-system --patch '{""spec"":{""template"":{""spec"":{""containers"":[{""name"":""external-health-monitor-controller"",""args"":[""--enable-node-watcher=true""]}]}}}}'",This command patches the Deployment named 'external-health-monitor-controller' in the 'kube-system' namespace to set the '--enable-node-watcher=true' argument. This enables the controller to watch for node failure events and report Events on PVCs accordingly. The patch modifies the container args in the pod template spec. It's important to target the correct Deployment and namespace where this controller runs. This feature is necessary for node failure volume health monitoring.
"Which of the following storage features is NOT supported on Windows nodes in Kubernetes?

A) Volume subpath mounts
B) Read-only volumes
C) In-tree volume plugins like azureFile
D) Persistent storage with Pod volume sharing",A) Volume subpath mounts,"The correct answer is A) Volume subpath mounts. Windows nodes in Kubernetes do not support volume subpath mounts, meaning only the entire volume can be mounted in a Windows container. Option B) Read-only volumes are supported on Windows even though read-only root filesystems are not. Option C) In-tree volume plugins such as azureFile are supported on Windows nodes. Option D) Persistent storage with Pod volume sharing is a core Kubernetes feature supported on Windows as well."
"Why are read-only root filesystems not supported in Windows containers in Kubernetes?

A) Because Windows containers require write access for the Windows registry and SAM database
B) Because Windows does not support NTFS
C) Because volume mounts can only target individual files
D) Because Docker does not support volumes on Windows",A) Because Windows containers require write access for the Windows registry and SAM database,"The correct answer is A) Because Windows containers require write access for the Windows registry and SAM database. These components always require write permissions, making read-only root filesystems unsupported. Option B) is incorrect because Windows does support NTFS, which is the base filesystem. Option C) is incorrect as volume mounts target directories, not individual files, but this is unrelated to read-only root filesystem support. Option D) is incorrect because Docker does support volumes on Windows, although with certain limitations."
"Which Kubernetes volume plugin type has been deprecated as of version 1.23 for Windows nodes?

A) CSI Plugins
B) FlexVolume plugins
C) In-tree volume plugins
D) None of the above",B) FlexVolume plugins,The correct answer is B) FlexVolume plugins. FlexVolumes have been deprecated as of Kubernetes version 1.23 and should not be used on Windows nodes. Option A) CSI Plugins are the recommended plugin type moving forward. Option C) In-tree volume plugins like azureFile and vsphereVolume are still supported on Windows. Option D) is incorrect because FlexVolumes have indeed been deprecated.
"Which of the following storage functionalities is supported on Windows nodes in Kubernetes?

A) Block device mapping
B) Memory as the storage medium (emptyDir.medium=Memory)
C) Read-only volumes
D) Host mount projection",C) Read-only volumes,"The correct answer is C) Read-only volumes. While read-only root filesystems are not supported, read-only volumes themselves are supported on Windows nodes. Option A) Block device mapping is not supported on Windows nodes. Option B) Using memory as a storage medium (emptyDir with medium set to Memory) is not supported on Windows. Option D) Host mount projection is also unsupported on Windows nodes."
"Create a persistent volume claim named 'win-pvc' in the default namespace with access mode ReadWriteOnce, requesting 10Gi storage, using the 'azurefile' storage class.",kubectl create pvc win-pvc --access-modes=ReadWriteOnce --resources=requests.storage=10Gi --storage-class=azurefile,"This command creates a persistent volume claim (PVC) named 'win-pvc' in the default namespace. The '--access-modes=ReadWriteOnce' flag specifies that the volume can be mounted as read-write by a single node. The '--resources=requests.storage=10Gi' flag requests 10Gi of storage. The '--storage-class=azurefile' flag specifies the storage backend, which is supported on Windows nodes as an in-tree volume plugin. No namespace flag is given, so it defaults to the 'default' namespace."
Create a Pod named 'win-pod' in the default namespace that mounts a volume named 'data' from a PVC called 'win-pvc' at the path 'C:\data'. Use the Windows container image 'mcr.microsoft.com/windows/servercore:ltsc2022'.,"kubectl run win-pod --image=mcr.microsoft.com/windows/servercore:ltsc2022 --restart=Never --overrides='{ ""spec"": { ""containers"": [{ ""name"": ""win-container"", ""image"": ""mcr.microsoft.com/windows/servercore:ltsc2022"", ""volumeMounts"": [{ ""name"": ""data"", ""mountPath"": ""C:\\data"" }] }], ""volumes"": [{ ""name"": ""data"", ""persistentVolumeClaim"": { ""claimName"": ""win-pvc"" } }] } }'","This command creates a Pod named 'win-pod' with a single container using the specified Windows Server Core image. The '--restart=Never' flag ensures it is a Pod, not a Deployment. The '--overrides' flag injects the volume and volumeMount configuration into the Pod spec: it mounts the persistent volume claim 'win-pvc' as a volume named 'data' and mounts it inside the container at 'C:\data'. Double backslashes '\\' are used to escape backslashes in JSON strings. This mounts persistent storage inside the Windows container."
"Which of the following is the recommended format for Kubernetes configuration files and why?

A) JSON, because it is more compact and easier to parse
B) YAML, because it is more user-friendly and supports comments
C) XML, because it is widely used and supports schemas
D) TOML, because it has better type support","B) YAML, because it is more user-friendly and supports comments","The correct answer is B) YAML, because it is more user-friendly and supports comments. Kubernetes recommends using YAML over JSON primarily for readability and maintainability reasons, as YAML supports comments and is easier for humans to write and understand. Option A) JSON is incorrect because although it is compact and widely supported, it lacks support for comments and is less readable for complex configurations. Option C) XML is not supported by Kubernetes for configuration files, making it incorrect. Option D) TOML is not used in Kubernetes for configuration and thus is incorrect."
"Why should you avoid using naked Pods in Kubernetes whenever possible?

A) Naked Pods do not support environment variables
B) Naked Pods cannot be scheduled on multiple nodes
C) Naked Pods will not be rescheduled automatically if the node fails
D) Naked Pods do not support volume mounts",C) Naked Pods will not be rescheduled automatically if the node fails,"The correct answer is C) Naked Pods will not be rescheduled automatically if the node fails. Naked Pods are standalone Pods that are not managed by higher-level controllers like ReplicaSets or Deployments, so Kubernetes won't recreate them if the node running the Pod fails. Option A) is incorrect because naked Pods do support environment variables like any other Pod. Option B) is incorrect since scheduling depends on node availability, not on whether the Pod is naked. Option D) is incorrect because naked Pods can use volume mounts just like Pods managed by controllers."
"When creating a Kubernetes Service and corresponding backend workloads, which of the following is a best practice regarding resource creation order?

A) Create backend workloads first, then create the Service
B) Create the Service first, then create backend workloads
C) Create both simultaneously to avoid race conditions
D) The order does not matter, Kubernetes handles dependencies automatically","B) Create the Service first, then create backend workloads","The correct answer is B) Create the Service first, then create backend workloads. This is recommended because when a container starts, it receives environment variables for all existing Services. If the Service does not exist before the Pod starts, these environment variables will not be populated, which might cause issues. Option A) is incorrect because creating backend workloads first will not populate service-related environment variables in Pods. Option C) is impractical as Kubernetes does not guarantee simultaneous creation order. Option D) is incorrect because Kubernetes environment variables are populated only at container start time, so the creation order matters."
"Which of the following label key-value pairs best represents semantic attributes for selecting frontend Pods of an application named MyApp in a 'test' phase?

A) { app: MyApp, tier: backend, phase: prod }
B) { app.kubernetes.io/name: MyApp, tier: frontend, phase: test }
C) { name: MyApp, role: frontend, environment: dev }
D) { app.kubernetes.io/name: MyApp, tier: backend, status: test }","B) { app.kubernetes.io/name: MyApp, tier: frontend, phase: test }","The correct answer is B) { app.kubernetes.io/name: MyApp, tier: frontend, phase: test }. This set uses recommended Kubernetes common labels (app.kubernetes.io/name) and meaningful semantic attributes (tier: frontend, phase: test) to identify the Pods. Option A) is incorrect because tier is backend, not frontend, and phase is prod, not test. Option C) uses non-standard labels and does not follow Kubernetes common label conventions. Option D) incorrectly labels tier as backend and uses status instead of phase, which is less semantically meaningful."
"Create a headless Service named 'db-service' in namespace 'prod' for Pods with label app=database, exposing port 5432 without kube-proxy load balancing.",kubectl create service clusterip db-service --cluster-ip=None --tcp=5432:5432 -n prod --dry-run=client -o yaml | kubectl apply -f -,"This command creates a headless Service by specifying '--cluster-ip=None', which disables kube-proxy load balancing. The service is named 'db-service' and created in the 'prod' namespace using '-n prod'. The '--tcp=5432:5432' flag exposes port 5432. The command uses 'kubectl create service clusterip' with '--dry-run=client -o yaml' to generate a manifest, then pipes it to 'kubectl apply -f -' to create the resource idempotently. This approach allows specifying headless Service semantics, which is useful for service discovery without load balancing."
"Apply all configuration files in the directory './configs' to the cluster, ensuring that any changes are updated and new resources created.",kubectl apply -f ./configs,"The command 'kubectl apply -f ./configs' recursively applies all Kubernetes manifests found in the './configs' directory, including any '.yaml', '.yml', or '.json' files. This command creates new resources if they don't exist and updates existing ones to match the specified configuration. Using 'apply' is preferred for declarative management of resources. This approach simplifies managing multiple related configuration files and ensures that the cluster state converges to the desired state described by the files."
"Which of the following statements about ConfigMaps in Kubernetes is TRUE?

A) ConfigMaps can store confidential data securely using encryption.
B) ConfigMaps can be consumed by Pods as environment variables, command-line arguments, or configuration files.
C) ConfigMaps can be shared across namespaces without any extra configuration.
D) ConfigMaps can store data exceeding 1 MiB without any issues.","B) ConfigMaps can be consumed by Pods as environment variables, command-line arguments, or configuration files.","Option B is correct because ConfigMaps are designed to provide non-confidential configuration data to Pods through environment variables, command-line arguments, or mounted volumes as configuration files. Option A is incorrect because ConfigMaps do not provide secrecy or encryption; Secrets should be used for confidential data. Option C is incorrect as Pods and ConfigMaps must be in the same namespace unless you access ConfigMaps via the Kubernetes API directly with custom code. Option D is incorrect because ConfigMaps have a size limit of 1 MiB; larger data should be stored using volumes or external services."
"What happens if you mount a ConfigMap as a volume in a Pod without specifying the 'items' field in the volume definition?

A) No files are created inside the volume.
B) Only the first key in the ConfigMap is created as a file.
C) Every key in the ConfigMap becomes a file with the same name as the key.
D) The ConfigMap keys are mounted as environment variables instead of files.",C) Every key in the ConfigMap becomes a file with the same name as the key.,"Option C is correct because when mounting a ConfigMap as a volume without specifying 'items', all keys in the ConfigMap are projected as files inside the mountPath, each with the filename matching the key. Option A is incorrect as files are indeed created. Option B is incorrect because all keys, not just the first, are mounted. Option D is incorrect because environment variables are a different consumption method and unrelated to volume mounting."
"Which of the following is a valid reason to use an immutable ConfigMap in Kubernetes?

A) To allow dynamic runtime updates to configuration without restarting Pods.
B) To prevent accidental changes to configuration data after creation.
C) To store large binary data exceeding 1 MiB.
D) To enable encryption of ConfigMap data automatically.",B) To prevent accidental changes to configuration data after creation.,"Option B is correct because setting the 'immutable' field on a ConfigMap ensures that its data cannot be changed after creation, thus preventing accidental modifications. Option A is incorrect because immutable ConfigMaps cannot be updated, so dynamic runtime updates are not possible. Option C is incorrect because ConfigMaps are limited to 1 MiB and are not intended for large data storage. Option D is incorrect because ConfigMaps do not provide encryption; Secrets or external tools are needed for that."
"You want to create a ConfigMap named 'app-config' from a file called 'config.properties'. Which kubectl command should you use?

A) kubectl create configmap app-config --from-literal=config.properties
B) kubectl create configmap app-config --from-file=config.properties
C) kubectl apply -f config.properties -n app-config
D) kubectl set configmap app-config --file=config.properties",B) kubectl create configmap app-config --from-file=config.properties,"Option B is correct because the '--from-file' flag creates a ConfigMap with the contents of the file 'config.properties' as a key-value pair. Option A is invalid because '--from-literal' expects a key=value pair, not a filename. Option C is incorrect because 'kubectl apply' applies manifest files, not creates ConfigMaps from files. Option D is not a valid kubectl command."
"Create a Pod named 'configmap-demo-pod' in the default namespace that runs an alpine image, sleeps for 3600 seconds, and sets environment variables 'PLAYER_INITIAL_LIVES' and 'UI_PROPERTIES_FILE_NAME' from the keys 'player_initial_lives' and 'ui_properties_file_name' respectively, in a ConfigMap named 'game-demo'.",kubectl run configmap-demo-pod --image=alpine --restart=Never -- sleep 3600 && kubectl set env pod/configmap-demo-pod --from=configmap/game-demo,"The provided command sequence first creates a Pod named 'configmap-demo-pod' running the alpine image with the sleep command for 3600 seconds and restart policy set to Never. The second command uses 'kubectl set env' to set the environment variables from the ConfigMap 'game-demo'. However, this approach sets all keys as environment variables rather than mapping specific keys. For precise key mapping, a YAML Pod manifest specifying env variables with 'valueFrom.configMapKeyRef' is preferred. This answer combines pragmatic command usage but note that for exam scenarios, providing a manifest might be expected for exact key mappings."
Write the kubectl command to create a ConfigMap named 'myconfigmap' with two literal key-value pairs: 'color=blue' and 'size=large'.,kubectl create configmap myconfigmap --from-literal=color=blue --from-literal=size=large,This command creates a ConfigMap named 'myconfigmap' with two keys: 'color' set to 'blue' and 'size' set to 'large'. The '--from-literal' flag allows specifying individual key-value pairs directly in the command line. Multiple '--from-literal' flags can be used to add multiple keys. This is a straightforward way to create small ConfigMaps without needing a file.
"Which of the following is TRUE about Kubernetes Secrets by default?

A) Secrets are encrypted at rest in etcd by default.
B) Anyone with API access can retrieve or modify a Secret.
C) Secrets are automatically rotated every 24 hours.
D) Secrets cannot be accessed by Pods once created.",B) Anyone with API access can retrieve or modify a Secret.,"The correct answer is B) Anyone with API access can retrieve or modify a Secret. By default, Kubernetes stores Secrets unencrypted in etcd, making them accessible to anyone with API or etcd access. Option A is incorrect because encryption at rest is not enabled by default and must be configured explicitly. Option C is incorrect as Kubernetes does not automatically rotate Secrets; rotation must be managed externally. Option D is incorrect because Secrets are designed to be consumed by Pods, for example, as environment variables or mounted volumes."
"What is the recommended Kubernetes approach to obtain short-lived, automatically rotating ServiceAccount tokens in v1.22 and later?

A) Using legacy ServiceAccount token Secrets.
B) Using the TokenRequest API.
C) Using static tokens mounted manually.
D) Embedding tokens directly in Pod specs.",B) Using the TokenRequest API.,"The correct answer is B) Using the TokenRequest API. Starting with Kubernetes v1.22, the preferred method for ServiceAccount tokens is to use short-lived tokens obtained via the TokenRequest API, which improves security by automatic rotation. Option A is incorrect as legacy ServiceAccount token Secrets are long-lived and considered less secure. Option C is incorrect because static tokens are deprecated and do not support rotation. Option D is incorrect as embedding tokens directly in Pod specs exposes secrets in plaintext and is not recommended."
"When creating a Secret with 'kubectl create secret generic', what is the default Secret type if none is specified?

A) kubernetes.io/tls
B) kubernetes.io/basic-auth
C) Opaque
D) kubernetes.io/service-account-token",C) Opaque,The correct answer is C) Opaque. The Opaque type is the default for Secrets created without specifying a type and is used for arbitrary user-defined data. Option A (kubernetes.io/tls) is specific for TLS credentials. Option B (kubernetes.io/basic-auth) is for basic authentication credentials. Option D (kubernetes.io/service-account-token) is reserved for ServiceAccount tokens and is not the default.
"Which of the following is NOT a recommended best practice for securing Kubernetes Secrets?

A) Enabling Encryption at Rest for Secrets.
B) Using RBAC rules with least-privilege access.
C) Storing Secrets in container images.
D) Restricting Secret access to specific containers.",C) Storing Secrets in container images.,The correct answer is C) Storing Secrets in container images. Storing Secrets inside container images is not recommended because it can expose sensitive data and make rotation difficult. Option A is correct and recommended to protect Secrets at rest. Option B is a best practice to limit access scope. Option D is recommended to ensure that only intended containers can access the Secrets.
Create a Secret named 'dotfile-secret' with a data key '.secret-file' containing the base64 encoded value 'value-2'. The Secret should be of default type.,kubectl create secret generic dotfile-secret --from-literal=.secret-file=value-2,This command creates a Secret named 'dotfile-secret' of type 'Opaque' (default) using the 'generic' subcommand. The '--from-literal=.secret-file=value-2' flag sets a data key named '.secret-file' with the literal value 'value-2'. The value will be base64 encoded automatically by kubectl. This key name starting with a dot represents a hidden dotfile when mounted into a Pod volume.
"Mount a Secret named 'dotfile-secret' as a volume into a Pod named 'secret-dotfiles-pod', and mount it at path '/etc/secret-volume' as read-only.","kubectl patch pod secret-dotfiles-pod --patch '{""spec"":{""volumes"":[{""name"":""secret-volume"",""secret"":{""secretName"":""dotfile-secret""}}],""containers"":[{""name"":""dotfile-test-container"",""volumeMounts"":[{""name"":""secret-volume"",""mountPath"":""/etc/secret-volume"",""readOnly"":true}]}]}}'","This patch command updates the Pod 'secret-dotfiles-pod' to add a volume named 'secret-volume' that sources from the Secret 'dotfile-secret'. It also mounts the volume into the container named 'dotfile-test-container' at '/etc/secret-volume' with readOnly set to true, ensuring the Secret files (including dotfiles) are accessible but not modifiable by the container. The patch is necessary if the Pod is already running; otherwise, the volume and volumeMounts would be specified in the Pod manifest."
"What is the primary purpose of a liveness probe in Kubernetes?

A) To determine if a container is ready to accept traffic
B) To verify if the application within a container has started
C) To determine when to restart a container
D) To monitor the resource usage of a container",C) To determine when to restart a container,"The correct answer is C) To determine when to restart a container. Liveness probes are designed to detect when an application inside a container is unhealthy and needs to be restarted by the kubelet. Option A is incorrect because readiness probes, not liveness probes, determine if a container is ready to accept traffic. Option B is incorrect because startup probes verify if the application has started, particularly for slow-starting containers. Option D is incorrect as resource monitoring is not the function of liveness probes."
"Which statement about readiness probes is correct?

A) They restart the container if the probe fails repeatedly
B) They determine when a container is ready to accept traffic
C) They run only once at container startup
D) They disable liveness probes when configured",B) They determine when a container is ready to accept traffic,"The correct answer is B) They determine when a container is ready to accept traffic. Readiness probes help Kubernetes decide whether a Pod should receive traffic by adding or removing it from Service endpoints. Option A is incorrect because readiness probes do not restart containers; liveness probes do that. Option C is incorrect because readiness probes run continuously during the container lifecycle, not just once at startup. Option D is incorrect because disabling liveness probes when configuring readiness probes is not the behavior; only startup probes disable them until success."
"What effect does configuring a startup probe have on liveness and readiness probes?

A) It disables both liveness and readiness probes until the startup probe succeeds
B) It runs liveness probes more frequently during startup
C) It replaces readiness probes permanently
D) It has no effect on liveness or readiness probes",A) It disables both liveness and readiness probes until the startup probe succeeds,The correct answer is A) It disables both liveness and readiness probes until the startup probe succeeds. This prevents premature killing of slow-starting containers by liveness probes. Option B is incorrect because startup probes do not cause liveness probes to run more frequently. Option C is incorrect as startup probes do not replace readiness probes permanently; readiness probes resume after startup probe success. Option D is incorrect because startup probes explicitly affect the behavior of liveness and readiness probes during startup.
"Which of the following statements about the execution frequency of probes is true?

A) Liveness and readiness probes run only once at container startup
B) Startup probes run periodically throughout the container lifecycle
C) Liveness and readiness probes run periodically, startup probes run only at startup
D) All probes run continuously and never stop","C) Liveness and readiness probes run periodically, startup probes run only at startup","The correct answer is C) Liveness and readiness probes run periodically, startup probes run only at startup. Liveness and readiness probes are continuous health checks during container lifecycle. Startup probes run only during container startup to verify the application has started. Option A is incorrect because liveness and readiness probes run repeatedly, not just once. Option B is incorrect since startup probes do not run periodically after startup. Option D is incorrect because startup probes stop once they succeed."
Create a Pod named 'webapp' with a single container using the image 'myapp:latest' and configure a readiness probe using an HTTP GET on path '/health' at port 8080 with an initial delay of 10 seconds and a period of 5 seconds.,"kubectl run webapp --image=myapp:latest --restart=Never --dry-run=client -o yaml > pod.yaml && 
cat <<EOF >> pod.yaml
spec:
  containers:
  - name: webapp
    image: myapp:latest
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
EOF
kubectl apply -f pod.yaml","This sequence first generates a Pod manifest named 'webapp' with the specified image using 'kubectl run' and '--dry-run=client -o yaml'. Then, it appends the readinessProbe configuration using HTTP GET on '/health' at port 8080 with an initial delay of 10 seconds before the first probe and a probe period of 5 seconds. Finally, 'kubectl apply -f pod.yaml' creates the Pod with the readiness probe configured. The initialDelaySeconds prevents the probe from running before the container is ready, and periodSeconds defines how often the probe runs."
Configure a Deployment named 'api-server' with 2 replicas using the image 'api:v2' and add a startup probe that uses an exec command 'cat /tmp/started' with a failure threshold of 30 and periodSeconds of 5.,"kubectl create deployment api-server --image=api:v2 --replicas=2 --dry-run=client -o yaml > deploy.yaml && 
cat <<EOF >> deploy.yaml
spec:
  template:
    spec:
      containers:
      - name: api-server
        image: api:v2
        startupProbe:
          exec:
            command:
              - cat
              - /tmp/started
          failureThreshold: 30
          periodSeconds: 5
EOF
kubectl apply -f deploy.yaml","This command first creates a Deployment manifest for 'api-server' with 2 replicas using 'kubectl create deployment' and outputs YAML without creating the resource. It then appends the startupProbe configuration to the container spec using the exec command 'cat /tmp/started'. The 'failureThreshold: 30' combined with 'periodSeconds: 5' means the kubelet will wait up to 150 seconds (30*5) for the startup probe to succeed before considering the container failed. Adding a startup probe delays liveness and readiness probes until the probe passes, suitable for slow-starting applications. Finally, 'kubectl apply -f deploy.yaml' creates the Deployment with the startup probe."
"What happens if you specify a resource limit but do not specify a resource request for a container in Kubernetes?

A) The Pod will fail to schedule.
B) Kubernetes sets the request equal to the limit.
C) The request is set to zero.
D) The container ignores the limit.",B) Kubernetes sets the request equal to the limit.,"The correct answer is B) Kubernetes sets the request equal to the limit. According to Kubernetes resource management behavior, if you specify a limit but no request, and no admission controller sets a default request, Kubernetes will copy the limit value to the request. Option A is incorrect because the Pod still schedules successfully. Option C is incorrect as the request is never set to zero automatically. Option D is incorrect because the container must respect the specified limit."
"How does Kubernetes enforce CPU limits for containers?

A) By killing the container when it exceeds CPU usage.
B) By throttling CPU access using the Linux kernel.
C) By denying the container network access.
D) By evicting the entire Pod.",B) By throttling CPU access using the Linux kernel.,"The correct answer is B) By throttling CPU access using the Linux kernel. Kubernetes enforces CPU limits by leveraging Linux cgroups to throttle the container's CPU usage, preventing it from exceeding its allocated CPU quota. Option A is incorrect because containers are not killed for CPU overusage, unlike memory. Option C is unrelated to CPU limits. Option D is incorrect because eviction is not the method used for CPU limit enforcement."
"Which statement best describes memory limit enforcement for containers in Kubernetes?

A) Containers cannot exceed memory limits under any circumstance.
B) Memory limits are enforced reactively; containers may be killed when kernel detects memory pressure.
C) Memory limits are enforced preemptively by the kubelet.
D) Memory limits are ignored if requests are not set.",B) Memory limits are enforced reactively; containers may be killed when kernel detects memory pressure.,The correct answer is B) Memory limits are enforced reactively; containers may be killed when kernel detects memory pressure. Kubernetes relies on the kernel's OOM killer to terminate containers exceeding memory limits only when memory pressure occurs. Option A is incorrect because containers can temporarily exceed limits. Option C is incorrect since preemptive enforcement (MemoryQoS) is an alpha feature and currently stalled. Option D is wrong because memory limits are enforced regardless of requests.
"Which resource type cannot be overcommitted in Kubernetes?

A) CPU
B) Memory
C) Hugepages
D) Ephemeral storage",C) Hugepages,"The correct answer is C) Hugepages. Hugepages are a Linux-specific resource that Kubernetes does not allow to be overcommitted; the node kernel allocates fixed blocks, and allocations exceeding limits fail. Options A and B are incorrect because CPU and memory resources can be overcommitted by specifying requests lower than limits. Option D, ephemeral storage, can also be overcommitted depending on configuration."
"Create a Pod named 'test-pod' with a single container 'myapp' using image 'nginx:latest' that requests 250m CPU and 64Mi memory, and limits 500m CPU and 128Mi memory.","kubectl run test-pod --image=nginx:latest --restart=Never --requests=cpu=250m,memory=64Mi --limits=cpu=500m,memory=128Mi",This command creates a Pod named 'test-pod' with a single container 'myapp' running the 'nginx:latest' image. The '--restart=Never' flag ensures a Pod rather than a Deployment is created. The '--requests' flag specifies resource requests: 250 milliCPU and 64Mi memory. The '--limits' flag specifies resource limits: 500 milliCPU and 128Mi memory. This sets proper resource management for scheduling and runtime enforcement.
"How can you specify resource requests and limits at the Pod level starting Kubernetes 1.32 alpha, and which resources are supported?

A) Use spec.resources in the Pod spec for cpu and memory only.
B) Use annotations to specify limits for all resource types.
C) Use spec.containers[].resources for Pod-level limits.
D) Pod-level resource requests and limits are not supported in Kubernetes 1.32.",A) Use spec.resources in the Pod spec for cpu and memory only.,"The correct answer is A) Use spec.resources in the Pod spec for cpu and memory only. Starting in Kubernetes 1.32 alpha, you can specify resource requests and limits at the Pod level in the Pod spec using spec.resources for cpu and memory to declare an overall resource budget. Option B is incorrect as annotations are not used for this purpose. Option C is incorrect because spec.containers[].resources apply to individual containers, not the Pod as a whole. Option D is incorrect because Pod-level resource specification is supported in alpha."
"Which of the following is NOT a valid method for kubectl to determine the current context it should use?

A) The --context command-line flag
B) The current-context field from merged kubeconfig files
C) The KUBECONFIG environment variable directly specifying the context
D) Default kubeconfig file at $HOME/.kube/config if no other context is set",C) The KUBECONFIG environment variable directly specifying the context,"The correct answer is C) because the KUBECONFIG environment variable lists one or more kubeconfig files to merge, but does not directly specify a context. Instead, contexts are read from the merged files or can be set with the --context flag or from the current-context field. Option A is valid because --context overrides the current context. Option B is valid as current-context from merged files is used when no --context flag is set. Option D is valid because if KUBECONFIG is not set, kubectl falls back to the default kubeconfig file."
"What happens when multiple kubeconfig files are merged via the KUBECONFIG environment variable and two files define the same user with different credentials?

A) The credentials from the last file in the list overwrite earlier ones
B) The credentials from the first file in the list are used, and conflicting entries in later files are ignored
C) kubectl merges the credentials from both files
D) kubectl throws an error and refuses to merge the files","B) The credentials from the first file in the list are used, and conflicting entries in later files are ignored","The correct answer is B) because when merging kubeconfig files, kubectl uses the first occurrence of a particular value or map key and ignores conflicting entries in subsequent files. Option A is incorrect because the last file does not overwrite earlier ones. Option C is incorrect since credentials cannot be merged. Option D is incorrect because merge conflicts on duplicate keys do not cause an error; the first wins."
"Which of the following statements about the --kubeconfig flag is TRUE?

A) It can be specified multiple times to merge several kubeconfig files
B) If set, kubectl ignores the KUBECONFIG environment variable and uses only the specified file
C) It overrides the current-context specified in kubeconfig files
D) It sets the directory where kubectl looks for kubeconfig files","B) If set, kubectl ignores the KUBECONFIG environment variable and uses only the specified file","The correct answer is B). When the --kubeconfig flag is set, kubectl uses only that single file and does not merge files listed in the KUBECONFIG environment variable. Option A is wrong because only one instance of --kubeconfig is allowed. Option C is wrong because --kubeconfig does not override context; --context flag does. Option D is incorrect because --kubeconfig specifies a file path, not a directory."
"Create a kubeconfig context named 'prod-context' that uses the cluster named 'prod-cluster', the user 'admin-user', and the namespace 'production'.",kubectl config set-context prod-context --cluster=prod-cluster --user=admin-user --namespace=production,"This command creates or modifies a context named 'prod-context' in the kubeconfig file. The '--cluster=prod-cluster' flag associates the context with the specified cluster. The '--user=admin-user' flag sets the user credentials for the context. The '--namespace=production' flag defines the default namespace for this context. Using contexts allows quick switching between clusters, users, and namespaces."
Set the environment variable KUBECONFIG on a Linux or macOS system to merge two kubeconfig files located at '/etc/kube/config1' and '/home/user/.kube/config2'.,export KUBECONFIG=/etc/kube/config1:/home/user/.kube/config2,"This command sets the KUBECONFIG environment variable to a colon-delimited list of kubeconfig files on Linux/macOS. This instructs kubectl to merge these files when determining cluster access configuration. The colon ':' is the path separator on Linux and Mac. On Windows, a semicolon ';' is used instead."
Display the effective kubeconfig configuration after merging all files specified by the KUBECONFIG environment variable.,kubectl config view,"This command shows the merged, effective kubeconfig configuration from all files specified in the KUBECONFIG environment variable (or the default kubeconfig if KUBECONFIG is not set). It outputs clusters, users, contexts, and other access info. This helps verify that merging behavior and contexts are as expected."
"Which process isolation mechanism does Kubernetes use for Windows containers to provide logical isolation from the host?

A) cgroups
B) Namespaces
C) Job objects with system namespace filter
D) Docker namespaces",C) Job objects with system namespace filter,"The correct answer is C) Job objects with system namespace filter. Windows containers use job objects combined with a system namespace filter to isolate container processes from the host, which is different from Linux where cgroups (Option A) are used. Option B, namespaces, is a Linux kernel feature and not applicable directly to Windows containers. Option D, Docker namespaces, is not a Windows isolation mechanism; Docker namespaces are part of Linux container isolation."
"Why are privileged containers not available on Windows nodes in Kubernetes?

A) Windows does not support containerization
B) Job objects with namespace filtering prevent asserting system privileges
C) Windows containers run as root by default
D) Kubernetes disables privileged containers on Windows for security reasons",B) Job objects with namespace filtering prevent asserting system privileges,"The correct answer is B) Job objects with namespace filtering prevent asserting system privileges. Windows uses job objects with system namespace filters that isolate container processes so they cannot assert system privileges on the host, making privileged containers unavailable. Option A is false because Windows does support containerization. Option C is incorrect; Windows containers do not run as root by default. Option D is misleading; the limitation is due to Windows OS mechanisms, not a Kubernetes policy."
"What is a key difference in memory management between Linux and Windows nodes in Kubernetes?

A) Windows uses cgroups for memory limits
B) Windows has an out-of-memory killer like Linux
C) Windows treats user-mode memory as virtual and uses mandatory pagefiles
D) Linux does not support memory overcommit",C) Windows treats user-mode memory as virtual and uses mandatory pagefiles,"The correct answer is C) Windows treats user-mode memory as virtual and uses mandatory pagefiles. Unlike Linux, Windows does not have an out-of-memory killer and instead relies on paging to disk when physical memory is exhausted. Option A is wrong because Windows does not use cgroups. Option B is incorrect as Windows lacks an OOM killer. Option D is false; Linux supports memory overcommit, whereas Windows does not."
"Which kubelet flag on Windows nodes allows you to increase the CPU scheduling priority of the kubelet process?

A) --cpu-priority
B) --windows-priorityclass
C) --kube-reserved
D) --system-reserved",B) --windows-priorityclass,The correct answer is B) --windows-priorityclass. This flag sets the scheduling priority class for the kubelet process on Windows to ensure it receives more CPU time slices. Option A is invalid as there is no such flag. Options C and D (--kube-reserved and --system-reserved) are used to reserve CPU and memory resources but do not affect kubelet process priority.
Create a Windows node kubelet configuration that reserves 2Gi of memory and 500m CPU for system and kube components using flags.,"kubelet --kube-reserved=memory=2Gi,cpu=500m --system-reserved=memory=2Gi,cpu=500m","This command sets the kubelet flags --kube-reserved and --system-reserved to reserve 2Gi of memory and 500m CPU for Kubernetes system components and the OS respectively on a Windows node. Reserving resources helps the node report accurate allocatable resources and prevents overcommitment. The memory value must be specified with units (Gi), and CPU in millicores (m). This is important for resource management and scheduling."
Explain why it is recommended to set resource limits on containers running on Windows nodes in Kubernetes.,"Setting resource limits on containers subtracts from NodeAllocatable, helping the scheduler prevent over-provisioning and node resource exhaustion.","On Windows nodes, resource limits are critical because Windows does not have an OOM killer and relies on paging, which can degrade performance if memory is exhausted. By setting CPU and memory limits, Kubernetes can calculate accurate NodeAllocatable resources, thus preventing scheduling too many pods that could cause node instability. Without limits, pods may consume resources uncontrollably, leading to unhealthy nodes."
"What is the primary role of the kube-scheduler in Kubernetes?

A) To monitor the health of Nodes and Pods
B) To assign newly created Pods to appropriate Nodes
C) To manage the lifecycle of Pods and containers
D) To store cluster state and configuration",B) To assign newly created Pods to appropriate Nodes,"The correct answer is B) To assign newly created Pods to appropriate Nodes. The kube-scheduler watches for Pods that do not have an assigned Node and selects the most suitable Node for each Pod to run on. Option A is incorrect because health monitoring is typically handled by components like kubelet and controllers. Option C is incorrect because lifecycle management is handled by controllers and kubelet. Option D is incorrect because cluster state and configuration are stored by etcd, not the scheduler."
"Which two steps does kube-scheduler perform when selecting a Node for a Pod?

A) Binding and Deployment
B) Filtering and Scoring
C) Scheduling and Provisioning
D) Quota Checking and Eviction",B) Filtering and Scoring,"The correct answer is B) Filtering and Scoring. kube-scheduler first filters Nodes to find feasible ones that meet the Pod's requirements, then scores these feasible Nodes to select the best one. Option A is incorrect because binding refers to assigning the Pod to the Node after scheduling, and deployment is unrelated. Option C is incorrect because provisioning Nodes is outside the scheduler's scope. Option D is incorrect because quota checking and eviction relate to resource management, not the scheduling process itself."
"If multiple feasible Nodes have the same highest score during scheduling, how does kube-scheduler choose the Node to assign the Pod to?

A) It chooses the Node with the lowest IP address
B) It selects one Node at random from the highest scoring Nodes
C) It assigns the Pod to the Node with the most available CPU
D) It waits until one Node becomes uniquely highest scoring",B) It selects one Node at random from the highest scoring Nodes,"The correct answer is B) It selects one Node at random from the highest scoring Nodes. kube-scheduler uses random selection to break ties among equally scored Nodes to ensure fairness. Option A is incorrect as IP addresses are not used for tie-breaking. Option C is incorrect because scoring already accounts for resource availability, and the scheduler does not pick based solely on CPU after scoring. Option D is incorrect because the scheduler does not delay scheduling waiting for a unique highest score."
"How can you configure kube-scheduler to customize the filtering and scoring behavior used during Pod scheduling?

A) By specifying Node affinity rules only
B) Using Scheduling Policies or Scheduling Profiles
C) By modifying the kube-apiserver configuration
D) Through Pod annotations",B) Using Scheduling Policies or Scheduling Profiles,"The correct answer is B) Using Scheduling Policies or Scheduling Profiles. These are the supported methods for configuring filtering (Predicates) and scoring (Priorities) behavior or plugins for scheduling stages. Option A is incorrect because Node affinity is a Pod specification, not scheduler configuration. Option C is incorrect because the kube-apiserver configuration does not control scheduler behavior. Option D is incorrect because Pod annotations cannot configure the scheduler's filtering/scoring logic."
Create a Pod named 'test-pod' using the image 'busybox' that runs the command 'sleep 3600' and ensure it is scheduled only on Nodes labeled with 'disktype=ssd'.,"kubectl run test-pod --image=busybox -- sleep 3600 --overrides='{ ""spec"": { ""nodeSelector"": { ""disktype"": ""ssd"" } } }'","This command creates a Pod named 'test-pod' using the 'kubectl run' command with the 'busybox' image that runs 'sleep 3600'. The '--overrides' flag is used to add a 'nodeSelector' field to the Pod spec, restricting scheduling to Nodes labeled with 'disktype=ssd'. This ensures kube-scheduler will only consider Nodes with this label when placing the Pod. Without the nodeSelector, the Pod could be scheduled on any Node."
How can you verify which Node a Pod has been scheduled onto after creation?,kubectl get pod <pod-name> -o jsonpath='{.spec.nodeName}',"This command retrieves the Node name where the specified Pod is scheduled. The '-o jsonpath' option filters the output to display only the '.spec.nodeName' field, which holds the Node assignment. This is useful for verifying scheduling decisions. Replacing '<pod-name>' with the actual Pod name provides the Node name directly. Other commands like 'kubectl describe pod <pod-name>' also show this info but with more verbose output."
"Which Kubernetes feature allows you to constrain a Pod to run only on nodes with specific labels and provides both required and preferred scheduling rules?

A) nodeSelector
B) nodeAffinity
C) nodeName
D) Pod topology spread constraints",B) nodeAffinity,The correct answer is B) nodeAffinity. Node affinity allows you to specify expressive rules to constrain Pods to nodes based on node labels. It supports both requiredDuringSchedulingIgnoredDuringExecution (hard constraints) and preferredDuringSchedulingIgnoredDuringExecution (soft constraints) rules. Option A) nodeSelector is simpler and only supports hard constraints requiring all specified labels. Option C) nodeName directly assigns a Pod to a specific node but lacks flexibility and label-based selection. Option D) Pod topology spread constraints help distribute Pods across topology domains but do not constrain Pods to specific nodes based on labels.
"What happens if you specify both nodeSelector and nodeAffinity in a Pod spec?

A) The Pod is scheduled if either nodeSelector or nodeAffinity constraints are met.
B) The Pod is scheduled only if nodeSelector constraints are met; nodeAffinity is ignored.
C) The Pod is scheduled only if both nodeSelector and nodeAffinity constraints are met.
D) The Pod is scheduled only if nodeAffinity constraints are met; nodeSelector is ignored.",C) The Pod is scheduled only if both nodeSelector and nodeAffinity constraints are met.,"The correct answer is C) The Pod is scheduled only if both nodeSelector and nodeAffinity constraints are met. Kubernetes requires all constraints in both fields to be satisfied for scheduling the Pod. Option A) is incorrect because the constraints are ANDed, not ORed. Options B) and D) are incorrect because neither nodeSelector nor nodeAffinity is ignored when both are specified."
"Which operator is NOT valid for use in node affinity matchExpressions?

A) In
B) Exists
C) Contains
D) DoesNotExist",C) Contains,"The correct answer is C) Contains. Kubernetes node affinity operators include In, NotIn, Exists, DoesNotExist, Gt, and Lt. ""Contains"" is not a valid operator. Options A) In, B) Exists, and D) DoesNotExist are all valid operators used in matchExpressions for node affinity rules."
"You want to ensure a Pod runs only on nodes labeled with topology.kubernetes.io/zone=antarctica-east1 or antarctica-west1, but you prefer nodes with label another-node-label-key=another-node-label-value. Which affinity type and settings should you use?

A) nodeSelector with topology.kubernetes.io/zone labels only
B) nodeAffinity with requiredDuringSchedulingIgnoredDuringExecution for zone labels and preferredDuringSchedulingIgnoredDuringExecution for the other label
C) nodeName set to a node with both labels
D) Pod topology spread constraints to prefer nodes with the second label",B) nodeAffinity with requiredDuringSchedulingIgnoredDuringExecution for zone labels and preferredDuringSchedulingIgnoredDuringExecution for the other label,The correct answer is B) nodeAffinity with requiredDuringSchedulingIgnoredDuringExecution for zone labels and preferredDuringSchedulingIgnoredDuringExecution for the other label. This setup enforces hard constraints on the zone label and soft preferences on the other label. Option A) nodeSelector cannot express preferences; it only enforces hard constraints. Option C) nodeName assigns to a specific node but does not express these label-based constraints. Option D) Pod topology spread constraints are used to spread Pods evenly but do not enforce or prefer nodes based on specific node labels.
Create a Pod named 'ssd-pod' that runs the 'nginx' container and schedules only on nodes labeled 'storage=ssd'.,"kubectl run ssd-pod --image=nginx --overrides='{""spec"":{""nodeSelector"":{""storage"":""ssd""}}}'",This command creates a Pod named 'ssd-pod' using 'kubectl run' with the nginx image. The '--overrides' flag injects a JSON patch to specify 'nodeSelector' with the label key 'storage' and value 'ssd' to constrain scheduling to nodes labeled accordingly. This ensures the Pod will only be scheduled on nodes that have the label 'storage=ssd'. Using 'kubectl run' with '--overrides' is a quick way to specify nodeSelector without creating a full manifest.
"Using a YAML manifest file named 'pod-node-affinity.yaml', apply a nodeAffinity rule that requires nodes to have the label 'kubernetes.io/os=linux'. What kubectl command do you use to create this Pod?",kubectl apply -f pod-node-affinity.yaml,The command 'kubectl apply -f pod-node-affinity.yaml' creates or updates a Pod resource defined in the YAML file. The file is expected to contain the nodeAffinity rule requiring nodes to have label 'kubernetes.io/os=linux'. Using 'kubectl apply' allows declarative management of resources. The YAML manifest should include the affinity section under spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution with the appropriate matchExpressions.
"What is Pod Overhead in Kubernetes?

A) Additional resources requested by containers inside a Pod
B) Resources consumed by the Pod infrastructure in addition to container requests
C) Extra CPU and memory reserved for the kubelet process
D) Resources allocated only for system pods running on a node",B) Resources consumed by the Pod infrastructure in addition to container requests,"The correct answer is B) Resources consumed by the Pod infrastructure in addition to container requests. Pod Overhead accounts for the resources used by the Pod infrastructure itself (like pause containers or VM overhead in case of virtualization runtimes), which are in addition to the container resource requests and limits. Option A is incorrect because container resource requests only specify the resources required by containers, not the Pod infrastructure. Option C is wrong as kubelet resource usage is not considered Pod Overhead. Option D is incorrect since Pod Overhead applies to all Pods using a RuntimeClass that defines it, not only system pods."
"Which Kubernetes resource defines Pod overhead that affects scheduling and resource quota calculations?

A) Pod
B) RuntimeClass
C) ResourceQuota
D) LimitRange",B) RuntimeClass,The correct answer is B) RuntimeClass. The Pod's overhead is defined in the RuntimeClass resource under the overhead field. This overhead is applied at admission time and considered during scheduling and resource quota enforcement. Option A (Pod) is incorrect because the Pod itself does not define overhead—it references a RuntimeClass. Option C (ResourceQuota) enforces limits on resource consumption but does not define overhead. Option D (LimitRange) defines default limits and requests within a namespace but does not specify Pod overhead.
"What happens if the PodSpec already defines an overhead field when using a RuntimeClass with overhead?

A) The Pod overhead is merged with the RuntimeClass overhead
B) The Pod admission is rejected
C) The Pod overhead is ignored
D) The Pod overhead field is overwritten by the RuntimeClass overhead",B) The Pod admission is rejected,"The correct answer is B) The Pod admission is rejected. The RuntimeClass admission controller mutates the PodSpec to add the overhead field if it is not present. If the PodSpec already includes an overhead field, the admission controller rejects the Pod to avoid conflicting overhead definitions. Option A is incorrect because no merging occurs. Option C is wrong as the overhead field cannot be ignored—it must be consistent. Option D is incorrect since the controller does not overwrite existing overhead fields."
"Which of the following statements about Pod overhead and scheduling is correct?

A) Pod overhead is ignored by the kube-scheduler during scheduling decisions
B) Pod overhead is added to the sum of container requests and considered for scheduling
C) Pod overhead only affects resource quota calculations, not scheduling
D) Pod overhead is only used by kubelet, not the scheduler",B) Pod overhead is added to the sum of container requests and considered for scheduling,The correct answer is B) Pod overhead is added to the sum of container requests and considered for scheduling. The kube-scheduler sums container requests and the Pod overhead to find a node with sufficient resources. Option A is incorrect as overhead is explicitly included in scheduling decisions. Option C is wrong because overhead affects both resource quotas and scheduling. Option D is incorrect since overhead is used by both the scheduler and kubelet.
"Create a Pod named 'test-pod' using the 'kata-fc' RuntimeClass with two containers: busybox (image: busybox:1.28) with 500m CPU and 100Mi memory limits, and nginx (image: nginx) with 1500m CPU and 100Mi memory limits. The busybox container should have stdin and tty enabled.","kubectl run test-pod --image=busybox:1.28 --restart=Never --overrides='{""spec"":{""runtimeClassName"":""kata-fc"",""containers"":[{""name"":""busybox-ctr"",""image"":""busybox:1.28"",""stdin"":true,""tty"":true,""resources"":{""limits"":{""cpu"":""500m"",""memory"":""100Mi""}}},{""name"":""nginx-ctr"",""image"":""nginx"",""resources"":{""limits"":{""cpu"":""1500m"",""memory"":""100Mi""}}}]}}'","This command creates a Pod named 'test-pod' with the specified runtimeClassName 'kata-fc'. The '--overrides' flag is used to specify the Pod spec with two containers: 'busybox-ctr' with stdin and tty enabled and resource limits of 500m CPU and 100Mi memory, and 'nginx-ctr' with 1500m CPU and 100Mi memory limits. The '--restart=Never' flag ensures a Pod is created instead of a Deployment. The RuntimeClass ensures the Pod overhead is accounted for during scheduling and resource quota calculations."
How can you verify the Pod overhead that was applied to a running Pod named 'test-pod'?,kubectl get pod test-pod -o jsonpath='{.spec.overhead}',"This command retrieves the Pod resource named 'test-pod' and outputs the value of the 'spec.overhead' field, which shows the additional CPU and memory overhead applied by the RuntimeClass admission controller. Using '-o jsonpath' allows extraction of just the overhead details. This is the recommended way to verify the overhead set on a Pod after admission mutation. Other commands like 'kubectl describe pod' do not directly show overhead."
"Which statement best describes the purpose of the Pod's schedulingGates field in Kubernetes?

A) It specifies the nodes where a Pod can be scheduled.
B) It marks a Pod as ready for scheduling immediately after creation.
C) It contains criteria that must be satisfied before a Pod is considered schedulable.
D) It controls the restart policy of a Pod's containers.",C) It contains criteria that must be satisfied before a Pod is considered schedulable.,"The correct answer is C) It contains criteria that must be satisfied before a Pod is considered schedulable. The schedulingGates field holds a list of strings representing conditions that must be cleared before the Pod can be scheduled. Option A is incorrect because node selection is controlled by nodeSelector or affinity, not schedulingGates. Option B is wrong because schedulingGates actually delay scheduling until conditions are met. Option D is incorrect since restart policies are unrelated to schedulingGates."
"After a Pod is created with schedulingGates, which of the following operations is allowed?

A) Adding new scheduling gates to the Pod's spec.
B) Removing existing scheduling gates in any order.
C) Replacing the entire schedulingGates list with a new list.
D) Changing the Pod's container image without removing scheduling gates.",B) Removing existing scheduling gates in any order.,"The correct answer is B) Removing existing scheduling gates in any order. Once a Pod is created with schedulingGates, you cannot add new gates (ruling out A) or replace the entire list (ruling out C), but you can remove gates individually or entirely. Option D is unrelated to schedulingGates behavior and is allowed but not specifically constrained by schedulingGates."
Which kubectl command would you use to verify the schedulingGates currently set on a Pod named 'test-pod'?,kubectl get pod test-pod -o jsonpath='{.spec.schedulingGates}',This command uses 'kubectl get' to fetch the Pod resource named 'test-pod' and outputs only the schedulingGates field using the jsonpath expression '{.spec.schedulingGates}'. This allows you to see the list of scheduling gates currently applied to the Pod. Using jsonpath filters output for clarity. Other commands without '-o jsonpath' would show the entire Pod manifest or summary.
"You have a Pod created with two schedulingGates defined. How do you mark it ready for scheduling by modifying the schedulingGates field?

A) Add an additional scheduling gate.
B) Remove all scheduling gates.
C) Replace the Pod with a new one using the same name.
D) Patch the Pod to change schedulingGates to null.",B) Remove all scheduling gates.,"The correct answer is B) Remove all scheduling gates. A Pod is considered 'SchedulingGated' and not ready for scheduling while any scheduling gates exist. To mark it ready, you must remove all scheduling gates. Option A is invalid because adding new scheduling gates after creation is disallowed. Option C is unnecessary and disruptive. Option D might work if implemented correctly, but the recommended approach is to remove schedulingGates entirely by applying a manifest without the field."
"When updating a Pod that still has schedulingGates set, which updates to the spec.nodeSelector field are allowed?

A) Removing existing nodeSelector keys.
B) Adding new nodeSelector keys only.
C) Completely replacing nodeSelector with a new set.
D) No changes to nodeSelector are allowed.",B) Adding new nodeSelector keys only.,"The correct answer is B) Adding new nodeSelector keys only. While schedulingGates are set, you can only tighten scheduling directives, which means adding nodeSelector keys but not removing or replacing existing keys. Option A and C would loosen or change scheduling constraints and are disallowed. Option D is incorrect because additions are allowed."
Create a Pod named 'test-pod' with the image 'registry.k8s.io/pause:3.6' and two scheduling gates named 'example.com/foo' and 'example.com/bar'.,"kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  schedulingGates:
  - name: example.com/foo
  - name: example.com/bar
  containers:
  - name: pause
    image: registry.k8s.io/pause:3.6
EOF","This command uses 'kubectl apply -f -' to create a Pod from inline YAML manifest. The manifest defines the Pod named 'test-pod' with two scheduling gates under spec.schedulingGates. The container uses the specified pause image. This sets the Pod in a SchedulingGated state, preventing scheduling until gates are removed. Using inline YAML with 'kubectl apply' is convenient for quick resource creation without separate files."
"What does the 'maxSkew' field specify in a Pod's topologySpreadConstraints?

A) The maximum number of pods allowed per node
B) The maximum permitted difference in the number of matching pods between topology domains
C) The minimum number of eligible topology domains
D) The key of the node label used for topology domains",B) The maximum permitted difference in the number of matching pods between topology domains,"The correct answer is B) The maximum permitted difference in the number of matching pods between topology domains. 'maxSkew' defines how uneven the distribution of pods can be across the specified topology domains. Option A) is incorrect because 'maxSkew' does not limit pods per node directly. Option C) describes 'minDomains', not 'maxSkew'. Option D) refers to 'topologyKey', not 'maxSkew'."
"Which 'whenUnsatisfiable' policy in topologySpreadConstraints prevents the scheduler from placing a Pod if the spread constraint cannot be met?

A) ScheduleAnyway
B) Ignore
C) DoNotSchedule
D) ForceSchedule",C) DoNotSchedule,"The correct answer is C) DoNotSchedule. This policy instructs the scheduler not to schedule the Pod if the spread constraint cannot be satisfied. Option A) ScheduleAnyway allows scheduling even if constraints are not met, just with lower priority. Option B) Ignore is not a valid value for 'whenUnsatisfiable'. Option D) ForceSchedule is not a valid Kubernetes scheduling policy."
"In the context of Pod topology spread constraints, what is the role of the 'topologyKey' field?

A) It lists all topology domains in the cluster
B) It specifies the node label key that defines topology domains
C) It selects Pods by label to spread across nodes
D) It defines the maximum skew allowed between Pods",B) It specifies the node label key that defines topology domains,"The correct answer is B) It specifies the node label key that defines topology domains. 'topologyKey' is used to group nodes into topology domains based on node labels. Option A) is incorrect because it does not list domains, it only specifies the label key. Option C) describes the purpose of 'labelSelector', not 'topologyKey'. Option D) refers to 'maxSkew', not 'topologyKey'."
"Which of the following statements about the 'minDomains' field in topologySpreadConstraints is TRUE?

A) It is required and must be set for every topology spread constraint
B) It can only be used with whenUnsatisfiable: ScheduleAnyway
C) It specifies the minimum number of eligible topology domains required to enforce the constraint
D) It is ignored if the number of eligible domains is less than the specified value",C) It specifies the minimum number of eligible topology domains required to enforce the constraint,"The correct answer is C) It specifies the minimum number of eligible topology domains required to enforce the constraint. 'minDomains' is optional and only valid with whenUnsatisfiable: DoNotSchedule. Option A) is false because it is optional. Option B) is incorrect because it can only be used with DoNotSchedule, not ScheduleAnyway. Option D) is incorrect because if eligible domains are fewer than minDomains, the global minimum is treated as zero, affecting skew calculation."
"Create a Pod named 'example-pod' with a topologySpreadConstraint that spreads pods evenly across nodes labeled with 'kubernetes.io/hostname', allows a maximum skew of 1, uses 'DoNotSchedule' when unsatisfiable, and selects pods with the label 'app=foo'.","kubectl run example-pod --image=nginx --dry-run=client -o yaml > pod.yaml && 
  sed -i '/spec:/a\  topologySpreadConstraints:\n  - maxSkew: 1\n    topologyKey: kubernetes.io/hostname\n    whenUnsatisfiable: DoNotSchedule\n    labelSelector:\n      matchLabels:\n        app: foo' pod.yaml && 
kubectl apply -f pod.yaml","This sequence creates a Pod manifest using 'kubectl run' with the nginx image, outputting YAML for editing. The 'sed' command inserts the specified topologySpreadConstraints under the spec field, setting maxSkew to 1, topologyKey to 'kubernetes.io/hostname', whenUnsatisfiable to 'DoNotSchedule', and selecting pods with label 'app=foo'. Finally, 'kubectl apply' creates the Pod from the modified YAML. Using a YAML manifest is recommended for complex specs like topologySpreadConstraints since 'kubectl run' has limited direct options for these fields."
"How does setting 'nodeAffinityPolicy' to 'Ignore' affect the calculation of pod topology spread skew?

A) It excludes nodes that don't match node affinity from the calculation
B) It includes all nodes, ignoring node affinity constraints
C) It forces the scheduler to respect node affinity strictly
D) It disables topology spread constraints entirely","B) It includes all nodes, ignoring node affinity constraints","The correct answer is B) It includes all nodes, ignoring node affinity constraints. Setting 'nodeAffinityPolicy' to 'Ignore' causes the scheduler to consider all nodes regardless of their node affinity or nodeSelector when calculating pod distribution skew. Option A) is the behavior of 'Honor', not 'Ignore'. Option C) is incorrect as it describes 'Honor'. Option D) is incorrect because topology spread constraints remain active; only node affinity consideration is altered."
"What effect does the taint 'key1=value1:NoSchedule' have on node scheduling?

A) It prevents all pods from running on the node regardless of tolerations.
B) It prevents pods without a matching toleration from being scheduled on the node.
C) It immediately evicts all pods running on the node.
D) It softly prefers pods without a matching toleration to avoid the node but does not prevent scheduling.",B) It prevents pods without a matching toleration from being scheduled on the node.,"The correct answer is B. A taint with the effect 'NoSchedule' means pods that do not have a matching toleration will not be scheduled onto the node. Option A is incorrect because pods with matching tolerations can still schedule. Option C is wrong as 'NoSchedule' does not evict existing pods; eviction is related to 'NoExecute'. Option D describes the effect 'PreferNoSchedule', not 'NoSchedule'."
"Which of the following tolerations would correctly allow a pod to tolerate a taint with key 'key1', value 'value1', and effect 'NoSchedule'?

A) key: ""key1"", operator: ""Exists"", effect: ""NoSchedule""
B) key: ""key1"", operator: ""Equal"", value: ""differentValue"", effect: ""NoSchedule""
C) key: ""key2"", operator: ""Equal"", value: ""value1"", effect: ""NoSchedule""
D) key: ""key1"", operator: ""Equal"", value: ""value1"", effect: ""NoExecute""","A) key: ""key1"", operator: ""Exists"", effect: ""NoSchedule""","Option A is correct because the operator 'Exists' allows the toleration to match any value for the key 'key1' with the specified effect 'NoSchedule'. Option B is incorrect because the value does not match the taint's value. Option C is incorrect because the key does not match the taint's key. Option D is incorrect because the effect does not match; the taint effect is 'NoSchedule', but the toleration effect is 'NoExecute'."
"What happens to a pod that is manually assigned to a node with a 'NoSchedule' taint by setting .spec.nodeName, but the pod lacks the matching toleration?

A) The pod will not be scheduled onto the node.
B) The pod will be scheduled and immediately evicted.
C) The pod will be bound to the node but will not be evicted.
D) The pod will be scheduled and evicted after a delay if 'tolerationSeconds' is not specified.",C) The pod will be bound to the node but will not be evicted.,"When a pod's .spec.nodeName is set manually, scheduling is bypassed, so the pod is bound to the node regardless of taints. A 'NoSchedule' taint does not cause eviction; it only prevents scheduling. Therefore, option C is correct. Option A is wrong because manual binding bypasses the scheduler. Option B is incorrect as 'NoSchedule' taints do not cause eviction. Option D is incorrect because eviction after a delay applies only to 'NoExecute' taints with tolerationSeconds."
"Which taint effect causes pods that do not tolerate the taint to be immediately evicted from the node?

A) PreferNoSchedule
B) NoSchedule
C) NoExecute
D) EvictNow",C) NoExecute,The correct answer is C. The 'NoExecute' taint effect causes pods that do not tolerate the taint to be immediately evicted from the node and prevents new pods without tolerations from scheduling. Option A is incorrect since 'PreferNoSchedule' is a soft preference and does not cause eviction. Option B only prevents scheduling but does not evict existing pods. Option D is invalid as 'EvictNow' is not a recognized taint effect.
"Create a taint on the node named 'worker-node1' with key 'special', value 'true', and effect 'PreferNoSchedule'.",kubectl taint nodes worker-node1 special=true:PreferNoSchedule,"This command adds a taint to the node 'worker-node1' with key 'special', value 'true', and effect 'PreferNoSchedule'. The 'kubectl taint nodes' command is used to add taints. The syntax 'key=value:effect' specifies the taint's key, value, and effect. The effect 'PreferNoSchedule' indicates that the scheduler will try to avoid placing pods that do not tolerate this taint on this node, but it is not guaranteed."
"Add a toleration to a pod specification that allows the pod to tolerate any taint with key 'example-key' and effect 'NoSchedule', regardless of the taint's value.","tolerations:
- key: ""example-key""
  operator: ""Exists""
  effect: ""NoSchedule""","This pod toleration matches any taint with key 'example-key' and effect 'NoSchedule', ignoring the taint's value. The operator 'Exists' means the toleration matches any value for that key. Specifying the effect ensures it only matches taints with 'NoSchedule' effect. This allows pods to tolerate all taints of that key and effect combination."
"Which scheduling framework extension point in Kubernetes is responsible for filtering out nodes that cannot run a Pod?

A) PreFilter
B) Filter
C) PostFilter
D) Reserve",B) Filter,"The correct answer is B) Filter. Filter plugins are invoked during the scheduling cycle to evaluate each node and exclude those that cannot run the Pod based on defined criteria. Option A) PreFilter is incorrect as it is used for pre-processing or checking conditions before filtering. Option C) PostFilter is incorrect because it only runs if no feasible nodes are found after filtering and typically helps with preemption. Option D) Reserve is incorrect as it is used to reserve resources on a node before binding, not for filtering nodes."
"During which phase of the Kubernetes scheduling framework are resources on a node reserved for a Pod to avoid race conditions before binding?

A) Permit
B) Reserve
C) PreFilter
D) PostFilter",B) Reserve,"The correct answer is B) Reserve. The Reserve phase occurs after filtering and scoring, where plugins reserve resources on a node to prevent race conditions before the Pod is actually bound. Option A) Permit is incorrect because it controls approval or delays for binding but does not reserve resources. Option C) PreFilter is incorrect since it is used before filtering nodes. Option D) PostFilter is incorrect as it runs after filtering when no nodes are feasible, usually for preemption."
"Which plugin interface in the Kubernetes scheduling framework allows delaying or denying the binding of a Pod to a node after scheduling decisions?

A) Permit
B) Reserve
C) PreEnqueue
D) EnqueueExtension",A) Permit,"The correct answer is A) Permit. Permit plugins are invoked at the end of the scheduling cycle and can approve, deny, or delay binding of a Pod to a node. Option B) Reserve is incorrect as it reserves resources but does not control binding approval. Option C) PreEnqueue is incorrect because it runs before a Pod is added to the active queue, not after scheduling decisions. Option D) EnqueueExtension controls retries for scheduling rejected Pods, not binding delays."
"In the Kubernetes scheduling framework, what is the purpose of the NormalizeScore extension point?

A) To filter out nodes that cannot run the Pod
B) To modify and normalize node scores from Score plugins before final ranking
C) To reserve node resources before binding
D) To sort Pods in the scheduling queue",B) To modify and normalize node scores from Score plugins before final ranking,The correct answer is B) To modify and normalize node scores from Score plugins before final ranking. NormalizeScore plugins adjust raw scores to a consistent scale so that the scheduler can fairly combine scores from multiple plugins. Option A) is incorrect because filtering is done by Filter plugins. Option C) is incorrect as reserving resources is done in the Reserve phase. Option D) is incorrect since sorting Pods in the scheduling queue is done by QueueSort plugins.
Create a Pod manifest file named 'test-pod.yaml' that uses the 'nginx' image and includes a scheduling annotation to enable the 'StorageCapacityScoring' feature gate during scheduling.,"kubectl create pod test-pod --image=nginx --dry-run=client -o yaml > test-pod.yaml
# Then manually add the annotation to the yaml under metadata.annotations:
# scheduling.kubernetes.io/storage-capacity-scoring: 'true'","The command 'kubectl create pod test-pod --image=nginx --dry-run=client -o yaml > test-pod.yaml' generates a Pod manifest file without applying it to the cluster. Since the StorageCapacityScoring feature is enabled via a feature gate and can be influenced via annotations for scheduling behavior, you add the annotation 'scheduling.kubernetes.io/storage-capacity-scoring: ""true""' manually to the pod manifest under metadata.annotations. This annotation signals the scheduler to apply storage capacity scoring logic when scheduling this Pod. There is no direct kubectl flag to enable feature gates or scheduling plugin behavior on a per-Pod basis."
"What is the role of the QueueingHint plugin interface in the Kubernetes scheduling framework?

A) It controls whether Pods rejected by a plugin should be retried based on cluster changes
B) It determines the order in which Pods are sorted in the scheduling queue
C) It triggers evaluation on cluster events to decide if unschedulable Pods should be requeued
D) It filters nodes that cannot run the Pod",C) It triggers evaluation on cluster events to decide if unschedulable Pods should be requeued,"The correct answer is C) It triggers evaluation on cluster events to decide if unschedulable Pods should be requeued. QueueingHint is a beta feature that, when enabled, allows the scheduler to requeue Pods from backoff or active queues based on certain cluster events that might make them schedulable. Option A) is incorrect because controlling retries of rejected Pods is handled by EnqueueExtension. Option B) is incorrect since sorting Pods in the queue is the responsibility of QueueSort plugins. Option D) is incorrect as filtering nodes is done by Filter plugins."
"Which Kubernetes API resource is responsible for describing a request for access to attached devices in Dynamic Resource Allocation (DRA)?

A) DeviceClass
B) ResourceSlice
C) ResourceClaim
D) ResourceClaimTemplate",C) ResourceClaim,"The correct answer is C) ResourceClaim. ResourceClaims are API objects that describe requests for access to attached resources, such as devices, in the cluster and provide Pods with access to those specific resources. Option A) DeviceClass defines categories of devices that can be claimed but does not itself describe the request. Option B) ResourceSlice represents one or more resources attached to nodes and is managed by device drivers, not used for requesting resources. Option D) ResourceClaimTemplate defines a template for creating per-Pod ResourceClaims, but it is not itself a request."
"In Dynamic Resource Allocation (DRA), what is the role of a DeviceClass?

A) To provide nodes with information about attached devices
B) To define device drivers that manage hardware
C) To categorize devices and define how to select them for claims
D) To generate ResourceClaims automatically for Pods",C) To categorize devices and define how to select them for claims,"The correct answer is C) To categorize devices and define how to select them for claims. DeviceClass objects allow cluster admins or device drivers to define categories of devices in the cluster and specify selection criteria, often using CEL expressions, for claims. Option A) describes ResourceSlices, which provide node and resource information. Option B) is incorrect because DeviceClass does not define drivers; drivers manage ResourceSlices. Option D) relates to ResourceClaimTemplates, not DeviceClass."
"Which type of Dynamic Resource Allocation user is responsible for attaching devices to nodes and installing DRA-compatible drivers?

A) Device owner
B) Workload operator
C) Cluster admin
D) Scheduler",C) Cluster admin,"The correct answer is C) Cluster admin. Cluster admins are responsible for configuring the cluster, attaching devices to nodes, and installing device drivers that support DRA. Option A) Device owners are responsible for devices and creating ResourceSlices or DeviceClasses, but not necessarily the physical attachment. Option B) Workload operators deploy workloads and create ResourceClaims but do not manage hardware or drivers. Option D) Scheduler places Pods but does not manage hardware or drivers."
"Which of the following benefits is NOT provided by Kubernetes Dynamic Resource Allocation compared to traditional device plugins?

A) Device sharing between multiple Pods or containers
B) Fine-grained device filtering using Common Expression Language (CEL)
C) Requirement to specify device quantities per container
D) Centralized device categorization via DeviceClasses",C) Requirement to specify device quantities per container,"The correct answer is C) Requirement to specify device quantities per container. This is NOT a benefit of DRA; in fact, DRA simplifies Pod resource requests by removing the need to specify device quantities per container. Option A) is a benefit because DRA supports device sharing. Option B) is correct as DRA supports fine-grained filtering with CEL. Option D) is also a benefit because DeviceClasses provide centralized device categorization, which traditional device plugins lack."
Create a ResourceClaim named 'gpu-claim' in the 'ml' namespace that references the DeviceClass 'high-performance-gpu'.,kubectl create resourceclaim gpu-claim --namespace=ml --device-class=high-performance-gpu,"This command creates a ResourceClaim named 'gpu-claim' in the 'ml' namespace that references the DeviceClass 'high-performance-gpu'. The '--namespace=ml' flag specifies the namespace where the ResourceClaim will be created. The '--device-class=high-performance-gpu' flag links the claim to the specified DeviceClass, allowing the pod that references this claim to get access to devices categorized under that DeviceClass. Note that 'kubectl create resourceclaim' requires the relevant CRDs and API group installed and enabled in the cluster."
Deploy a Pod named 'app-pod' in the default namespace that uses an existing ResourceClaim called 'shared-devices' to access devices.,"kubectl run app-pod --image=nginx --overrides='{""spec"":{""containers"":[{""name"":""nginx"",""image"":""nginx"",""resources"":{""requests"":{""resource.k8s.io/resourceClaims"":""shared-devices""}}}]}}'","This command creates a Pod named 'app-pod' running an nginx container in the default namespace, referencing an existing ResourceClaim named 'shared-devices' in its resource requests. The '--overrides' flag is used to specify the Pod spec JSON patch that adds the resource claim under 'resources.requests' for the container. This ensures the Pod requests the devices linked by the ResourceClaim. It's important that the ResourceClaim 'shared-devices' exists in the same namespace; otherwise, the Pod will fail to schedule."
"What does the kube-scheduler's percentageOfNodesToScore setting control?

A) The percentage of pods that can be scheduled simultaneously
B) The percentage of feasible nodes to score during scheduling
C) The percentage of nodes to be drained during maintenance
D) The percentage of nodes to be removed from the cluster",B) The percentage of feasible nodes to score during scheduling,"The correct answer is B) The percentage of feasible nodes to score during scheduling. The percentageOfNodesToScore setting determines the threshold for how many nodes the scheduler considers for scoring when placing a Pod, expressed as a percentage of all nodes in the cluster. Option A is incorrect because it relates to pods, not nodes. Option C is incorrect because draining nodes is unrelated to scheduling scoring thresholds. Option D is incorrect because node removal is unrelated to the scheduler's scoring process."
"If you want the kube-scheduler to score all nodes in a large cluster, what value should you set for percentageOfNodesToScore?

A) 0
B) 50
C) 100
D) 200",C) 100,"The correct answer is C) 100. Setting percentageOfNodesToScore to 100 instructs the scheduler to consider and score all nodes in the cluster for pod placement. Option A (0) means using the default compiled-in value, not scoring all nodes. Option B (50) means scoring only 50% of nodes. Option D (200) is invalid as values above 100 are treated as 100."
"Why should you generally avoid setting percentageOfNodesToScore below 10%?

A) It causes the scheduler to ignore node affinity rules
B) It may lead to poor pod placement because fewer nodes are scored
C) It will cause the scheduler to crash
D) It disables the scoring phase completely",B) It may lead to poor pod placement because fewer nodes are scored,"The correct answer is B) It may lead to poor pod placement because fewer nodes are scored. When percentageOfNodesToScore is set too low, the scheduler evaluates fewer nodes for scoring, increasing the chance that higher scoring nodes are missed, resulting in suboptimal pod placement. Option A is incorrect because node affinity is unrelated to this setting. Option C is incorrect as it does not cause the scheduler to crash. Option D is incorrect because the scoring phase still occurs but on fewer nodes."
"In a cluster with multiple zones, how does kube-scheduler ensure fairness when checking node feasibility?

A) It checks nodes in a zone sequentially before moving to the next zone
B) It checks nodes randomly without regard to zones
C) It iterates nodes in round-robin fashion across zones
D) It only checks nodes in the zone with the highest resource availability",C) It iterates nodes in round-robin fashion across zones,"The correct answer is C) It iterates nodes in round-robin fashion across zones. This approach ensures nodes from different zones get a fair chance to be considered for pod placement. Option A is incorrect because it does not interleave zones, potentially biasing one zone. Option B is incorrect because the scheduler does consider zones. Option D is incorrect because it does not limit feasibility checks to one zone."
Create a kube-scheduler configuration file snippet to set percentageOfNodesToScore to 50%.,"apiVersion: kubescheduler.config.k8s.io/v1alpha1
kind: KubeSchedulerConfiguration
algorithmSource:
  provider: DefaultProvider
percentageOfNodesToScore: 50","This snippet defines a KubeSchedulerConfiguration resource with the apiVersion and kind set appropriately. The algorithmSource uses the DefaultProvider, which is standard. The key setting percentageOfNodesToScore is set to 50, instructing the scheduler to score 50% of the feasible nodes. This file would typically be saved and used to replace or update the scheduler's config, requiring a scheduler restart to take effect."
"After modifying the kube-scheduler configuration file to change percentageOfNodesToScore, which command verifies that the kube-scheduler is healthy and running in the cluster?",kubectl get pods -n kube-system | grep kube-scheduler,This command lists all pods in the kube-system namespace and filters the output to show only those related to kube-scheduler. It helps confirm that the kube-scheduler pod is running and healthy after configuration changes. The '-n kube-system' flag targets the system namespace where the scheduler runs. The grep filters for kube-scheduler to quickly identify the scheduler pod status.
"Which scoring strategy in Kubernetes kube-scheduler's NodeResourcesFit plugin favors nodes with higher resource allocation for bin packing?

A) LeastAllocated
B) MostAllocated
C) RequestedToCapacityRatio
D) BalancedAllocation",B) MostAllocated,"The correct answer is B) MostAllocated. This strategy scores nodes based on resource utilization, favoring nodes with higher allocation to achieve bin packing. Option A) LeastAllocated is incorrect because it prefers nodes with fewer allocated resources. Option C) RequestedToCapacityRatio is a different scoring strategy that scores nodes based on the ratio of requested to capacity resources, not simply most allocated. Option D) BalancedAllocation is not a valid strategy in the NodeResourcesFit plugin."
"In the RequestedToCapacityRatio scoring strategy, what does the 'shape' field control?

A) The order of resource evaluation
B) The weight of each resource
C) The tuning of the scoring function based on utilization and score values
D) The maximum resource limits per node",C) The tuning of the scoring function based on utilization and score values,"The correct answer is C) The tuning of the scoring function based on utilization and score values. The 'shape' field defines how node scores change with resource utilization (e.g., enabling bin packing by scoring higher for higher utilization). Option A) is incorrect because 'shape' does not control evaluation order. Option B) is incorrect as weights are specified separately in the 'resources' field. Option D) is incorrect because 'shape' does not relate to resource limits."
"Which of the following statements about resource weights in the NodeResourcesFit plugin configuration is TRUE?

A) Weights can be negative to penalize certain resources
B) If weight is omitted, it defaults to 1
C) Weights are only applied to CPU and memory, not extended resources
D) Weights must sum to 100","B) If weight is omitted, it defaults to 1","The correct answer is B) If weight is omitted, it defaults to 1. The weight parameter is optional and defaults to 1 when not specified. Option A) is incorrect because weights cannot be negative. Option C) is incorrect since weights can be assigned to extended resources like 'intel.com/foo'. Option D) is incorrect because weights do not need to sum to any fixed number."
"You want to enable the RequestedToCapacityRatio scoring strategy in kube-scheduler for extended resources 'intel.com/foo' and 'intel.com/bar' with weights 3 each, and tune the scoring function so nodes with 100% utilization get maximum score 10, and 0% utilization gets 0. Which configuration snippet correctly specifies this?","apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: RequestedToCapacityRatio
        resources:
        - name: intel.com/foo
          weight: 3
        - name: intel.com/bar
          weight: 3
        requestedToCapacityRatio:
          shape:
          - utilization: 0
            score: 0
          - utilization: 100
            score: 10","This configuration sets the NodeResourcesFit plugin to use the RequestedToCapacityRatio strategy with specified weights for extended resources. The 'shape' defines scoring so that 0% utilization corresponds to score 0 and 100% utilization to score 10, enabling bin packing. Other options like MostAllocated or different shape values would not achieve the desired behavior."
"Calculate the node score for a node with the following specs using the RequestedToCapacityRatio scoring function with shape points {{0,0}, {100,10}} and weights: intel.com/foo=5, memory=1, cpu=3.
Requested: intel.com/foo=2, memory=256MB, cpu=2
Node Available: intel.com/foo=4, memory=1GB, cpu=8
Node Used: intel.com/foo=1, memory=256MB, cpu=1
What is the final node score?

A) 3
B) 5
C) 7
D) 10",B) 5,"The correct answer is B) 5, based on the calculation: intel.com/foo utilization = ((2+1)/4)*100 = 75%, raw score = 7; memory utilization = ((256+256)/1024)*100 = 50%, raw score = 5; cpu utilization = ((2+1)/8)*100 = 37.5%, raw score = 3. Weighted score = ((7*5)+(5*1)+(3*3))/(5+1+3) = (35+5+9)/9 = 49/9 ≈ 5.44, floored to 5. Option A) 3 is too low, C) 7 and D) 10 are too high given the weighted average."
"Create a kube-scheduler configuration file named 'scheduler-config.yaml' that enables the MostAllocated scoring strategy for NodeResourcesFit with CPU weight 1, memory weight 1, and an extended resource 'intel.com/foo' with weight 3.","cat <<EOF > scheduler-config.yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
- pluginConfig:
  - name: NodeResourcesFit
    args:
      scoringStrategy:
        type: MostAllocated
        resources:
        - name: cpu
          weight: 1
        - name: memory
          weight: 1
        - name: intel.com/foo
          weight: 3
EOF","This command creates a kube-scheduler configuration file that sets the NodeResourcesFit plugin's scoringStrategy to MostAllocated, with specified weights on CPU, memory, and the extended resource 'intel.com/foo'. The 'cat <<EOF' syntax is used to create the multi-line YAML file. Each resource has a weight, with 1 as default for CPU and memory, and 3 for the extended resource. This is the correct way to configure bin packing using MostAllocated strategy."
"Which of the following statements about PriorityClass in Kubernetes is TRUE?

A) PriorityClass names must begin with the prefix 'system-'.
B) Only one PriorityClass can exist in a cluster.
C) The higher the PriorityClass value, the higher the priority of Pods using it.
D) PriorityClass objects are namespaced resources.","C) The higher the PriorityClass value, the higher the priority of Pods using it.","Option C is correct because PriorityClass maps a name to an integer value, and a higher value means higher priority for scheduling and preemption. Option A is incorrect because PriorityClass names must NOT be prefixed with 'system-'; those names are reserved for built-in critical PriorityClasses. Option B is incorrect since multiple PriorityClasses can exist in a cluster to represent different priority levels. Option D is incorrect because PriorityClass is a non-namespaced resource; it is cluster-scoped."
"What happens if you create a Pod with a priorityClassName that does not exist in the cluster?

A) The Pod is scheduled with priority 0.
B) The Pod is scheduled but marked as low priority.
C) The Pod creation is rejected by the admission controller.
D) The Pod is created but cannot preempt other Pods.",C) The Pod creation is rejected by the admission controller.,"The correct answer is C. When a Pod specifies a priorityClassName that is not found in the cluster, the priority admission controller rejects the Pod creation. Option A is incorrect because Pods without a priorityClassName default to priority 0 only if no globalDefault PriorityClass exists; but a non-existent priorityClassName causes rejection. Option B is incorrect because there is no fallback to low priority; the Pod is rejected instead. Option D is incorrect because the Pod will not be created at all, so scheduling or preemption does not occur."
"In Kubernetes, which of the following best describes the behavior of a Pod with preemptionPolicy set to 'Never'?

A) It can preempt lower priority Pods but cannot be preempted.
B) It is scheduled ahead of lower priority Pods but cannot preempt any Pods.
C) It cannot be scheduled until all lower priority Pods are evicted.
D) It is treated with normal preemption rules as the default.",B) It is scheduled ahead of lower priority Pods but cannot preempt any Pods.,"Option B is correct because Pods with preemptionPolicy: Never are prioritized in the scheduling queue ahead of lower priority Pods but do not preempt (evict) other Pods. Option A is incorrect because such Pods cannot preempt others. Option C is incorrect as the Pod waits in the queue until resources naturally free up; it does not force eviction of lower priority Pods. Option D is incorrect since the default preemptionPolicy is PreemptLowerPriority, not Never."
"When a high priority Pod preempts lower priority Pods on a Node, what does the nominatedNodeName field in the Pod's status indicate?

A) The Node where the Pod is finally scheduled.
B) The Node where resources are reserved for the Pod during preemption.
C) The Node where the Pod's containers are running.
D) The Node where the Pod was rejected from scheduling.",B) The Node where resources are reserved for the Pod during preemption.,The nominatedNodeName field indicates the Node where the scheduler has reserved resources for the pending Pod during preemption. This helps the scheduler track resource reservations and gives users insight into preemption events. Option A is incorrect because the Pod might be scheduled on a different Node later. Option C is incorrect since the Pod is not running while pending. Option D is incorrect because nominatedNodeName does not indicate rejection but reservation.
"Create a PriorityClass named 'data-science-priority' with value 500000, set to not preempt other Pods, and include a description 'Priority class for data science workloads'.","kubectl apply -f - <<EOF
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: data-science-priority
value: 500000
preemptionPolicy: Never
globalDefault: false
description: ""Priority class for data science workloads""
EOF",This command creates a PriorityClass resource named 'data-science-priority' with a priority value of 500000. The 'preemptionPolicy: Never' field ensures Pods of this priority class cannot preempt other Pods. 'globalDefault: false' means this class is not the default priority for Pods without a priorityClassName. The description provides useful context. The command uses a 'kubectl apply -f -' with inline YAML to define the resource.
Create a Pod named 'nginx-priority' using the nginx image and assign it to the existing PriorityClass 'high-priority'.,kubectl run nginx-priority --image=nginx --restart=Never --priority-class-name=high-priority,"This command creates a Pod named 'nginx-priority' running the 'nginx' image. The '--restart=Never' flag ensures a Pod (not a Deployment) is created. The '--priority-class-name=high-priority' flag assigns the Pod to the PriorityClass named 'high-priority', which affects its scheduling priority and preemption behavior. This is a quick way to create a Pod with a specific priority class for testing or demonstration."
"Which of the following statements about node-pressure eviction by the kubelet is TRUE?

A) The kubelet respects PodDisruptionBudget during node-pressure eviction.
B) Pods are terminated with a grace period of 30 seconds during hard eviction thresholds.
C) The kubelet proactively terminates pods to reclaim resources like memory and disk space.
D) Node-pressure eviction is initiated via the Kubernetes API server.",C) The kubelet proactively terminates pods to reclaim resources like memory and disk space.,"Option C is correct because node-pressure eviction is a proactive process by the kubelet to terminate pods when resources like memory, disk space, or inodes are low to prevent node starvation. Option A is incorrect because the kubelet does not respect PodDisruptionBudgets during node-pressure eviction. Option B is incorrect because during hard eviction thresholds, the kubelet uses a 0 second (immediate) grace period, not 30 seconds. Option D is incorrect because node-pressure eviction is triggered locally by the kubelet, not initiated via the API server."
"What eviction signal does the kubelet use to monitor the available memory on Linux nodes?

A) memory.usage
B) memory.available
C) memory.total
D) memory.free",B) memory.available,"The correct answer is B) memory.available. On Linux nodes, kubelet calculates memory.available from cgroupfs metrics and excludes inactive_file memory, representing reclaimable memory under pressure. Option A (memory.usage) is not an eviction signal used by kubelet for eviction decisions. Option C (memory.total) and D (memory.free) are not used as eviction signals; 'memory.free' is also misleading because free memory in the system does not account for reclaimable caches and buffers."
"Which filesystem is used by container runtimes to store container images (read-only layers) separately from writable container layers?

A) nodefs
B) imagefs
C) containerfs
D) rootfs",B) imagefs,B) imagefs is the correct answer because it refers to the optional filesystem used by container runtimes to store container images (read-only layers) separately from writable layers. Option A) nodefs is the main node filesystem used for ephemeral storage and local volumes. Option C) containerfs is a filesystem for writable container layers and local volumes when imagefs is used separately. Option D) rootfs is a generic term for the root filesystem and is not specific to container image storage.
"Which of the following is TRUE regarding the kubelet behavior for static pods under node pressure?

A) Static pods are never evicted regardless of resource pressure.
B) Kubelet considers the static pod's priority when recreating it after eviction.
C) Static pods respect PodDisruptionBudget during eviction.
D) Static pods are immediately deleted without replacement on resource pressure.",B) Kubelet considers the static pod's priority when recreating it after eviction.,Option B is correct because the kubelet takes into account the static pod's priority when attempting to create a replacement after eviction. Option A is incorrect since static pods can be evicted under resource pressure. Option C is incorrect because PodDisruptionBudget is not respected by kubelet during node-pressure eviction. Option D is incorrect because kubelet tries to recreate static pods after eviction to maintain their intended state.
Create a kubelet eviction soft threshold to trigger eviction when available node memory falls below 10% of total memory.,kubectl get node <node-name> --kubelet-eviction-soft=memory.available<10%,"This command syntax specifies a soft eviction threshold for the kubelet to trigger eviction when the available memory on the node falls below 10% of total memory. However, note that eviction thresholds are typically set as kubelet startup flags or configuration file parameters, not via kubectl directly. The parameter 'memory.available<10%' defines the signal (memory.available), the operator (<), and the threshold quantity (10%). Replace <node-name> with the actual node name when applying configuration."
"Using kubectl, create a Pod manifest named 'pressure-test' with a single container running 'busybox' that sleeps for 3600 seconds.",kubectl run pressure-test --image=busybox --restart=Never -- sleep 3600,"This command creates a Pod named 'pressure-test' with a single container using the 'busybox' image. The '--restart=Never' flag ensures it creates a Pod (not a Deployment). The command to run inside the container is 'sleep 3600', which keeps the container running for 3600 seconds. This is useful for testing node pressure scenarios by running a simple pod that consumes minimal resources but persists."
"What is the primary purpose of using the API-initiated Eviction in Kubernetes?

A) To force delete a Pod immediately without respecting grace periods
B) To trigger graceful Pod termination respecting PodDisruptionBudgets and terminationGracePeriodSeconds
C) To increase the number of replicas in a Deployment
D) To update the container image of a running Pod",B) To trigger graceful Pod termination respecting PodDisruptionBudgets and terminationGracePeriodSeconds,"The correct answer is B) To trigger graceful Pod termination respecting PodDisruptionBudgets and terminationGracePeriodSeconds. API-initiated eviction creates an Eviction object that causes the Pod to be terminated gracefully, adhering to configured PodDisruptionBudgets and termination grace periods. Option A is incorrect because eviction respects termination grace periods and is not an immediate force delete. Option C is incorrect as eviction does not affect replica counts. Option D is incorrect because eviction does not update container images."
"Which API version should be used to create an Eviction object in Kubernetes version 1.22 and later?

A) policy/v1beta1
B) policy/v1
C) v1/pod
D) apps/v1",B) policy/v1,"The correct answer is B) policy/v1. Starting from Kubernetes v1.22, the Eviction API is available under policy/v1. Option A (policy/v1beta1) is deprecated in v1.22 and earlier versions should use this beta version. Option C (v1/pod) and D (apps/v1) are unrelated API groups and versions for Pods and Deployments respectively."
"If a Pod is not covered by any PodDisruptionBudget, what is the typical API server response when an eviction is requested?

A) 429 Too Many Requests
B) 500 Internal Server Error
C) 200 OK
D) 404 Not Found",C) 200 OK,"The correct answer is C) 200 OK. When the Pod is not part of any PodDisruptionBudget, the eviction is allowed and the API server returns 200 OK. Option A (429) indicates eviction is disallowed due to PodDisruptionBudget limits or rate limiting. Option B (500) indicates misconfiguration such as conflicting PodDisruptionBudgets. Option D (404) would mean the Pod does not exist, which is unrelated to eviction behavior."
"Which of the following best describes what happens after the API server allows an eviction?

A) The Pod is immediately removed from the cluster without any grace period
B) The Pod resource is annotated, and the kubelet forcefully deletes the Pod
C) The Pod receives a deletion timestamp and is gracefully terminated respecting the terminationGracePeriodSeconds
D) The Pod is scaled down by the ReplicaSet controller",C) The Pod receives a deletion timestamp and is gracefully terminated respecting the terminationGracePeriodSeconds,"The correct answer is C). After the eviction is allowed, the Pod resource is updated with a deletion timestamp, and the kubelet initiates a graceful shutdown honoring terminationGracePeriodSeconds. Option A is wrong because the Pod is not deleted immediately without grace period. Option B is incorrect because the Pod is not just annotated, and termination is graceful initially, not forceful immediately. Option D is incorrect because eviction does not scale ReplicaSets."
"You want to evict a Pod named 'myapp-pod' in the 'production' namespace using curl by posting an Eviction object. Which of the following commands is correct assuming you have a file 'eviction.json' with the Eviction object definition?

A) curl -X DELETE https://api-server/api/v1/namespaces/production/pods/myapp-pod
B) curl -v -H 'Content-type: application/json' -d @eviction.json https://api-server/api/v1/namespaces/production/pods/myapp-pod/eviction
C) curl -v -H 'Content-type: application/json' -d @eviction.json https://api-server/api/v1/namespaces/production/pods/myapp-pod
D) curl -X POST https://api-server/api/v1/namespaces/production/pods/myapp-pod/eviction",B) curl -v -H 'Content-type: application/json' -d @eviction.json https://api-server/api/v1/namespaces/production/pods/myapp-pod/eviction,"The correct answer is B). To evict a Pod using the Eviction API, you POST an Eviction object to the Pod's eviction subresource URL with the proper content-type header and payload. Option A is incorrect because DELETE deletes the Pod directly, bypassing eviction logic. Option C is incorrect because the URL lacks the '/eviction' subresource and so does not trigger eviction. Option D is incorrect because it lacks the content-type header and the POST data, which are required."
"Evictions are stuck returning 429 or 500 errors. Which of the following is NOT a recommended troubleshooting action?

A) Abort or pause the automated operation causing the issue
B) Wait and then directly delete the Pod instead of using the Eviction API
C) Increase the terminationGracePeriodSeconds of the Pod
D) Investigate the application status before restarting the operation",C) Increase the terminationGracePeriodSeconds of the Pod,"The correct answer is C). Increasing terminationGracePeriodSeconds is not recommended to resolve stuck evictions and may worsen the situation by prolonging Pod termination. Options A, B, and D are recommended steps to troubleshoot stuck evictions: pausing the operation, directly deleting the Pod if needed, and investigating the application health before resuming."
